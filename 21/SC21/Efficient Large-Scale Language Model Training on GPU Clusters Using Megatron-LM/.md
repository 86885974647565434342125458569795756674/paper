# ABSTRACT

å¦‚ä½•å°†å¼ é‡ã€ç®¡é“å’Œæ•°æ®å¹¶è¡Œæ€§ç»„åˆèµ·æ¥ä»¥æ‰©å±•åˆ°æ•°åƒä¸ªgpuã€‚We propose a novel interleaved pipelining schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. 

# INTRODUCTION

å³ä½¿æ˜¯æœ€å¤§çš„GPU (NVIDIAæœ€è¿‘å‘å¸ƒçš„80GB-A100å¡)ï¼Œä¹Ÿä¸å¯èƒ½åœ¨ä¸»å†…å­˜ä¸­æ‹Ÿåˆè¿™äº›æ¨¡å‹çš„å‚æ•°ï¼Œå³ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸­æ‹Ÿåˆæ¨¡å‹(ä¾‹å¦‚ï¼Œé€šè¿‡åœ¨ä¸»æœºå’Œè®¾å¤‡å†…å­˜ä¹‹é—´äº¤æ¢å‚æ•°[38])ï¼Œæ‰€éœ€çš„å¤§é‡è®¡ç®—æ“ä½œå¯èƒ½å¯¼è‡´ä¸åˆ‡å®é™…çš„é•¿è®­ç»ƒæ—¶é—´

Data-parallel scale-out usually works well, but suffers from two limitationsï¼ˆä¸èƒ½æ‰©å±•å¹¶è¡Œæ€§ï¼‰: a) beyond a point, the per-GPU batch size becomes too small, reducing GPU utilization and increasing communication cost, and b) the maximum number of devices that can be used is the batch size, limiting the number of accelerators that can be used for trainingã€‚tensor (intra-layer) model parallelism, where matrix multiplications within each transformer layer are split over multiple GPUs, can be used to overcome these limitations. ï¼ˆï¼Ÿï¼‰å°½ç®¡è¿™ç§æ–¹æ³•åœ¨NVIDIA DGX A100æœåŠ¡å™¨(å¸¦æœ‰8ä¸ª80GB-A100 gpu)ä¸Šçš„200äº¿ä¸ªå‚æ•°çš„æ¨¡å‹ä¸Šå·¥ä½œå¾—å¾ˆå¥½ï¼Œä½†å®ƒä¸é€‚ç”¨äºæ›´å¤§çš„æ¨¡å‹ã€‚Larger models need to be split across multiple multi-GPU servers, which leads to two problems: (a) the all-reduce communication required for tensor parallelism needs to go through inter-server links, which are slower than the highbandwidth NVLink [9] available within a multi-GPU server, and (b) a high degree of model parallelism can create small matrix multiplications (GEMMs), potentially decreasing GPU utilization.

A batch is split into smaller microbatches, and execution is pipelined across these microbatches. The layer assignment and scheduling strategy results in different performance tradeoffs. Regardless of schedule, to preserve strict optimizer semantics, optimizer steps need to be synchronized across devices, leading to a pipeline flush at the end of every batch, where microbatches are allowed to complete execution (and no new microbatches are injected). As much as 50% of time can be spent flushing the pipeline depending on the number of microbatches injected into the pipeline. å¾®æ‰¹æ•°ä¸ç®¡é“å°ºå¯¸ä¹‹æ¯”è¶Šå¤§ï¼Œç®¡é“å†²æ´—æ‰€ç”¨æ—¶é—´è¶ŠçŸ­ã€‚å› æ­¤ï¼Œä¸ºäº†å®ç°é«˜æ•ˆç‡ï¼Œé€šå¸¸éœ€è¦æ›´å¤§çš„æ‰¹é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„æµæ°´çº¿è®¡åˆ’ï¼Œä»¥æé«˜å°æ‰¹é‡ç”Ÿäº§çš„æ•ˆç‡ã€‚

å¦‚ä½•ç»“åˆå¹¶è¡ŒæŠ€æœ¯æ¥æœ€å¤§é™åº¦åœ°æé«˜ç»™å®šæ‰¹å¤„ç†å¤§å°çš„å¤§å‹æ¨¡å‹çš„è®­ç»ƒååé‡ï¼ŒåŒæ—¶ä¿æŒä¸¥æ ¼çš„ä¼˜åŒ–å™¨è¯­ä¹‰?

Our method leverages the combination of pipeline parallelism across multi-GPU servers, tensor parallelism within a multi-GPU server, and data parallelism, to practically train models with a trillion parameters with graceful scaling in an optimized cluster environment with high-bandwidth
links between GPUs on the same server and across servers.æˆ‘ä»¬å±•ç¤ºäº†æ¥è¿‘çº¿æ€§æ‰©å±•åˆ°3072ä¸ªA100 GPUï¼Œä½¿ç”¨æ··åˆç²¾åº¦å…·æœ‰ä¸€ä¸‡äº¿å‚æ•°çš„GPTæ¨¡å‹[11]ã€‚è¿™ç§ååé‡ä¿ƒè¿›äº†å®é™…åŸ¹è®­æ—¶é—´ã€‚æˆ‘ä»¬è¿˜æ¯”è¾ƒäº†ZeRO[36]ï¼Œå‘ç°å¯¹äºå…·æœ‰1750äº¿ä¸ªå’Œ5300äº¿ä¸ªå‚æ•°çš„æ¨¡å‹ï¼Œç”±äºè¾ƒå°‘çš„è·¨èŠ‚ç‚¹é€šä¿¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”ZeRO-3çš„æ€§èƒ½é«˜å‡º70%ã€‚è¿™äº›æ¨¡å‹å¤ªå¤§äº†ï¼Œä¸é€‚åˆå¤šgpuæœåŠ¡å™¨ã€‚

Achieving this throughput at scale required innovation and careful engineering along multiple axes: efficient kernel implementations that allowed most of the computation to be compute-bound as opposed to memory-bound, smart partitioning of computation graphs over the devices to reduce the number of bytes sent over network links while also limiting device idle periods, domain-specific communication optimization, and fast hardware (state-of-the-art GPUs and high-bandwidth links between GPUs on the same and different servers).

the parallelization strategy has an impact on the amount of communication, the compute efficiency with which kernels are executed, as well as the idle time workers spend waiting for computation due to pipeline flushes (pipeline bubbles).å¼ é‡å’Œç®¡é“æ¨¡å‹å¹¶è¡Œæ€§çš„æ¬¡ä¼˜ç»„åˆå¯èƒ½å¯¼è‡´ååé‡é™ä½2å€ï¼Œå³ä½¿æœåŠ¡å™¨ä¹‹é—´æœ‰é«˜å¸¦å®½çš„ç½‘ç»œé“¾æ¥ã€‚tensor model parallelism is effective within a multi-GPU server, but pipeline model
parallelism must be used for larger models.

ç”¨äºç®¡é“å¹¶è¡Œæ€§çš„è°ƒåº¦å¯¹é€šä¿¡é‡ã€ç®¡é“æ°”æ³¡å¤§å°å’Œç”¨äºå­˜å‚¨æ¿€æ´»çš„å†…å­˜æœ‰å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„äº¤é”™è°ƒåº¦ï¼Œä¸ä¹‹å‰æå‡ºçš„å…·æœ‰å¯æ¯”å†…å­˜å ç”¨çš„è°ƒåº¦ç›¸æ¯”ï¼Œè¯¥è°ƒåº¦å¯ä»¥å°†ååé‡æé«˜å¤šè¾¾10% 

Values of hyperparameters such as microbatch size have an impact on the memory footprint, the arithmetic efficiency of kernels executed on the worker, and the pipeline bubble size. In our experiments, the optimal value of the microbatch size is problem-dependent and can increase throughput by 15%.

åœ¨è§„æ¨¡ä¸Šï¼Œåˆ†å¸ƒå¼è®­ç»ƒæ˜¯é€šä¿¡å¯†é›†å‹çš„ã€‚ä½¿ç”¨è¾ƒæ…¢çš„èŠ‚ç‚¹é—´äº’è¿æˆ–æ›´å¯†é›†çš„é€šä¿¡åˆ†åŒºä¼šå½±å“æ‰©å±•æ€§èƒ½

æˆ‘ä»¬ä¸ä¼šè‡ªåŠ¨æ¢ç´¢å¹¶è¡Œç­–ç•¥çš„æœç´¢ç©ºé—´(å¦‚FlexFlow [22]ï¼Œ PipeDream [29]ï¼Œ Tarnawskiç­‰[41]å’ŒDAPPLE[14])ï¼Œè€Œæ˜¯å»ºè®®å¯å‘å¼(åœ¨Â§3ä¸­)ï¼Œæˆ‘ä»¬å‘ç°åœ¨å®è·µä¸­å·¥ä½œå¾—å¾ˆå¥½ã€‚

# MODES OF PARALLELISM

æˆ‘ä»¬å°†ç®¡é“æ¨¡å‹å¹¶è¡Œæ€§å’Œå¼ é‡æ¨¡å‹å¹¶è¡Œæ€§(å¦‚å›¾2æ‰€ç¤ºçš„ç»„åˆ)ä¸æ•°æ®å¹¶è¡Œæ€§ç»“åˆèµ·æ¥ã€‚æˆ‘ä»¬ç®€ç§°å®ƒä¸ºPTD-Pã€‚

å¯¹äºä¸é€‚åˆå•ä¸ªworkerçš„å¤§å‹æ¨¡å‹ï¼Œå¯ä»¥åœ¨è¾ƒå°çš„æ¨¡å‹åˆ†ç‰‡ä¸Šä½¿ç”¨æ•°æ®å¹¶è¡Œæ€§ã€‚

## Pipeline Model Parallelism

A batch is split into smaller microbatches; execution is then
pipelined across microbatches.æµæ°´çº¿æ¨¡å¼éœ€è¦ç¡®ä¿è¾“å…¥åœ¨å‘å‰ä¼ é€’å’Œå‘åä¼ é€’ä¸­çœ‹åˆ°ä¸€è‡´çš„æƒé‡ç‰ˆæœ¬ï¼Œä»¥å®ç°å®šä¹‰è‰¯å¥½çš„åŒæ­¥æƒé‡æ›´æ–°è¯­ä¹‰ã€‚å…·ä½“æ¥è¯´ï¼Œå¹¼ç¨šçš„æµæ°´çº¿å¯èƒ½å¯¼è‡´è¾“å…¥åœ¨å‘åä¼ é€’ä¸­çœ‹åˆ°æƒé‡æ›´æ–°ï¼Œè€Œåœ¨å‘å‰ä¼ é€’ä¸­çœ‹ä¸åˆ°ã€‚

ä¸ºäº†å‡†ç¡®åœ°ä¿ç•™ä¸¥æ ¼çš„ä¼˜åŒ–å™¨è¯­ä¹‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†å®šæœŸçš„ç®¡é“åˆ·æ–°ï¼Œä»¥ä¾¿åœ¨è®¾å¤‡ä¹‹é—´åŒæ­¥ä¼˜åŒ–å™¨æ­¥éª¤ã€‚åœ¨æ¯ä¸ªæ‰¹å¤„ç†çš„å¼€å§‹å’Œç»“æŸæ—¶ï¼Œè®¾å¤‡éƒ½æ˜¯ç©ºé—²çš„ã€‚æˆ‘ä»¬å°†è¿™ç§ç©ºé—²æ—¶é—´ç§°ä¸ºç®¡é“æ°”æ³¡ï¼Œå¹¶å¸Œæœ›ä½¿å…¶å°½å¯èƒ½å°ã€‚. Asynchronous and bounded-staleness approaches such as PipeMare, PipeDream, and PipeDream-2BW [23, 29, 30, 45] do away with flushes completely, but relax weight update semantics.æœ‰å‡ ç§å¯èƒ½çš„æ–¹æ³•å¯ä»¥è·¨è®¾å¤‡è°ƒåº¦å‘å‰å’Œå‘åå¾®æ‰¹;æ¯ç§æ–¹æ³•åœ¨ç®¡é“æ°”æ³¡å¤§å°ã€é€šä¿¡å’Œå†…å­˜å ç”¨ä¹‹é—´æä¾›äº†ä¸åŒçš„æƒè¡¡ã€‚

![](gpipe.png)

We can quantify the size of GPipeâ€™s pipeline bubble (ğ‘¡ğ‘ğ‘).
We denote the number of microbatches in a batch as ğ‘š, the number of pipeline stages (number of devices used for pipeline parallelism) as ğ‘, the ideal time per iteration as ğ‘¡ğ‘–ğ‘‘ (assuming perfect or ideal scaling)ï¼ˆstageså¹¶è¡Œï¼Œä¸²è¡Œå¤„ç†mä¸ªæ•°æ®çš„æ—¶é—´ï¼‰, and the time to execute a single microbatchâ€™s forward and backward pass as ğ‘¡ğ‘“ and ğ‘¡ğ‘
.  In this schedule, the pipeline bubble consists of ğ‘ âˆ’ 1 forward passes at the start of a batch, and ğ‘ âˆ’ 1 backward passes at the end.  The total amount of time spent in the
pipeline bubble is then ğ‘¡ğ‘ğ‘ = (ğ‘ âˆ’1) Â· (ğ‘¡ğ‘“ +ğ‘¡ğ‘).(æœ€ä¹…ç­‰å¾…æ—¶é—´) The ideal processing time for the batch is ğ‘¡ğ‘–ğ‘‘ = ğ‘š Â· (ğ‘¡ğ‘“ + ğ‘¡ğ‘
). Therefore, the fraction of ideal computation time spent in the pipeline bubble is:

![](fraction.png)

ä¸ºäº†ä½¿æ°”æ³¡æ—¶é—´åˆ†æ•°å°ï¼Œæˆ‘ä»¬éœ€è¦ğ‘š>>ğ‘ã€‚ç„¶è€Œï¼Œå¯¹äºå¦‚æ­¤å¤§çš„ğ‘šï¼Œè¿™ç§æ–¹æ³•çš„å†…å­˜å ç”¨å¾ˆé«˜ï¼Œå› ä¸ºå®ƒéœ€è¦åœ¨è®­ç»ƒè¿­ä»£çš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸå†…ä¸ºæ‰€æœ‰ğ‘šå¾®æ‰¹ä¿å­˜éšè—çš„ä¸­é—´æ¿€æ´»(æˆ–è€…ä½¿ç”¨æ¿€æ´»é‡æ–°è®¡ç®—æ—¶æ¯ä¸ªç®¡é“é˜¶æ®µçš„è¾“å…¥æ¿€æ´»)(batchå¤§å†…å­˜å¤§ï¼Œbatchå°è®¡ç®—å°)

![](pipedream.png)

ï¼ˆå‡è®¾éƒ½æ˜¯åˆ‡æˆmicrobatchï¼Œä¸€æ ·æ•°é‡ï¼Œä¸€æ ·å¤§å°ï¼‰

æˆ‘ä»¬ä½¿ç”¨PipeDream-Flushè°ƒåº¦[30]ã€‚æˆ‘ä»¬é¦–å…ˆè¿›å…¥ä¸€ä¸ªçƒ­èº«é˜¶æ®µï¼Œå·¥ä½œäººå‘˜æ‰§è¡Œä¸åŒæ•°é‡çš„å‘å‰ä¼ é€’ï¼Œå¦‚å›¾4(é¡¶éƒ¨)æ‰€ç¤ºã€‚This
schedule limits the number of in-flight microbatches (the number of microbatches for which the backward pass is outstanding and activations need to be maintained) to the depth of the pipelineï¼ˆä¸èƒ½è¿›å…¥å¤ªå¤šbatchï¼Œä¸ºä»€ä¹ˆæ˜¯microbatch?ï¼‰, instead of the number of microbatches in a batch(å¯ä»¥åˆ‡å¤šï¼Œä½†ä¸èƒ½åŒæ—¶è¿›å¤š)åœ¨çƒ­èº«é˜¶æ®µä¹‹åï¼Œæ¯ä¸ªå·¥äººè¿›å…¥ç¨³å®šçŠ¶æ€ï¼Œå·¥äººè¿›è¡Œä¸€æ¬¡å‘å‰ä¼ é€’ï¼Œç„¶åè¿›è¡Œä¸€æ¬¡å‘åä¼ é€’(ç®€ç§°1F1B)ã€‚ï¼ˆè¿™ä¸ªæœ‰flush?ï¼Œé‚£microbatchåˆç†)æœ€åï¼Œåœ¨æ‰¹å¤„ç†ç»“æŸæ—¶ï¼Œæˆ‘ä»¬å®Œæˆæ‰€æœ‰å‰©ä½™çš„é£è¡Œå¾®æ‰¹çš„å‘åä¼ é€’ã€‚The time spent in the bubble is the same for this new scheduleï¼ˆå’Œgpipeä¸€æ ·ï¼‰, but the number of outstanding forward passes is at most the number of pipeline stages for the PipeDream-Flush scheduleï¼ˆæ´»åŠ¨çš„æ•°é‡ä¸è¶…è¿‡stageæ•°é‡ï¼‰As a result, this schedule requires activations to be stashed for ğ‘ or fewer microbatches (compared to ğ‘š microbatches for the GPipe schedule). Consequently, when ğ‘š â‰« ğ‘, PipeDream-Flush is much more memory-efficient than GPipe.ï¼ˆæ°”æ³¡ä¸€æ ·å¤§ï¼Œæ´»åŠ¨çš„æ•°æ®å°‘ï¼Œå¤„ç†çš„æ€»æ•°ä¸€æ ·ï¼Œå†…å­˜å ç”¨å°‘ï¼Œè¿™æ˜¯åŒæ­¥ç‰ˆæœ¬ï¼Ÿï¼‰

To reduce the size of the pipeline bubble, each device can perform computation for multiple subsets of layers (called a model chunk), instead of a single contiguous set of layers. For example, if each device had 4 layers before
(i.e., device 1 had layers 1 âˆ’ 4, device 2 had layers 5 âˆ’ 8, and so on), we could have each device perform computation for two model chunks (each with 2 layers), i.e., device 1 has layers 1, 2, 9, 10; device2 has layers 3, 4, 11, 12; and so on. With this scheme, each device in the pipeline is assigned multiple pipeline stages (each pipeline stage has less computation compared to before).ï¼ˆä¸€ä¸ªgpuå¯¹åº”å¤šä¸ªstageï¼Œä¸€ä¸ªæ•°æ®è¦åœ¨ä¸€ä¸ªè®¾å¤‡å¤šæ¬¡å‰å‘åå‘)

å’Œä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯¥è°ƒåº¦çš„â€œå…¨éƒ¨å‘å‰ï¼Œå…¨éƒ¨å‘åâ€ç‰ˆæœ¬ï¼Œä½†è¿™å°†å ç”¨å¤§é‡å†…å­˜(ä¸ğ‘šæˆæ­£æ¯”)ã€‚(gpipe)ç›¸åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªäº¤é”™è°ƒåº¦ï¼Œå®ƒé€‚åº”äº†ä¹‹å‰çš„å†…å­˜é«˜æ•ˆ1F1Bè°ƒåº¦ã€‚è¿™ä¸ªæ–°çš„è®¡åˆ’å¦‚å›¾4æ‰€ç¤ºï¼Œå®ƒè¦æ±‚æ‰¹å¤„ç†ä¸­çš„å¾®æ‰¹æ•°é‡æ˜¯ç®¡é“å¹¶è¡Œåº¦(ç®¡é“ä¸­çš„è®¾å¤‡æ•°é‡)çš„æ•´æ•°å€ã€‚

![](int.png)

ç›¸åŒæ‰¹å¤§å°çš„ç®¡é“åˆ·æ–°åœ¨æ–°è°ƒåº¦ä¸­å‘ç”Ÿå¾—æ›´å¿«ã€‚If each device has ğ‘£ stages (or model chunks), then the forward and backward time for a microbatch for each stage or chunk will now be ğ‘¡ğ‘“ /ğ‘£ and ğ‘¡ğ‘/ğ‘£. The pipeline bubble time thus reduces to (ğ‘âˆ’1) Â· (ğ‘¡ğ‘“ +ğ‘¡ğ‘ ) / ğ‘£, and the bubble time fraction is then: 

![](fra.png)

è¿™ç§å‡å°‘çš„ç®¡é“æ°”æ³¡å¤§å°å¹¶ä¸æ˜¯å…è´¹çš„:è¿™ä¸ªè®¡åˆ’éœ€è¦é¢å¤–çš„é€šä¿¡ã€‚åœ¨æ•°é‡ä¸Šï¼Œäº¤æµçš„æ•°é‡ä¹Ÿå¢åŠ äº†ğ‘£ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•åœ¨å¤šgpuæœåŠ¡å™¨(ä¾‹å¦‚ï¼ŒDGX A100èŠ‚ç‚¹)ä¸­åˆ©ç”¨8 InfiniBandç½‘å¡æ¥å‡å°‘è¿™ç§é¢å¤–é€šä¿¡çš„å½±å“(ä¸€ä¸ªæœºå™¨å†…ä¸ºä»€ä¹ˆè¦ç”¨InfiniBandï¼Ÿ)

## Tensor Model Parallelism

åˆ©ç”¨å¼ é‡æ¨¡å‹å¹¶è¡Œæ€§ï¼Œæ¨¡å‹çš„å„ä¸ªå±‚è¢«åˆ’åˆ†åˆ°å¤šä¸ªè®¾å¤‡ä¸Šã€‚A transformer layer consists of a self-attention block followed
by a two-layer multi-layer perceptron (MLP). The MLP block consists of two GEMMs and a GeLU non-linearity: ğ‘Œ = GeLU(ğ‘‹ğ´). ğ‘ = Dropout(ğ‘Œ ğµ).

We can split ğ´ along its columns ğ´ = [ğ´1, ğ´2]. This partitioning allows the GeLU non-linearity to be independently applied to the output of each partitioned GEMM: [ğ‘Œ1, ğ‘Œ2] = [GeLU(ğ‘‹ğ´1), GeLU(ğ‘‹ğ´2)].è¿™æ˜¯æœ‰åˆ©çš„ï¼Œå› ä¸ºå®ƒæ¶ˆé™¤äº†åŒæ­¥çš„éœ€è¦(å¦‚æœæ²¿å…¶è¡Œæ‹†åˆ†ï¼Œåˆ™éœ€è¦åŒæ­¥ï¼Œå› ä¸ºGeLUæ˜¯éçº¿æ€§çš„)ã€‚

ç¬¬äºŒä¸ªæƒé‡çŸ©é˜µçš„è¡Œå¯ä»¥æ²¿ç€å®ƒçš„è¡Œæ‹†åˆ†ï¼Œä»¥æ¶ˆé™¤gemmä¹‹é—´ä»»ä½•é€šä¿¡çš„éœ€è¦(å¦‚å›¾5aæ‰€ç¤º)ï¼Œğµ =[ğµ1;ğµ2], ğ‘Œ = [ğ‘Œ1, ğ‘Œ2].The output of the second GEMM is then reduced across the GPUs before the dropout layer

æˆ‘ä»¬åˆ©ç”¨å¤šå¤´æ³¨æ„æ“ä½œä¸­å›ºæœ‰çš„å¹¶è¡Œæ€§æ¥åˆ’åˆ†è‡ªæ³¨æ„å—

![](tp.png)

This approach splits GEMMs in the MLP and self-attention blocks across GPUs while requiring only two all-reduce operations in the forward pass (ğ‘” operator) and two all-reduces in the backward pass (ğ‘“ operator). We implemented ğ‘“ and ğ‘” in a few lines of code.

all-reduce(reduce+å¤åˆ¶åˆ°å¤šä¸ªGPUç”¨äºä¸‹ä¸ªç®—å­)

# PERFORMANCE ANALYSIS OF PARALLELIZATION CONFIGURATIONS
æˆ‘ä»¬å°†è€ƒè™‘å°†ç®¡é“å’Œå¼ é‡æ¨¡å‹å¹¶è¡Œæ€§ä¸æ•°æ®å¹¶è¡Œæ€§ç›¸ç»“åˆå¯¹æ€§èƒ½çš„å½±å“ã€‚ç»™å®šå›ºå®šçš„gpué¢„ç®—å’Œæ‰¹å¤„ç†å¤§å°ï¼Œå¯ä»¥åœ¨PTD-Pä¸­ä½¿ç”¨ä¸åŒç¨‹åº¦çš„å¹¶è¡Œç±»å‹æ¥è®­ç»ƒæ¨¡å‹;æ¯ä¸ªç»´åº¦éƒ½æ­ç¤ºäº†å†…å­˜å ç”¨ã€è®¾å¤‡åˆ©ç”¨ç‡å’Œé€šä¿¡é‡ä¹‹é—´çš„æƒè¡¡ã€‚We present analytical models where relevant for the pipeline bubble size. We qualitatively describe how communication time behaves and present cost models for amount of communication; ï¼ˆé€šä¿¡é‡ï¼‰however, we do not present direct cost models for communication timeï¼ˆé€šä¿¡æ—¶é—´è¿˜è¦è€ƒè™‘å¸¦å®½ï¼‰, which is harder to model for a hierarchical network topology where interconnects between GPUs on the same server have higher bandwidth than interconnects between servers.

![](notation.png)

## Tensor and Pipeline Model Parallelism

ä½¿ç”¨ç®¡é“å¹¶è¡Œæ€§å’Œå‘¨æœŸæ€§å†²æ´—ä¼šäº§ç”Ÿå¤§å°ä¸º(ğ‘âˆ’1)/ğ‘šçš„ç®¡é“æ°”æ³¡ã€‚å‡è®¾ğ‘‘= 1(æ•°æ®å¹¶è¡Œå¤§å°);å› æ­¤ï¼Œğ‘¡Â·ğ‘=ğ‘›ã€‚ç®¡é“æ°”æ³¡å°ºå¯¸ä»¥ğ‘¡è¡¨ç¤ºä¸º:(ğ‘ âˆ’ 1)/ğ‘š=(ğ‘›/ğ‘¡ âˆ’ 1)/ğ‘š. As ğ‘¡ increases, the pipeline bubble thus decreases for fixed ğµ, ğ‘, and ğ‘‘ (ğ‘š = ğµ/(ğ‘ Â· ğ‘‘) is fixed as well).(mæ˜¯å›ºå®šçš„ï¼Œstageæ•°é‡å‡å°‘ï¼Œåº”è¯¥æ˜¯ä¸€ä¸ªstageè¿›è¡Œtpï¼Œä¸ºä»€ä¹ˆæ²¡æœ‰v)

ä¸åŒgpuä¹‹é—´çš„é€šä¿¡é‡ä¹Ÿå—ğ‘å’Œğ‘¡çš„å€¼çš„å½±å“ã€‚ç®¡é“æ¨¡å‹çš„å¹¶è¡Œæ€§æä¾›äº†æ›´ä¾¿å®œçš„ç‚¹å¯¹ç‚¹é€šä¿¡ã€‚å¦ä¸€æ–¹é¢ï¼Œå¼ é‡æ¨¡å‹çš„å¹¶è¡Œæ€§ä½¿ç”¨all-reduceé€šä¿¡(åœ¨å‘å‰å’Œå‘åä¼ é€’ä¸­å„æœ‰ä¸¤ä¸ªall-reduceæ“ä½œï¼Œå‚è§Â§2.3)ã€‚With pipeline parallelism, the total amount of communication that needs to be performed between every pair of consecutive devices (for either the forward or backward pass) for each microbatch is ğ‘ğ‘ â„, where ğ‘  is the sequence length and â„ is the hidden size. (è¾“å…¥å¤§å°) 

With tensor model parallelism, tensors of total size ğ‘ğ‘ â„ need to be all-reduced among ğ‘¡ model replicas twice each
in the forward and backward pass for each layer(æ¯ä¸€å±‚éœ€è¦ä¸¤æ¬¡ï¼Œåˆ†åˆ«åœ¨å‰å‘å’Œåå‘), leading to a total communication of 8ğ‘ğ‘ â„ï¼ˆğ‘¡âˆ’1ï¼‰/ğ‘¡ per layer per device for each microbatch. ï¼ˆï¼Ÿï¼‰Each device typically has multiple layers; the total amount of tensor-parallel-communication per device for each microbatch is then $ğ‘™^{stage}*$ 8ğ‘ğ‘ â„(ğ‘¡âˆ’1)/ğ‘¡, where $ğ‘™^{stage}$ is the number of layers in a pipeline stage.

å¼ é‡æ¨¡å‹çš„å¹¶è¡Œæ€§å¢åŠ äº†è®¾å¤‡ä¹‹é—´çš„é€šä¿¡é‡ã€‚å› æ­¤ï¼Œå½“ğ‘¡å¤§äºå•ä¸ªèŠ‚ç‚¹ä¸­çš„gpuæ•°é‡æ—¶ï¼Œåœ¨è¾ƒæ…¢çš„èŠ‚ç‚¹é—´é“¾æ¥ä¸Šæ‰§è¡Œå¼ é‡æ¨¡å‹å¹¶è¡Œçš„å¼€é”€å¯èƒ½æ˜¯ä¸åˆ‡å®é™…çš„ã€‚

Takeaway #1: When considering different forms of model parallelism, tensor model parallelism should generally be used up to degree ğ‘” when using ğ‘”-GPU servers, and then pipeline model parallelism can be used to scale up to larger models across servers.(tpé™åˆ¶åœ¨ä¸€ä¸ªæœºå™¨)

## Data and Model Parallelism

æˆ‘ä»¬è¿˜éœ€è¦è€ƒè™‘æ•°æ®å¹¶è¡Œæ€§å’Œä¸¤ç§ç±»å‹çš„æ¨¡å‹å¹¶è¡Œæ€§ä¹‹é—´çš„äº¤äº’ã€‚

Let ğ‘¡ = 1 (tensor-model-parallel size). The number of microbatches per pipeline is ğ‘š = ğµ/(ğ‘‘ Â· ğ‘) =ğ‘â€²/ğ‘‘, where ğ‘â€²
:= ğµ/ğ‘. With total number of GPUs ğ‘›, the number of pipeline stages is ğ‘ = ğ‘›/(ğ‘¡ Â· ğ‘‘) = ğ‘›/ğ‘‘. The pipeline bubble size is

![](pp_dp.png)

éšç€ğ‘‘å˜å¤§ï¼Œğ‘›âˆ’ğ‘‘å˜å°ï¼Œç®¡é“æ°”æ³¡å˜å°ã€‚It might not be possible to increase ğ‘‘ all the way to ğ‘› for all models, since a modelâ€™s full training memory footprint might be larger than the memory capacity of a single accelerator.

Overall throughput will thus increase if the all-reduce communication needed for data parallelism does not drastically increase with higher ğ‘‘, which should hold since the communication time for a ring-based implementation scales with ğ‘‘âˆ’1/ğ‘‘= 1 âˆ’1/ğ‘‘.

For a given parallel configuration, as the batch size ğµ increases, ğ‘â€² = ğµ/ğ‘ increases, (ğ‘› âˆ’ ğ‘‘)/ğ‘â€² decreases, consequently increasing throughput.ï¼ˆmicrobatchæ•°é‡å¢å¤šï¼Œå¤§å°æ²¡å˜ï¼‰ All-reduce communication required by data parallelism also becomes more infrequent, further increasing throughput.(å•æ¬¡å¢å¤šäº†ï¼Ÿ)

With tensor model parallelism, all-reduce communication needs to be performed for every microbatch. This can be expensive across multi-GPU servers. On the other hand, data parallelism only needs to perform expensive all-reduce communication once per batch. ä½¿ç”¨å¼ é‡æ¨¡å‹å¹¶è¡Œæ€§ï¼Œæ¯ä¸ªæ¨¡å‹å¹¶è¡Œç­‰çº§åœ¨æ¯ä¸ªæ¨¡å‹å±‚ä¸­æ‰§è¡Œè®¡ç®—çš„å­é›†ï¼Œå› æ­¤å¯¹äºä¸å¤Ÿå¤§çš„å±‚ï¼Œç°ä»£gpuå¯èƒ½æ— æ³•ä»¥å³°å€¼æ•ˆç‡æ‰§è¡Œè¿™äº›å­çŸ©é˜µè®¡ç®—

Takeaway #2: When using data and model parallelism, a total model-parallel size of ğ‘€ = ğ‘¡ Â· ğ‘ should be used so that the modelâ€™s parameters and intermediate metadata fit in GPU memory; data parallelism can be used to scale up training to more GPUs.

## Microbatch Size

per-GPU throughput increases by up to 1.3Ã— with a larger microbatch size on a single GPUã€‚We now want to determine the optimal microbatch size ğ‘ given a parallel configuration (ğ‘, ğ‘¡, ğ‘‘) and batch size ğµ. The amount of data-parallel communication will be the same regardless of the
microbatch size.ï¼ˆå’Œbatchå¤§å°æœ‰å…³ï¼‰ Given functions ğ‘¡ğ‘“
(ğ‘) and ğ‘¡ğ‘(ğ‘) that map the microbatch size to the forward and backward computation times for a single microbatch, the total time spent computing a batch, ignoring communication cost, is

![](tf_tb.png)

(è¿ç»­æ‰§è¡Œ(b'/b)çš„æ—¶é—´åŠ ä¸Šæ°”æ³¡ï¼ˆp-1)çš„æ—¶é—´)

å¾®æ‰¹å¤§å°å› æ­¤å½±å“è¿ç®—çš„ç®—æœ¯å¼ºåº¦(t(b))ä»¥åŠç®¡é“æ°”æ³¡å¤§å°(é€šè¿‡å½±å“ğ‘š)ã€‚(b'/b)

![](micro.png)

ç»“è®º3:æœ€ä½³å¾®æ‰¹å¤§å°ğ‘å–å†³äºæ¨¡å‹çš„ååé‡å’Œå†…å­˜å ç”¨ç‰¹æ€§ï¼Œä»¥åŠç®¡é“æ·±åº¦ğ‘ã€æ•°æ®å¹¶è¡Œå¤§å°ğ‘‘å’Œæ‰¹å¤§å°ã€‚

## Activation Recomputation

æ¿€æ´»é‡è®¡ç®—[12,18,20,21]æ˜¯ä¸€ç§å¯é€‰çš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡åœ¨å‘åä¼ é€’ä¹‹å‰è¿è¡Œç¬¬äºŒæ¬¡å‘å‰ä¼ é€’(å¹¶ä¸”ä»…å­˜å‚¨ç»™å®šç®¡é“é˜¶æ®µçš„è¾“å…¥æ¿€æ´»ï¼Œè€Œä¸æ˜¯å­˜å‚¨æ•´ä¸ªä¸­é—´æ¿€æ´»é›†ï¼Œåè€…è¦å¤§å¾—å¤š)ï¼Œtrades off an increase in the number of compute operations performed for additional memory footprint. Activation recomputation is required to train reasonably large models with pipeline parallelism to keep memory footprint acceptably low(å¹¶è¡Œåº¦å¤§ï¼Œå¤„ç†çš„æ•°æ®å¤š)

æ¿€æ´»æ£€æŸ¥ç‚¹çš„æ•°é‡ä¸ä¼šå½±å“ååé‡ï¼Œä½†ä¼šå½±å“å†…å­˜å ç”¨ã€‚Let $ğ´^{input}$ be the size of the input activations of a layer, and $ğ´^{intermediate}$ be the size of intermediate activations per layer. If a model stage has ğ‘™ layers, and if ğ‘ is the number of checkpoints, the total memory footprint is going to be ğ‘ Â·$ğ´^{input}$ +ğ‘™/ğ‘ Â·$ğ´^{intermediate}$. (ä¸€ä¸ªstageé‡Œæœ‰cä¸ªï¼Œä¸­é—´æ¿€æ´»æ˜¯æ€ä¹ˆå›äº‹ï¼Ÿ) . In practice, we measure $ğ´^{intermediate}$ empirically. For most cases, checkpointing every 1 or 2 transformer layers is optimal.å…¶ä»–æŠ€æœ¯ï¼Œå¦‚æ¿€æ´»åˆ†åŒº[36]ä¹Ÿå¯ä»¥ä¸å¼ é‡æ¨¡å‹å¹¶è¡Œç»“åˆä½¿ç”¨ï¼Œä»¥è¿›ä¸€æ­¥å‡å°‘ç”±äºæ¿€æ´»è€Œäº§ç”Ÿçš„å†…å­˜å ç”¨ã€‚

# IMPLEMENTATION

æˆ‘ä»¬å°†PTD-Pä½œä¸ºMegatron-LMä»£ç åº“çš„æ‰©å±•æ¥å®ç°ã€‚æˆ‘ä»¬çš„å®ç°æ˜¯ä½¿ç”¨PyTorch[32]æ„å»ºçš„ã€‚æˆ‘ä»¬ä½¿ç”¨NCCL[7]æ¥å®ç°è®¾å¤‡é—´çš„é€šä¿¡ã€‚

##  Communication Optimizations

When using pipeline parallelism, we want to send and receive tensors in the forward and backward direction in parallel.  Each DGX A100 is equipped with 8 InfiniBand (IB) networking cards. Unfortunately, sends and receives are point-to-point, and only happen between a pair of GPUs on two servers, making it hard to leverage all 8 cards for a single communication call within the pipeline.

æˆ‘ä»¬å¯ä»¥åˆ©ç”¨åŒæ—¶ä½¿ç”¨å¼ é‡æ¨¡å‹å¹¶è¡Œæ€§å’Œç®¡é“æ¨¡å‹å¹¶è¡Œæ€§çš„äº‹å®æ¥å‡å°‘è·¨èŠ‚ç‚¹é€šä¿¡çš„å¼€é”€ã€‚In particular, we note that the output of each transformer layer is replicated (after ğ‘” in MLP block, see Figure 5a) across the tensor-parallel ranks.(è¾“å‡ºè¢«å¤åˆ¶å¤šä»½åˆ†åˆ«ä¼ åˆ°ä¸‹ä¸€å±‚çš„å¤šä¸ªgpu) As a result, ranks in two consecutive pipeline stages that are performing tensor model parallelism send and receive the exact same set of tensors (Figure 9a).

![](tp.png)

For large enough models, we use a tensor-model-parallel size of 8. This means we are sending the same set of tensors 8 times between corresponding GPUs on adjacent multi-GPU servers. To reduce this redundancy, we can instead split the tensor on the send side into equal-sized chunks, and then only send one chunk to the corresponding rank on the next node using the rankâ€™s own InfiniBand card.  With 8 tensor-model-parallel ranks, each chunk would be one-eighth smaller. Then, on the receive side, we can
perform an all-gather over NVLink, which is much faster than the InfiniBand interconnect, to re-materialize the full tensor. è¿™ç§ä¼˜åŒ–æœ‰åŠ©äºæ›´å¥½åœ°åˆ©ç”¨DGX A100æœåŠ¡å™¨ä¸Šçš„å¤šä¸ªIBå¡ï¼Œå¹¶ä½¿æ›´å¤šçš„é€šä¿¡å¯†é›†å‹è°ƒåº¦(å¦‚äº¤é”™è°ƒåº¦)å˜å¾—å¯è¡Œ

![](nv-link.png)

with the scatter-gather communication optimization, the total amount of communication that needs to be performed
between every pair of consecutive stages is reduced to ğ‘ğ‘ â„/t(ä¸€ä¸ªinfiniband)

## Computation Optimizations

æˆ‘ä»¬å¯¹è®¡ç®—å›¾å®ç°äº†ä¸‰ä¸ªç‰¹å®šäºæ¨¡å‹çš„ä¼˜åŒ–ï¼Œä»¥è·å¾—é«˜æ€§èƒ½ã€‚First, we changed the data layout in the transformer layer to avoid memory-intensive transpose operations, and to enable the use of strided batched GEMM kernels. we changed the data layout from [ğ‘, ğ‘ , ğ‘, â„] to [ğ‘ , ğ‘, ğ‘, â„], where ğ‘, ğ‘ , ğ‘, and â„ are batch, sequence, attention-head, and hidden-size dimensions.å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨PyTorch JIT[10]ä¸ºä¸€ç³»åˆ—å…ƒç´ æ“ä½œ(bias + GeLUå’Œbias + dropout + add)ç”Ÿæˆèåˆå†…æ ¸ã€‚ Third, we created two custom kernels to enable the fusion of scale, mask, and softmax (reduction) operations: one to support general masking (used in models such as BERT) and another to support implicit causal masking (used in auto-regressive models such as GPT). 

# EVALUATION

å¯¹äºæˆ‘ä»¬çš„å¤§å¤šæ•°ç»“æœï¼Œæˆ‘ä»¬æŠ¥å‘Šæ¯ä¸ªGPUçš„ååé‡ã€‚æ€»ååé‡å¯ä»¥é€šè¿‡ä¸æ‰€ä½¿ç”¨çš„gpuæ•°é‡ç›¸ä¹˜æ¥è®¡ç®—ã€‚

## End-to-End Performance

ä½¿ç”¨å¼ é‡ï¼Œç®¡é“å’Œæ•°æ®å¹¶è¡Œæ€§(ä½¿ç”¨Â§3ä¸­æè¿°çš„å¯å‘å¼é€‰æ‹©çš„åº¦)ã€‚æˆ‘ä»¬åœ¨å¯ç”¨äº†åˆ†æ•£/æ”¶é›†ä¼˜åŒ–çš„æƒ…å†µä¸‹ä½¿ç”¨äº¤é”™ç®¡é“è°ƒåº¦ã€‚As the model size increases, we also increase the batch size (ğµ) and the number of GPUs (ğ‘›). assumes activation recomputation and takes into account the floating-point
operations associated with the extra forward pass.

![](linear.png)

We see superlinear scaling to 3072 A100 GPUs (384 DGX A100 nodes), since GPU utilization improves as the models get larger (larger matrix multiplications) without significant increase in the communication time relative to computation time.ååé‡æ˜¯ä¸ºç«¯åˆ°ç«¯è®­ç»ƒæµ‹é‡çš„ï¼Œå³åŒ…æ‹¬æ‰€æœ‰æ“ä½œï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€ä¼˜åŒ–å™¨æ­¥éª¤ã€é€šä¿¡å’Œæ—¥å¿—è®°å½•ã€‚å¯¹äºæœ€å¤§çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å®ç°äº†52%çš„å³°å€¼è®¾å¤‡ååé‡ï¼Œå¯¹äºæœ€å°çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å®ç°äº†44%çš„å³°å€¼è®¾å¤‡ååé‡ã€‚

Given these throughputs, we can also estimate the total amount of time needed for end-to-end training on ğ‘‡ tokens. Training requires ğ¼ = ğ‘‡ /(ğµ Â· ğ‘ ) iterations. å¯¹äº1ä¸‡äº¿å‚æ•°æ¨¡å‹ï¼Œæˆ‘ä»¬å‡è®¾ç«¯åˆ°ç«¯è®­ç»ƒéœ€è¦4500äº¿ä¸ªä»¤ç‰Œã€‚ä½¿ç”¨3072ä¸ªA100 gpuï¼Œæˆ‘ä»¬å¯ä»¥å®ç°163 teraFLOP/sçš„æ¯gpuååé‡ï¼Œç«¯åˆ°ç«¯è®­ç»ƒæ—¶é—´ä¸º84å¤©ã€‚

## Comparison to ZeRO-3

ç»“æœä¸ºä¸ä½¿ç”¨æ¨¡å‹å¹¶è¡Œæ€§çš„æ–¹æ³•æä¾›äº†ä¸€ä¸ªæ¯”è¾ƒç‚¹ã€‚(?)æˆ‘ä»¬ä½¿ç”¨DeepSpeed Pythonåº“[3]å°†ZeROé›†æˆåˆ°æˆ‘ä»¬çš„ä»£ç åº“ä¸­.æˆ‘ä»¬åœ¨å¢åŠ gpuæ•°é‡çš„åŒæ—¶ä¿æŒå…¨å±€æ‰¹å¤„ç†å¤§å°ä¸å˜ã€‚ With fewer GPUs and a microbatch size of 4, PTD-P results in 6% and 24% higher throughput for the 175- and 530-billion-parameter models respectively.éšç€gpuæ•°é‡çš„å¢åŠ ï¼ŒPTD-Påœ¨éš”ç¦»çŠ¶æ€ä¸‹çš„æ‰©å±•æ¯”ZeRO-3æ›´ä¼˜é›…(å‚è§å›¾10)ã€‚é€šè¿‡å°†gpuæ•°é‡å¢åŠ ä¸€å€(ä¿æŒæ‰¹å¤„ç†å¤§å°ç›¸åŒ)ï¼Œç”±äºè·¨èŠ‚ç‚¹é€šä¿¡å‡å°‘ï¼ŒPTD-Påœ¨ä¸¤ç§å‹å·ä¸Šçš„æ€§èƒ½éƒ½æ¯”ZeRO-3é«˜70%ã€‚

![](zero.png)

![](zero-3.png)

## Pipeline Parallelism

æˆ‘ä»¬å­¤ç«‹åœ°è¯„ä¼°äº†ç®¡é“å¹¶è¡Œçš„å¼±ä¼¸ç¼©æ€§èƒ½ï¼Œå¹¶æ¯”è¾ƒäº†éäº¤é”™è°ƒåº¦å’Œäº¤é”™è°ƒåº¦çš„æ€§èƒ½

We evaluate the scaling of the default noninterleaved pipeline-parallel schedule using a weak scaling setup.As we increase the number of pipeline stages, we also increase the size of the model by proportionally increasing the number of layers in the model.Figure 11 shows throughput per GPU for two different batch sizes to illustrate the impact of the pipeline bubble, which behaves as (ğ‘âˆ’1)/ğ‘š (Â§2.2.1). æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæ›´é«˜çš„æ‰¹å¤§å°å¯ä»¥æ›´å¥½åœ°æ‰©å±•ï¼Œå› ä¸ºç®¡é“æ°”æ³¡è¢«åˆ†æ‘Šåˆ°æ›´å¤šçš„å¾®æ‰¹ä¸Š

![](throug.png)

The interleaved schedule with the scatter/gather communication optimization has higher computational performance than the non-interleaved (default) schedule.ç”±äºä¸¤ä¸ªåŸå› ï¼Œè¿™ä¸ªå·®è·éšç€æ‰¹å¤§å°çš„å¢åŠ è€Œç¼©å°:(a)éšç€æ‰¹å¤§å°çš„å¢åŠ ï¼Œé»˜è®¤è°ƒåº¦ä¸­çš„æ°”æ³¡å¤§å°å‡å°ï¼Œä»¥åŠ(b)ç®¡é“å†…ç‚¹å¯¹ç‚¹é€šä¿¡çš„æ•°é‡ä¸æ‰¹å¤§å°æˆæ­£æ¯”ï¼Œå› æ­¤éäº¤é”™è°ƒåº¦éšç€é€šä¿¡æ•°é‡çš„å¢åŠ è€Œèµ¶ä¸Š(äº¤é”™è°ƒåº¦çš„ç‰¹å¾æ˜¯æ¯ä¸ªæ ·æœ¬çš„é€šä¿¡æ›´å¤š)ã€‚å¦‚æœæ²¡æœ‰åˆ†æ•£/æ”¶é›†ä¼˜åŒ–ï¼Œé»˜è®¤è°ƒåº¦åœ¨æ›´å¤§çš„æ‰¹å¤„ç†è§„æ¨¡(æœªæ˜¾ç¤º)ä¸‹æ¯”äº¤é”™è°ƒåº¦æ‰§è¡Œå¾—æ›´å¥½ã€‚(äº¤é”™è°ƒåº¦é€šä¿¡å¤§ï¼Œæ°”æ³¡å°ï¼Œbatchå¢å¤§ï¼Œæ°”æ³¡éƒ½å¾ˆå°ï¼Œé€šä¿¡å¾ˆå¤§)

![](through-bat.png)

## Comparison of Parallel Configurations

we show the performance for parallel configurations using the same number of GPUs for a given model and multiple batch sizes

å›¾13ä¸­çš„å®è¯ç»“æœè¡¨æ˜ï¼Œç»“åˆä½¿ç”¨å¼ é‡å’Œç®¡é“æ¨¡å‹å¹¶è¡Œæ€§æ¥è®­ç»ƒä¸€ä¸ª1610äº¿ä¸ªå‚æ•°çš„GPTæ¨¡å‹(32ä¸ªå˜å‹å™¨å±‚ï¼Œæ”¯æŒç®¡é“å¹¶è¡Œå¤§å°ä¸º32,128ä¸ªæ³¨æ„å¤´ï¼Œéšè—å¤§å°ä¸º20480)å…·æœ‰ä½é€šä¿¡å¼€é”€å’Œé«˜è®¡ç®—èµ„æºåˆ©ç”¨ç‡çš„é‡è¦æ€§ã€‚We observe that tensor model parallelism is best within a node (DGX A100 server) due to its expensive all-reduce communication. Pipeline model parallelism, on the other hand, uses much cheaper point-to-point communication that can be performed across nodes without bottlenecking the entire computation.ç„¶è€Œï¼Œåœ¨ç®¡é“å¹¶è¡Œçš„æƒ…å†µä¸‹ï¼Œå¤§é‡çš„æ—¶é—´å¯èƒ½ä¼šèŠ±è´¹åœ¨ç®¡é“æ°”æ³¡ä¸­:å› æ­¤ï¼Œç®¡é“çº§çš„æ€»æ•°åº”è¯¥å—åˆ°é™åˆ¶ï¼Œä»¥ä¾¿ç®¡é“ä¸­çš„å¾®æ‰¹æ•°é‡æ˜¯ç®¡é“çº§æ•°é‡çš„åˆç†å€æ•°ã€‚å½“å¼ é‡å¹¶è¡Œå¤§å°ç­‰äºå•ä¸ªèŠ‚ç‚¹ä¸­çš„gpuæ•°é‡(DGX A100èŠ‚ç‚¹ä¸º8ä¸ª)æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å³°å€¼æ€§èƒ½ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯å•ç‹¬çš„å¼ é‡æ¨¡å‹å¹¶è¡Œæ€§(ç”±Megatron[40]ä½¿ç”¨)è¿˜æ˜¯ç®¡é“æ¨¡å‹å¹¶è¡Œæ€§(ç”±PipeDream[30]ç­‰ä½¿ç”¨)ï¼Œéƒ½æ— æ³•åŒ¹é…å°†è¿™ä¸¤ç§æŠ€æœ¯ç»“åˆä½¿ç”¨çš„æ€§èƒ½ã€‚

![](tp_pp.png)

å¯¹äºæ¯ä¸ªæ‰¹å¤§å°ï¼Œååé‡éšç€ç®¡é“å¹¶è¡Œå¤§å°çš„å¢åŠ è€Œé™ä½ï¼Œä¸Â§3.3ä¸­çš„åˆ†ææ¨¡å‹ç›¸åŒ¹é…ã€‚åº”è¯¥ä¸»è¦ä½¿ç”¨ç®¡é“æ¨¡å‹å¹¶è¡Œæ€§æ¥æ”¯æŒä¸é€‚åˆå•ä¸ªå·¥äººçš„å¤§å‹æ¨¡å‹çš„è®­ç»ƒï¼Œå¹¶ä¸”åº”è¯¥ä½¿ç”¨æ•°æ®å¹¶è¡Œæ€§æ¥æ‰©å±•è®­ç»ƒã€‚

![](dp_pp.png)

With larger batch sizes and a microbatch size of 1, data-parallel communication is infrequent;(ä¸€æ¬¡å¤„ç†çš„æ•°æ®å¤š) the all-to-all communication required in tensor model parallelism needs to be performed for every microbatch in a batch. è¿™ç§å…·æœ‰å¼ é‡æ¨¡å‹å¹¶è¡Œæ€§çš„å…¨å¯¹å…¨é€šä¿¡æ”¯é…ç€ç«¯åˆ°ç«¯è®­ç»ƒæ—¶é—´ï¼Œç‰¹åˆ«æ˜¯å½“é€šä¿¡éœ€è¦è·¨å¤šä¸ªgpuèŠ‚ç‚¹æ‰§è¡Œæ—¶ã€‚éšç€å¼ é‡æ¨¡å‹å¹¶è¡Œå¤§å°çš„å¢åŠ ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªGPUä¸Šæ‰§è¡Œæ›´å°çš„çŸ©é˜µä¹˜æ³•ï¼Œä»è€Œé™ä½æ¯ä¸ªGPUçš„åˆ©ç”¨ç‡ã€‚æˆ‘ä»¬åº”è¯¥æ³¨æ„åˆ°ï¼Œè™½ç„¶æ•°æ®å¹¶è¡Œæ€§å¯ä»¥å¯¼è‡´æœ‰æ•ˆçš„æ‰©å±•ï¼Œä½†æˆ‘ä»¬ä¸èƒ½å­¤ç«‹åœ°ä½¿ç”¨æ•°æ®å¹¶è¡Œæ€§æ¥å¤„ç†å…·æœ‰æœ‰é™è®­ç»ƒæ‰¹å¤§å°çš„éå¸¸å¤§çš„æ¨¡å‹ï¼Œå› ä¸ºa)å†…å­˜å®¹é‡ä¸è¶³ï¼Œ b) scaling limitations of data parallelism (e.g., GPT-3 was trained to convergence with a batch size of 1536.æ•°æ®å¹¶è¡Œæ€§å› æ­¤åªæ”¯æŒ1536ä¸ªgpuçš„å¹¶è¡ŒåŒ–;ç„¶è€Œï¼Œåœ¨åˆç†çš„æ—¶é—´å†…ï¼Œå¤§çº¦ä½¿ç”¨äº†10,000ä¸ªgpuæ¥è®­ç»ƒè¿™ä¸ªæ¨¡å‹)ã€‚

![](tp_dp.png)

## Microbatch Size

![](tp_pp.png)

æœ€ä½³å¾®æ‰¹å¤§å°å¯¹äºå…¶ä»–æ¨¡å‹æ˜¯ä¸åŒçš„(å›¾ä¸­æœªæ˜¾ç¤º)ï¼Œå¹¶ä¸”æ˜¯æ¨¡å‹ç›¸å…³çš„ã€‚å¯¹äºç»™å®šçš„æ‰¹å¤§å°ï¼Œå¢åŠ å¾®æ‰¹å¤§å°ä¼šå‡å°‘ç®¡é“ä¸­çš„å¾®æ‰¹æ•°é‡(ğ‘š)ï¼Œå¯¼è‡´æ›´å¤§çš„ç®¡é“æ°”æ³¡;ç„¶è€Œï¼Œå¢åŠ å¾®æ‰¹å¤§å°ä¹Ÿå¯ä»¥é€šè¿‡å¢åŠ æ‰§è¡Œå†…æ ¸çš„ç®—æœ¯å¼ºåº¦æ¥æé«˜GPUåˆ©ç”¨ç‡ã€‚æˆ‘ä»¬åœ¨Â§3.3ä¸­çš„åˆ†ææ¨¡å‹åˆç†åœ°é€¼è¿‘äº†çœŸå®çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥ç”¨ä½œä»£ç†æ¥ç¡®å®šå¦‚ä½•ä¸ºå„ç§è®­ç»ƒé…ç½®å’Œæ¨¡å‹é€‰æ‹©è¿™ä¸ªè¶…å‚æ•°å€¼ã€‚

## Activation Recomputation

![](act.png)

For small batch sizes, activation recomputation leads to up to 33% lower throughput (in sequences per second) due to the extra forward pass that needs to be executed during the backward pass. However, activation recomputation is needed to support larger batch sizes. . Throughput at
large batch sizes with activation recomputation is up to 2Ã— higher than the best throughput achieved without activation recomputation (for a smaller batch size) due to a smaller pipeline bubble.ï¼ˆæ¯”ä¸åŒbatch size?)

## Scatter-Gather Optimization

é€šè¿‡å‡å°‘è·¨èŠ‚ç‚¹é“¾è·¯ä¸Šçš„é€šä¿¡é‡ï¼Œé€šä¿¡å¯†é›†å‹è°ƒåº¦(å…·æœ‰äº¤é”™çš„å¤§æ‰¹å¤„ç†å¤§å°)çš„ååé‡æé«˜äº†11%ã€‚

## Fused Operators

å¯¹äºGPT-3æ¨¡å‹(1750äº¿ä¸ªå‚æ•°)ï¼Œååé‡é€šè¿‡èåˆæé«˜äº†19%

## Inter-Node Communication Bandwidth

Our strong results are a byproduct of using an optimized software and hardware stack together. we take advantage of the high-bandwidth communication links between GPUs on the same server and across servers.æµæ°´çº¿çº§ä¹‹é—´ç‚¹å¯¹ç‚¹é€šä¿¡çš„æœ‰æ•ˆå¯¹åˆ†å¸¦å®½ä¸º892 GB/sï¼Œè€Œæ•°æ®å¹¶è¡Œå‰¯æœ¬ä¹‹é—´å…¨çº¦æ“ä½œçš„æœ‰æ•ˆå¯¹åˆ†å¸¦å®½ä¸º12.9 TB/sã€‚ A less-optimized partitioning of operators across devices would lead to more inter-node
communication, hampering scaling performance.

# RELATED WORK

ç®¡é“å¹¶è¡Œæœ‰å‡ ç§æ–¹å¼:æœ¬æ–‡è®¨è®ºçš„æ¨¡å¼ä½¿ç”¨flushæ¥ç¡®ä¿ä¸¥æ ¼çš„ä¼˜åŒ–å™¨è¯­ä¹‰ã€‚ä¸æœ¬æ–‡ä¸­è€ƒè™‘çš„ç®¡é“å†²æ´—æŠ€æœ¯ç›¸æ¯”ï¼Œè¿™äº›æŠ€æœ¯æé«˜äº†ååé‡ï¼Œä½†å¯èƒ½ä»¥æ”¶æ•›é€Ÿåº¦æˆ–æœ€ç»ˆç²¾åº¦ä¸ºä»£ä»·ã€‚

DeepSpeed[2]å°†ç®¡é“å¹¶è¡Œæ€§ä¸å¼ é‡å¹¶è¡Œæ€§å’Œæ•°æ®å¹¶è¡Œæ€§ç›¸ç»“åˆï¼Œä»¥è®­ç»ƒå…·æœ‰å¤šè¾¾ä¸€ä¸‡äº¿å‚æ•°çš„æ¨¡å‹ï¼Œä½†ååé‡ä½äºæœ¬æ–‡æ‰€ç¤º(52% vs.å³°å€¼çš„36%)ï¼Œ for a few reasons: operator fusion to keep most of the operator graph compute-bound, a more-efficient pipeline parallelism schedule to minimize the pipeline bubble size, fast hardware and scaling to more GPUs.æˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹ï¼Œä½†éœ€è¦æ›´å¤šçš„gpuæ¥ä¿æŒè®­ç»ƒæ—¶é—´çš„å®ç”¨æ€§ã€‚

FlexFlow[22]ã€PipeDream[29]ã€DAPPLE[14]å’ŒTarnawskiç­‰[41]éƒ½æ˜¯å€ŸåŠ©æˆæœ¬æ¨¡å‹åœ¨å¤šè®¾å¤‡ä¸Šè‡ªåŠ¨åˆ’åˆ†æ¨¡å‹è®­ç»ƒå›¾ã€‚ç„¶è€Œï¼Œè¿™äº›éƒ½æ²¡æœ‰è€ƒè™‘åˆ°æœ¬æ–‡ä¸­è€ƒè™‘çš„æ‰€æœ‰å¹¶è¡Œæ€§ç»´åº¦:ç®¡é“å’Œå¼ é‡æ¨¡å‹å¹¶è¡Œæ€§ã€æ•°æ®å¹¶è¡Œæ€§ã€å¾®æ‰¹å¤§å°ï¼Œä»¥åŠæ¿€æ´»é‡è®¡ç®—ç­‰å†…å­˜èŠ‚çœä¼˜åŒ–å¯¹å¤§äºåŠ é€Ÿå™¨å†…å­˜å®¹é‡çš„æ¨¡å‹çš„è®­ç»ƒçš„å½±å“ã€‚è¿™äº›å¢åŠ çš„ç»´åº¦å¢åŠ äº†éœ€è¦æ¢ç´¢çš„æœç´¢ç©ºé—´ã€‚

# DISCUSSION AND CONCLUSION

æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ç»„æˆPTD-P(èŠ‚ç‚¹é—´ç®¡é“å¹¶è¡Œï¼ŒèŠ‚ç‚¹å†…å¼ é‡å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œ)æ¥å®ç°é«˜æ€»ååé‡(502 petaFLOP/s)ã€‚ a) the idea of smartly partitioning the model training graph to minimize the amount of communication while still keeping devices active, b) minimizing the number of memorybound kernels with operator fusion and careful data layout, c) other
domain-specific optimizations (e.g., scatter-gather optimization).

pp:å¤šæœº

tp:å•æœºå¤šå¡

Pipeline model parallelism should be used primarily to support the training of large models that do not fit on a single worker, and data parallelism should be used to scale up training

dpå—åˆ°æœ€å¤§batch sizeçš„é™åˆ¶ï¼Œä¸èƒ½æ— é™æ‰©å±•

åŒæ­¥pipedream(microbatch)

äº¤é”™ï¼šä¸€ä¸ªgpuå¯¹åº”å¤šä¸ªstageï¼ˆå¾ªç¯åˆ†é…ï¼‰

åˆ‡åˆ†ä¹˜æ³•ï¼ˆå·¦ä¹˜ä»¥å³åˆ—ã€å·¦åˆ—ä¹˜ä»¥å³è¡Œ+all-reduceï¼‰

