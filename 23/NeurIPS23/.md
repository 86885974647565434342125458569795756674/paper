[NeurIPS 2023 Papers (nips.cc)](https://nips.cc/virtual/2023/papers.html?filter=titles)

showing 400 of 3564 papers



Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time

（推理）保持kv大小：保留一定数量的重要token

Blockwise Parallel Transformer for Large Models

ffn(softmax(QiKj^T)Vj)

Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers

（推理）可学习的注意力mask



QuIP: 2-Bit Quantization of Large Language Models With Guarantees

QLoRA: Efficient Finetuning of Quantized LLMs

BRAM: Communication-Efficient 1-bit Adaptive Optimizer for Practical Distributed DNN Training
