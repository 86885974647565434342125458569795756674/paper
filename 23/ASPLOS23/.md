[Program (asplos-conference.org)](https://www.asplos-conference.org/asplos2023/index.html%3Fp=3602.html)



FLAT: An Optimized Dataflow forMitigating Attention Bottlenecks

低操作强度的瓶颈在内存，只能用算子融合，中间变量不载入载出，细粒度保证依赖和长序列，没有参数重用，但还是减少了矩阵乘法的重用，减少并行利用。节省时间和资源，支持长序列

Mobius: Fine Tuning Large-Scale Models on Commodity GPU Servers

每个GPU负责多个阶段的执行(减少通信量)，通过在GPU和DRAM之间交换阶段，预取下一阶段(重叠)，交叉映射减少通信争用

DeepUM Tensor Migration and Prefetching in Unified Memory

um(page fault),预取，预驱逐

Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models

二维划分激活和参数，固定激活、参数大小，需要计算时收集激活和参数

将计算和通信分解成小步骤，交替重叠执行

Optimus-CC Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression

低秩压缩流水线后向激活，保留上个microbatch的压缩误差加在下一个microbatch梯度，再压缩，可以只压缩后面几个microbatch

将embedding的all-reduce+synchronization换成all-reduce

前面的stage最后dp，可以压缩前面的stage的dp
