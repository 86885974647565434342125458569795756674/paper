# .

å—é€šä¿¡ï¼ŒåŠ¨æ€å†…å­˜åˆ†é…ï¼Œé‡ç”¨å—

å¤§æ¨¡å‹

é¢„è®­ç»ƒï¼šå¤§æ•°æ®ã€supercomputers using hundreds of GPU nodes connected with high-speed network fabricã€å°‘æ•°äºº

å¾®è°ƒï¼šå°æ•°æ®ã€a single node equipped with multiple GPU cards

å¼‚æ„(CPU+GPU)è®­ç»ƒæ˜¯é™ä½å¾®è°ƒé˜¶æ®µç¡¬ä»¶è¦æ±‚çš„æœ€æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚

Zero-infinityã€ZeRO-Offloadçš„ç¡¬ä»¶è¦æ±‚é«˜ï¼šDGX-2 supercomputerï¼Œæ¯ä¸ªèŠ‚ç‚¹æä¾›è¶³å¤Ÿçš„å†…å­˜èµ„æºï¼Œå³8x32GB GPUå†…å­˜ï¼Œ1.5TB CPUå†…å­˜ï¼Œ3.84TB NVMe ssdã€‚å†…å­˜é…ç½®è¿œè¿œè¶…è¿‡æ•°æ®ä¸­å¿ƒå’Œäº‘è®¡ç®—å¹³å°çš„å¹³å‡æ ‡å‡†ã€‚å½“åœ¨é€šå¸¸å¯è®¿é—®çš„ç¡¬ä»¶ä¸Šé‡‡ç”¨è¿™äº›ç³»ç»Ÿæ—¶ï¼Œæ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—åˆ°æŸå®³ã€‚

ZeRO-Offloadåœ¨4xV100 GPUçš„DGX-2HæœåŠ¡å™¨ä¸Šçš„æœ€å¤§æ¨¡å‹è§„æ¨¡è¾¾åˆ°äº†30B (Billion)ä¸ªå‚æ•°ï¼Œæ¯ä¸ªGPUçš„ååé‡è¾¾åˆ°äº†30tflopsã€‚ä½†åœ¨4xV100 GPUå’Œ240gb DRAM CPUæœåŠ¡å™¨ä¸Šï¼Œå…¶æœ€å¤§æ¨¡å‹è§„æ¨¡åªèƒ½è¾¾åˆ°6Bä¸ªå‚æ•°ã€‚

è®¡ç®—æ•ˆç‡ä¹Ÿä¸æ˜¯æœ€ä¼˜çš„ã€‚å¯¹äº4xV100 DGX-2HæœåŠ¡å™¨ä¸Šçš„10Bå‹å·ï¼ŒZeRO-Offloadä»…ä½¿ç”¨æœ€å¤§è®¡ç®—èƒ½åŠ›çš„25%ã€‚

æ¨¡å‹æ•°æ®ç”±å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ç»„æˆï¼Œå®ƒä»¬çš„å­˜å‚¨ç”±æ¨¡å‹ç»“æ„å®šä¹‰å…³è”;éæ¨¡å‹æ•°æ®ç”±ç®—å­ç”Ÿæˆçš„ä¸­é—´å¼ é‡ç»„æˆã€‚éæ¨¡å‹æ•°æ®æ ¹æ®è®­ç»ƒä»»åŠ¡çš„é…ç½®(å¦‚æ‰¹å¤„ç†å¤§å°)åŠ¨æ€å˜åŒ–ã€‚

æ¨¡å‹æ•°æ®å’Œéæ¨¡å‹æ•°æ®ç›¸äº’ç«äº‰GPUå†…å­˜ã€‚ç°æœ‰çš„è§£å†³æ–¹æ¡ˆ[35]åœ¨ä¸è€ƒè™‘è¿­ä»£å†…éƒ¨éæ¨¡å‹æ•°æ®é‡å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œå°†æ¨¡å‹æ•°æ®é™æ€åœ°åˆ’åˆ†åœ¨CPUå’ŒGPUå†…å­˜ä¹‹é—´ï¼Œå…¶å†…å­˜å¸ƒå±€å¯¹å„ç§è®­ç»ƒé…ç½®æ˜¯ä¸å˜çš„ã€‚è¿™ç§é™æ€åˆ†åŒºç­–ç•¥ä¼šå¯¼è‡´å‡ ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œå½“GPUæˆ–CPUå†…å­˜ä¸è¶³ä»¥æ»¡è¶³å…¶ç›¸åº”çš„æ¨¡å‹æ•°æ®éœ€æ±‚æ—¶ï¼Œå³ä½¿å½“æ—¶å…¶ä»–è®¾å¤‡ä¸Šè¿˜æœ‰å¯ç”¨çš„å†…å­˜ï¼Œç³»ç»Ÿä¹Ÿä¼šå´©æºƒã€‚ï¼ˆçœ‹çŸ­æ¿ï¼‰

 Second, communication is inefficient when data transferring among different memory spaces in the granularity of the tensor, and CPU-GPU
communication volume can be reduced when placing model data
on the target computing device in advance.ï¼ˆä»¥å¼ é‡ä½œä¸ºæ•°æ®ç§»åŠ¨çš„å•ä½ä¼šå¯¼è‡´é€šä¿¡æ•ˆç‡ä¸è¶³ï¼‰

å°†æ¨¡å‹æ•°æ®å¼ é‡ç»„ç»‡æˆå—ï¼Œè¿™äº›å—æ˜¯å…·æœ‰ç›¸åŒå…ƒç´ å¤§å°çš„è¿ç»­å†…å­˜å—ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ ¹æ®å—çš„å¼ é‡çŠ¶æ€åŠ¨æ€ç¼–æ’å—åœ¨å¼‚æ„å­˜å‚¨ç©ºé—´ä¸­çš„åˆ†å¸ƒï¼Œæ›´åŠ åŠæ—¶ã€‚é€šè¿‡é‡ç”¨ä¸å…±å­˜çš„å—ï¼ŒPatrickStarè¿˜æ¯”SOTA(æœ€å…ˆè¿›çš„)è§£å†³æ–¹æ¡ˆè¿›ä¸€æ­¥é™ä½äº†æ¨¡å‹æ•°æ®çš„å†…å­˜å ç”¨ã€‚

åŸºäºå—çš„é€šä¿¡æ¨¡å¼å¯ä»¥æé«˜CPU-GPUå’Œgpué—´çš„å¸¦å®½åˆ©ç”¨ç‡ã€‚åŠ¨æ€å†…å­˜ç®¡ç†å¯ä»¥æé«˜å†…å­˜æ•ˆç‡ï¼ˆé‡ç”¨ï¼‰ï¼Œå‡å°‘CPU-GPUé€šä¿¡å¼€é”€ï¼ˆæå‰ï¼‰ã€‚

PatrickStarè®­ç»ƒGPT-likeæ¨¡å‹æ˜¯DeepSpeedæœ€å¤§æ¨¡å‹è§„æ¨¡çš„2.27å€å’Œ2.5å€ï¼Œåœ¨ç›¸åŒæ¨¡å‹è§„æ¨¡ä¸‹è®­ç»ƒé€Ÿåº¦æ›´å¿«

PatrickStarå…·æœ‰æ¯”DeepSpeedæ›´é«˜çš„è®¡ç®—å’Œå†…å­˜æ•ˆç‡ï¼Œå¹¶åœ¨8å€gpuä¸Šå®ç°è¶…çº¿æ€§å¯æ‰©å±•æ€§ã€‚

ç”»å‡ºä¸¤åˆ—å¯¹æ¯”

Param fp16ï¼ŒGrads fp16ï¼ŒOptimizer states (OS) momentum fp32 and variance fp32 and param fp32ï¼ŒActivationsï¼ŒTemporary data

æˆ‘ä»¬å°†æ¿€æ´»å’Œä¸´æ—¶æ•°æ®ç§°ä¸ºéæ¨¡å‹æ•°æ®ï¼Œè€Œå°†å…¶ä»–æ•°æ®ç§°ä¸ºæ¨¡å‹æ•°æ®ã€‚

![](f1.png)

å®ƒä»¬åœ¨å¼‚æ„å†…å­˜ç©ºé—´ä¸­é™æ€åœ°ç®¡ç†æ¨¡å‹æ•°æ®ï¼Œä½¿å‚æ•°fp16æ•°æ®å­˜å‚¨åœ¨GPUä¸­ï¼Œè€Œæ¢¯åº¦fp16å’Œæ“ä½œç³»ç»Ÿæ•°æ®å­˜å‚¨åœ¨CPUä¸­ã€‚å‚æ•°fp16å’Œåˆ†çº§fp16æ•°æ®ï¼Œæ€»å…±4ğ‘€å­—èŠ‚ï¼Œåœ¨æ¯æ¬¡è¿­ä»£æœŸé—´åœ¨CPUå’ŒGPUä¹‹é—´ç§»åŠ¨ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒZeRO-Offloadçš„å®éªŒç¯å¢ƒæœ‰1.5TBçš„CPUå†…å­˜ï¼Œæ‰¿æ‹…äº†å¤§éƒ¨åˆ†æ¨¡å‹æ•°æ®çš„å­˜å‚¨ã€‚ç„¶è€Œï¼Œå½“å‡å°‘CPUå†…å­˜çš„æ•°é‡æ—¶ï¼Œæœ€å¤§æ¨¡å‹è§„æ¨¡ä¸‹é™äº†å¾ˆå¤šï¼Œä¾‹å¦‚ï¼Œå°†å…¶éƒ¨ç½²ä¸º240 GB CPUå†…å­˜ï¼Œæœ€å¤§æ¨¡å‹è§„æ¨¡é™ä½åˆ°4Bã€‚éšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§ï¼Œå…¶è®¡ç®—æ•ˆç‡ä¹Ÿéšä¹‹é™ä½ï¼Œè¿™ä¸ºè¿›ä¸€æ­¥æ”¹è¿›æŒ‡æ˜äº†æ–¹å‘ã€‚ä¾‹å¦‚ï¼Œåœ¨1Bæ¨¡å‹ä¸Šå¯ä»¥å®ç°47 tflopï¼Œè€Œåœ¨4Bæ¨¡å‹ä¸Šåªèƒ½å®ç°33 tflopã€‚

ä½¿ç”¨ä¸Šè¿°é™æ€åˆ†åŒºç­–ç•¥ï¼Œä¸æ¨¡å‹ç›¸å…³çš„é…ç½®ä¼šå—åˆ°ä¸ä»»åŠ¡ç›¸å…³çš„é…ç½®çš„å¼ºçƒˆå½±å“ã€‚å¦ä¸€ä¸ªé—®é¢˜æ¥è‡ªå¸¦å®½åˆ©ç”¨ç‡ã€‚ZeRO-Offloadå’ŒL2L[31]ä»¥å¼ é‡çš„ç²’åº¦åœ¨ä¸åŒçš„å†…å­˜ç©ºé—´ä¹‹é—´ä¼ è¾“æ•°æ®ã€‚å½“ç§»åŠ¨å°æ¶ˆæ¯å¤§å°çš„å¼ é‡æ—¶ï¼Œå¯èƒ½ä¼šæµªè´¹å¸¦å®½[19]ã€‚å› æ­¤ï¼Œå¸¦å®½åˆ©ç”¨ç‡ä¸è¶³ä¼šå¯¼è‡´ä¼ è¾“æ•ˆç‡é™ä½å’Œæ‰§è¡Œæ—¶é—´å»¶é•¿ã€‚

åŸºäºä¸Šè¿°ç°è±¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¼‚æ„è®­ç»ƒç³»ç»Ÿè®¾è®¡æ–¹æ¡ˆï¼Œå…¶é‡ç‚¹åœ¨äº1)æé«˜é€šä¿¡å¸¦å®½åˆ©ç”¨ç‡ï¼Œ2)ä¼˜åŒ–å†…å­˜åˆ©ç”¨ç‡ã€‚

![](f3.png)

We propose an efficient chunk-tensor
mapping method with three features: 1) increase the locality of tensor access. 2) reduce peak memory usage. 3) be parallel-friendly

 param fp16 list, param fp32 list, momentum list, and
variance list

å—åŒ…å«ç›¸åŒçš„å…ƒç´ ï¼Œå› æ­¤ç›¸åŒç±»å‹çš„ä¸åŒå—å¯ä»¥é‡ç”¨ç›¸åŒçš„å†…å­˜ç©ºé—´ã€‚

![](t1.png)

COMPUTEçŠ¶æ€è¡¨ç¤ºå¼ é‡å³å°†ç”±ç‰¹å®šè®¡ç®—è®¾å¤‡ä¸Šçš„è¿ç®—ç¬¦è®¡ç®—

HOLD_likeçŠ¶æ€è¡¨æ˜å¼ é‡ç°åœ¨ä¸å‚ä¸è®¡ç®—ï¼Œä½†å®ƒçš„æœ‰æ•ˆè½½è·å¿…é¡»åœ¨å†…å­˜ä¸­ç»´æŠ¤ï¼ŒCPUæˆ–GPUéƒ½å¯ä»¥

å—åœ¨å¼‚æ„ç©ºé—´ä¸­çš„ä½ç½®ç”±å…¶æ‰€æœ‰å¼ é‡çš„çŠ¶æ€å†³å®šã€‚å½“ä¸€ä¸ªå—çš„æ‰€æœ‰å¼ é‡éƒ½å¤„äºFREEçŠ¶æ€æ—¶ï¼Œè¯¥å—çš„å†…å­˜ç©ºé—´å¯ä»¥è¢«å…¶ä»–å—é‡ç”¨æˆ–é‡Šæ”¾ã€‚å¦‚æœå—çš„ä»»ä½•å¼ é‡å¤„äºCOMPUTEçŠ¶æ€ï¼Œåˆ™è¯¥å—å¿…é¡»ä½äºæ‰€éœ€çš„è®¡ç®—è®¾å¤‡ä¸Šã€‚å¦‚æœå®ƒçš„å¼ é‡éƒ½ä¸åœ¨COMPUTEä¸­ï¼Œå¹¶ä¸”è‡³å°‘æœ‰ä¸€ä¸ªå¼ é‡å¤„äºç±»ä¼¼holdçš„çŠ¶æ€ï¼Œåˆ™å—å¯èƒ½ä½äºå¼‚æ„å†…å­˜ç©ºé—´çš„ä»»ä½•ä½ç½®ã€‚

å‚æ•°fp16å¼ é‡åœ¨åˆå§‹åŒ–(éšæœºåˆå§‹åŒ–æˆ–ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½)åçŠ¶æ€è¢«è®¾ç½®ä¸ºHOLDã€‚

algo1

åœ¨æ“ä½œç¬¦FWDè®¡ç®—å¼€å§‹ä¹‹å‰ï¼ŒPatrickStarä½¿ç”¨ç®—æ³•1ä»å—ä¸­è®¿é—®å¼ é‡ã€‚åœ¨ç¬¬24è¡Œä¸­ï¼ŒåŒ…å«è¦è®¡ç®—çš„å‚æ•°fp16çš„å—å¿…é¡»é©»ç•™åœ¨è®¡ç®—è®¾å¤‡ä¸Šã€‚å¦‚æœæ²¡æœ‰ï¼Œå—ç®¡ç†å™¨å°†æŠŠè¯¥å—ä»å…¶ä»–è®¾å¤‡ç§»åŠ¨åˆ°è®¡ç®—è®¾å¤‡ï¼Œå¦‚æœè®¡ç®—è®¾å¤‡çš„å†…å­˜å·²æ»¡ï¼Œåˆ™å¯èƒ½ä¼šåœ¨è®¡ç®—è®¾å¤‡ä¸Šé©±é€ä¸€ä¸ªå—ã€‚æœ€åå°†å¼ é‡çš„çŠ¶æ€è½¬æ¢ä¸ºCOMPUTE(ç¬¬30è¡Œ)

After the computing
is finished, we release tensor using Algorithm 2 by setting training_stage as FWD and target_state as HOLD_AFTER_FWD.å¼ é‡çŠ¶æ€å˜ä¸ºHOLD_AFTER_FWDï¼Œå› æ­¤å¼ é‡å¯ä»¥åœ¨å¿…è¦æ—¶è¢«é©±é€åˆ°å…¶ä»–è®¾å¤‡ã€‚

å¼ é‡-å—-å­˜å‚¨

å½“æ¨¡å‹çš„æ‰€æœ‰ç®—å­å®ŒæˆFWDåï¼Œæ‰€æœ‰å‚æ•°fp16å¼ é‡çš„çŠ¶æ€è¢«é‡ç½®ä¸ºHOLDï¼Œä»¥ç¡®ä¿BWDçš„æ­£ç¡®æ‰§è¡Œã€‚

å¯¹äºBWD, BWDç®—å­çš„è¾“å…¥æ˜¯æ¿€æ´»å’Œå‚æ•°fp16å¼ é‡ï¼Œè¾“å‡ºæ˜¯æ¿€æ´»å’Œæ¢¯åº¦fp16å¼ é‡ã€‚

how to reuse param fp16
chunk for grad fp16 tensors

![](f5.png)

åœ¨BWDè®¡ç®—ä¹‹å‰ï¼Œæ“ä½œå‘˜è®¿é—®å‚æ•°fp16å¼ é‡å¹¶å°†å…¶çŠ¶æ€æ›´æ”¹ä¸ºCOMPUTEã€‚åœ¨BWDè®¡ç®—è¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆçš„æ¢¯åº¦fp16å¼ é‡è¢«åˆ†é…åˆ°ä¸´æ—¶å­˜å‚¨ç©ºé—´ä¸­

åå‘åï¼Œç”±äºä¸å†éœ€è¦å‚æ•°fp16ï¼Œæˆ‘ä»¬å°†grad fp16æ•°æ®ä»ä¸´æ—¶å†…å­˜ç©ºé—´å¤åˆ¶åˆ°å¯¹åº”çš„å‚æ•°fp16å¼ é‡çš„å†…å­˜ç©ºé—´ï¼Œå¹¶å°†å¼ é‡çŠ¶æ€æ›´æ”¹ä¸ºHOLD_AFTER_BWDã€‚ï¼ˆä¸€ä¸ªç®—å­ä¸€ä¸ªç®—å­çš„æ¢å—ï¼Œè¿˜æ˜¯å…¨éƒ¨ç®—å®Œæ¢ï¼Œå…¨éƒ¨ç®—å®Œæ²¡æœ‰æ„ä¹‰ï¼Œä¸èƒ½å‡å°‘å­˜å‚¨å³°å€¼ï¼‰è€ƒè™‘åˆ°å‚æ•°å¯ä»¥ç”±å¤šä¸ªæ“ä½œç¬¦å…±äº«ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼•ç”¨è®¡æ•°å™¨æ¥æŒ‡ç¤ºå¼ é‡æœ€åä¸€æ¬¡è¢«è®¿é—®ã€‚ï¼ˆæœ€åä¸€æ¬¡è®¿é—®ç»“æŸåå°±æ¢ï¼‰

åœ¨ADAMè®¡ç®—ä¹‹å‰ï¼ŒOSå¼ é‡(å‚æ•°fp32ã€åŠ¨é‡å’Œæ–¹å·®)è¢«è®¾ç½®ä¸ºCOMPUTEã€‚åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œä¸ºäº†èŠ‚çœå†…å­˜ï¼Œfp16å—è¢«å®æ—¶è½¬æ¢ä¸ºfp32å—ã€‚ï¼ˆæ€ä¹ˆè½¬ï¼Œæ”¾å“ªé‡Œï¼‰è®¡ç®—åï¼Œæ›´æ–°çš„å‚æ•°fp32å¼ é‡å’Œä½¿ç”¨çš„OSå¼ é‡è¢«è®¾ç½®ä¸ºHOLDã€‚å½“å‚æ•°fp32å—ä¸­çš„æ‰€æœ‰å¼ é‡éƒ½å¤„äºHOLDçŠ¶æ€æ—¶ï¼Œå‚æ•°fp32å—è¢«å¤åˆ¶åˆ°ç›¸åº”çš„å‚æ•°fp16å—ä¸­ã€‚ï¼ˆé€å—å¤åˆ¶ï¼‰

![](f6.png)

PatrickStar uses ZeRO-DP [33] to scale training to multi-GPU via
multiple-processing.

 ZeRO-DP
reduces memory requirements by ğ‘›ğ‘ğ‘Ÿğ‘œğ‘ times compared to DP by
keeping the 1/ğ‘›ğ‘ğ‘Ÿğ‘œğ‘ of the total chunks in its local memory space.

A communication group consists of ğ‘›ğ‘ğ‘Ÿğ‘œğ‘ continuous chunks of a chunk
list, where each chunk belongs to a different process.(ä¸ºä»€ä¹ˆæ˜¯è¿ç»­çš„ï¼Œä¸ºä»€ä¹ˆæ¯äººåªä¼ ä¸€ä¸ª)ï¼ˆç±»ä¼¼zero-infinityï¼Œæ¯ä¸ªæ•°æ®å•å…ƒéƒ½å¹³åˆ†äº†ï¼Œä¸€ä¸ªæ•°æ®å•å…ƒå°±æ˜¯ç»„ï¼Œç”±nå—ç»„æˆï¼Œæ¯äººä¸€å—ï¼‰

Before computing,
a process may fetch the remote chunks from the other processes; After computation of the communication group, the process releases
remote chunks. 

the processes only need to communicate the param
fp16 and grad fp16 chunks during the FWD and BWD stages.

![](f7.png)

Chunk0 consists of param fp16 tensors of layer 0-3 and belongs
to Proc#0.

 Before FWD computing of layer 0, all processes
have found remote chunks are not resident in local memory space.

 After
collecting remote chunks, all processes will have their copy of
chunk 0 to chunk 2, and all tensor states in the remote chunks
are set to HOLD.(è™½ç„¶ç¼º0ï¼Œä½†all gatheråæœ‰äº†0-2)

When the states of all tensors in a communication group are all HOLD_AFTER_FWD/BWD after FWD/BWD
operator computing finished, the tensors in the remote chunks are
set to FREE, and the remote chunks are released. During FWD,
we set training_stage as FWD and is_allreduce as False. During
BWD, we set training_stage as BWD and is_allreduce as True. A
reduce-scatter operation distributes the reduced gradients among
processes.

When scaling to multi-node, like over 16 GPUs, maintaining a chunk list as the
buffer for collective communication consumes excessive memory.
To reduce memory consumption, we sperate a collective operation into ğ‘›ğ‘ğ‘Ÿğ‘œğ‘ serial broadcast and reduction operations so that
only one chunk is kept as the communication bufferï¼ˆä¼ 3ä¸ªå—ï¼Œä¸€æ¬¡ä¼ ä¸€ä¸ªï¼Œåªç”¨ä¸€ä¸ªbuffer)

It
rearranges model data layout around heterogeneous spaces before
operator execution. 

![](f8left.png)

(grad16æ˜¯ç”Ÿæˆåç«‹å³ä¼ åˆ°cpuå—ï¼Œgpuæ²¡æœ‰æ˜¾ç¤º)ï¼ˆæ˜¾å­˜å ç”¨å¢åŠ äº†ï¼Œæ²¡æœ‰checkpointå’Œoffloadå—æ¿€æ´»ç”±å¤šåˆ°å°‘ï¼‰ï¼ˆcpuè¿˜å­˜å‚¨p16?å…ˆç”Ÿæˆå†ä¼ è¾“ï¼Ÿ)ï¼ˆADAMçš„å‚æ•°ä¼ åˆ°gpuæ˜¯ä¸ºä»€ä¹ˆï¼Œåªä¼ å‚æ•°æ²¡æœ‰os?ï¼‰

When the
param fp16 and non-model data exceed the GPU memory capacity, data not recently used can be dynamically evicted to the CPU.

å¼‚æ„è®­ç»ƒ[31,35]é€šå¸¸ä¼šå¼•å…¥é¢å¤–çš„CPU-GPUæ•°æ®ç§»åŠ¨å¼€é”€ã€‚DMMç”±ä¸‰ä¸ªåˆ›æ–°ç»„æˆï¼Œå¯ä»¥è¿›ä¸€æ­¥å°½å¯èƒ½åœ°å‡å°‘å¼€é”€ã€‚

DMMé€šè¿‡è¿è¡Œæ—¶å†…å­˜è·Ÿè¸ªå™¨è·å¾—éæ¨¡å‹æ•°æ®å†…å­˜çš„ç»Ÿè®¡ä¿¡æ¯ã€‚

åŠ¨æ€æ¡†æ¶PyTorchæ²¡æœ‰æä¾›åœ¨æ‰§è¡Œä¹‹å‰æ•è·æ¯ä¸ªæ¿€æ´»å¼ é‡çš„ç”Ÿå‘½å‘¨æœŸçš„æ¥å£ã€‚ç”Ÿå‘½å‘¨æœŸå½¼æ­¤ä¸é‡å çš„æ¿€æ´»å¼ é‡å¯èƒ½é‡ç”¨ç›¸åŒçš„å†…å­˜ç©ºé—´[18,30]ã€‚

Linearè¿™æ ·çš„è®¡ç®—å¯†é›†å‹è¿ç®—ç¬¦å¿…é¡»åœ¨GPUä¸Šæ‰§è¡Œã€‚å†…å­˜å¯†é›†å‹è¿ç®—ç¬¦ï¼Œå¦‚ADAMä¸­çš„å…ƒç´ è¿ç®—ç¬¦ï¼Œå¯ä»¥åœ¨CPUå’ŒGPUä¸ŠåŒæ—¶æ‰§è¡Œã€‚æ³¨æ„ï¼Œå†…å­˜å¯†é›†å‹æ“ä½œç¬¦åªå æ€»æ—¶é—´çš„ä¸€å°éƒ¨åˆ†ã€‚å®ƒä»¬çš„æ™ºèƒ½å¸ƒå±€å¯ä»¥é€šè¿‡å‡å°‘CPU-GPUé€šä¿¡é‡æ¥æé«˜ç«¯åˆ°ç«¯çš„æ€§èƒ½ï¼Œå³ä½¿å®ƒç¨å¾®å¢åŠ äº†å®ƒä»¬è‡ªå·±éƒ¨åˆ†çš„è¿è¡Œæ—¶é—´ã€‚

GPU
margin space is the remaining space after removing peak nonmodel data and param fp16 from the overall GPU memory. The
peak non-model data volume is tracked by the memory tracer. We
place as many OS tensors in the margin space as possible. This way,
CPU-GPU communication volume of ADAM is reduced without
introducing extra model-data eviction in FWD+BWD.ï¼ˆä¸éœ€è¦ä¼ è¾“å…¨éƒ¨æ¢¯åº¦åˆ°cpuï¼Œä¹Ÿä¸éœ€è¦ä¼ è¾“å…¨éƒ¨å‚æ•°åˆ°gpuï¼‰

![](f9.png)

 At the time an operator starts, which is called as moment ğ‘– here

DMM can also accurately know the volume of model-data memory ğ¶ğ‘– in use

Assuming the volume of model data not in GPU but required(Ciæ˜¯å…±ç”¨çš„ï¼Ÿ)
by the current operator is ğ‘‚ğ‘—
, then if ğ¶ğ‘—+ğ‘‚ğ‘—
is larger than ğ´ğ‘—+1,
ğ´ğ‘—+1-ğ¶ğ‘—-ğ‘‚ğ‘— of data has to be evicted to CPU.ï¼ˆè¢«å‡æ•°å†™åäº†ï¼Ÿï¼‰

ä¼˜å…ˆé©±é€çš„å—ä»…ç”±ç±»ä¼¼holdçš„å¼ é‡ç»„æˆã€‚

 In the warm-up iteration, the tracer records a list of moments related to each chunk.ï¼ˆè®°å½•æ¯ä¸ªå—æ¯ä¸ªæ—¶åˆ»çš„ä½¿ç”¨æƒ…å†µï¼‰A greedy algorithm evicts the longest future reference chunk on
this computing device

 It is implemented in ğ‘‚(ğ‘™ğ‘œğ‘”ğ‘‡ğ¶) by traversing the moment list of all chunks and binary searching the next
moment to be used, where ğ¶ is the chunk count, and ğ‘‡ is the moment count.ï¼ˆæ¯ä¸ªæ—¶åˆ»çš„å—æŒ‰ä¸‹æ¬¡ä½¿ç”¨æ—¶åˆ»æ’åºï¼ŒäºŒå‰æœç´¢ï¼ŒTä¸ªæ—¶åˆ»ï¼Ÿï¼Ÿï¼‰ Our proposed strategy can be viewed as Beladyâ€™s OPT
algorithm [3, 23], which replaces the buffer page with the longest
future reference distance.

![](f8left.png)

(adamå·¦è¾¹è“è‰²æ˜¯æ¢¯åº¦ï¼Ÿï¼Œå³è¾¹ä¸ºä»€ä¹ˆæ²¡æœ‰å®Œå…¨ä¼ åˆ°cpu?æ€ä¹ˆæ›´æ–°osï¼Œåˆ°åº•è“è‰²æ˜¯ä»€ä¹ˆï¼Ÿ)

![](f8right.png)

, the margin space on GPU can accommodate unsatisfied OS
chunks to make the system work smoothly and efficiently

