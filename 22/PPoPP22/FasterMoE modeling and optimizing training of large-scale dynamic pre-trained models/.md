# Abstract

æ··åˆä¸“å®¶(MoE)æ˜¯ç›®å‰æœ€æµè¡Œçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒä½¿å‚æ•°è¶…è¿‡ä¸‡äº¿å°ºåº¦çš„æ¨¡å‹çš„è®­ç»ƒæˆä¸ºå¯èƒ½ã€‚å®ƒå…è®¸æ›´å¤§æ¨¡å‹çš„ç¨€ç–è®­ç»ƒï¼Œæ¶ˆé™¤æ¨¡å‹å¤§å°å’Œè®¡ç®—ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚ä¸ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸åŒï¼Œå®ƒå¯¹è¿™äº›è®­ç»ƒç³»ç»Ÿçš„æ•ˆç‡æå‡ºäº†å·¨å¤§çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŠ¨æ€è´Ÿè½½ä¸å¹³è¡¡ã€ä½æ•ˆçš„åŒæ­¥æ‰§è¡Œæ¨¡å¼ã€æ‹¥å¡çš„å…¨å¯¹å…¨é€šä¿¡ã€‚

æˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§æ€§èƒ½æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¢å¯ä»¥å‡†ç¡®é¢„æµ‹ç‰¹å®šè®­ç»ƒä»»åŠ¡çš„ä¸åŒæ“ä½œçš„å»¶è¿Ÿï¼Œåˆå¯ä»¥é€šè¿‡ä¸€ç§æ–°çš„ç±»ä¼¼å±‹é¡¶çº¿çš„æ¨¡å‹ç›´è§‚åœ°åˆ†æå…¶ç«¯åˆ°ç«¯æ€§èƒ½ã€‚ç„¶åï¼Œåœ¨è¯¥æ¨¡å‹çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬å‘æ˜äº†ä¸€ç§åŠ¨æ€é˜´å½±æ–¹æ³•æ¥å¤„ç†è´Ÿè½½ä¸å¹³è¡¡ï¼Œä»¥åŠä¸€ç§æ™ºèƒ½çš„ç»†ç²’åº¦è°ƒåº¦ï¼Œå°†ä¸åŒçš„æ“ä½œåˆ†å¼€å¹¶å¹¶å‘æ‰§è¡Œã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é¿å…æ‹¥å¡çš„ä¸“å®¶é€‰æ‹©ç­–ç•¥ï¼Œåœ¨å…è®¸ä¿®æ”¹ä¸“å®¶é€‰æ‹©çš„æƒ…å†µä¸‹ï¼Œä»¥è¾ƒä½çš„è¿­ä»£å»¶è¿Ÿç¼“è§£ç½‘ç»œæ‹¥å¡ã€‚

å®ç°é«˜æ•ˆçš„åˆ†å¸ƒå¼MoEæ¨¡å‹è®­ç»ƒã€‚ä¸æœ€å…ˆè¿›çš„å¤§å‹æ¨¡å‹ç³»ç»Ÿ(åŒ…æ‹¬ZeRO, GShardå’ŒBASE Layer)ç›¸æ¯”ï¼Œå®ƒå®ç°äº†1.37Ã— - 17.87Ã—çš„åŠ é€Ÿã€‚

# Introduction

ç ”ç©¶è¡¨æ˜ï¼Œæ›´å¤§çš„æ¨¡å‹å¯ä»¥å¸¦æ¥æ›´é«˜çš„ç²¾åº¦[1,5,11]ã€‚

åœ¨è¿™äº›å·¥ä½œä¸­ï¼Œmix -of- expert (MoE)[17]ä¼¼ä¹æœ‰æœ›å°†æ¨¡å‹ç¼©æ”¾åˆ°æç«¯å°ºå¯¸ã€‚è®­ç»ƒæ ·æœ¬è¢«è¾“å…¥ä¸åŒçš„ä¸“å®¶ï¼Œç”±ä¸€ä¸ªè½»é‡çº§å¯è®­ç»ƒé—¨ç½‘ç»œåŠ¨æ€é€‰æ‹©ã€‚åœ¨MoEä¸­ï¼Œç”±äºä¸“å®¶æ˜¯ç¨€ç–æ¿€æ´»çš„ï¼Œå¹¶ä¸”èŠ‚çœäº†å¤§é‡çš„é¢å¤–è®¡ç®—ï¼Œä¸ç»å…¸ç›¸æ¯”ï¼Œå®ƒå¯ä»¥æ˜¾è‘—å¢åŠ åŒä¸€æ—¶é—´å†…è®­ç»ƒçš„æ ·æœ¬æ•°é‡ï¼Œæé«˜æ¨¡å‹çš„ç²¾åº¦

å°½ç®¡çµæ´»çš„MoEç»“æ„ä½¿å¾—è®­ç»ƒä¸€ä¸ªè¶…è¿‡ä¸‡äº¿è§„æ¨¡çš„å·¨å‹æ¨¡å‹å˜å¾—æ›´åŠ å¯è¡Œï¼Œä½†å®ƒä»ç„¶éå¸¸æ˜‚è´µã€‚ä¸ºäº†å‡å°‘è®­ç»ƒæ—¶é—´ï¼Œå¼•å…¥ä¸“å®¶å¹¶è¡Œæ¥åˆ†å¸ƒå¼è®­ç»ƒMoEæ¨¡å‹ï¼Œå…¶ä¸­ä¸“å®¶è¢«åˆ’åˆ†åˆ°ä¸åŒçš„workerä¸Šï¼Œæ¯ä¸ªworkerå¤„ç†ä¸åŒæ‰¹æ¬¡çš„è®­ç»ƒæ ·æœ¬(è¯¦è§2.3èŠ‚)ã€‚

ç°æœ‰MoEè®­ç»ƒæ–¹æ³•çš„ä½æ•ˆç‡ä¸»è¦æ¥è‡ªäºåŠ¨æ€çš„ä¸“å®¶é€‰æ‹©å’Œçµæ´»çš„MoEç»“æ„ã€‚

åŠ¨æ€ä¸“å®¶é€‰æ‹©ã€‚éšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œä¸“å®¶é€šå¸¸åˆ†å¸ƒåœ¨ä¸åŒçš„å·¥ä½œäººå‘˜ä¸­ã€‚

å—æ¬¢è¿çš„ä¸“å®¶æ¯”å…¶ä»–äººæ¥æ”¶æ›´å¤šçš„ä»¤ç‰Œï¼Œè¿™å¯¼è‡´å…¶å¸¸é©»å·¥ä½œäººå‘˜è´Ÿè½½è¿‡é‡ï¼Œè€Œå…¶ä»–å·¥ä½œäººå‘˜å¯èƒ½ç©ºé—²ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œè¿™ç§æ¨¡å¼ä¼šåœ¨ä¸åŒçš„è¿­ä»£ä¸­åŠ¨æ€å˜åŒ–ã€‚è¿™ç§è¡Œä¸ºä¼šæ˜¾è‘—å½±å“ç¡¬ä»¶åˆ©ç”¨ç‡å’Œè®­ç»ƒæ•ˆç‡ã€‚

ä½æ•ˆçš„åŒæ­¥æ“ä½œã€‚æ‰€æœ‰ä¸“å®¶éƒ½éœ€è¦ä»è®¸å¤šå…¶ä»–å·¥ä½œäººå‘˜é‚£é‡Œè·å¾—ä»–ä»¬çš„è¾“å…¥ï¼Œè¿™æ˜¯è®­ç»ƒMoEæ¨¡å‹æ—¶æœ€è€—æ—¶çš„æ“ä½œä¹‹ä¸€ã€‚It is commonly implemented as synchronous allto-all operations with variable message sizes.è€ƒè™‘åˆ°ä¸ç»Ÿä¸€çš„ä¸“å®¶é€‰æ‹©ä¼šå¯¼è‡´è®¡ç®—å’Œé€šä¿¡çš„ä¸¥é‡ä¸å¹³è¡¡ï¼Œè¿™ç§å¯åŠ¨åŒæ­¥æ“ä½œçš„æ–¹æ³•ä¼šå¯¼è‡´æ›´å¤šçš„å¼€é”€

æ¨¡å‹è®¾è®¡ä¸ç½‘ç»œæ‹“æ‰‘ä¸åŒ¹é…ã€‚åœ¨MoEæ¨¡å‹è®­ç»ƒä¸­ï¼Œä¸“å®¶é€‰æ‹©å†³å®šäº†è´Ÿè½½å‡è¡¡å’Œé€šä¿¡æµé‡ï¼Œå¯¹è®­ç»ƒæ•ˆç‡æœ‰å¾ˆå¤§å½±å“ã€‚GShard[11]å’ŒBASE Layer[12]ç­‰ç°æœ‰å·¥ä½œä½¿ç”¨ä¸åŒçš„ä¸“å®¶é€‰æ‹©ç­–ç•¥æ¥å¹³è¡¡è®¡ç®—è´Ÿè½½ï¼Œä½†å¿½ç•¥äº†é€šä¿¡ï¼Œå°½ç®¡ç½‘ç»œæ‹“æ‰‘å¯¹é€šä¿¡æ€§èƒ½è‡³å…³é‡è¦ã€‚åœ¨å½“å‰å¹¿æ³›ä½¿ç”¨çš„ç½‘ç»œæ‹“æ‰‘ç»“æ„ä¸­ï¼Œç”±äºç§»åŠ¨é€šä¿¡çš„å¤æ‚æ€§ï¼Œç»å¸¸å¼•èµ·ç½‘ç»œäº‰ç”¨ã€‚

ï¼Œæˆ‘ä»¬æå‡ºäº†fastmoeï¼Œä¸€ä¸ªé«˜æ•ˆçš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œç”¨äºè®­ç»ƒå¤§å‹åŠ¨æ€é¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸ºäº†æ•æ‰ç”±MoEå¼•å…¥çš„åŠ¨æ€è¡Œä¸ºï¼Œæˆ‘ä»¬ä¸ºè®­ç»ƒä»»åŠ¡å»ºç«‹äº†ç²¾ç¡®çš„æ€§èƒ½æ¨¡å‹ã€‚ç»™å®šä¸€ä¸ªMoEæ¨¡å‹å’Œç³»ç»Ÿé…ç½®ï¼Œæˆ‘ä»¬çš„æ€§èƒ½æ¨¡å‹å¯ä»¥é¦–å…ˆä¼°è®¡æ“ä½œçš„å»¶è¿Ÿï¼Œç„¶åä½¿ç”¨ç±»ä¼¼å±‹é¡¶çº¿çš„æ¨¡å‹å°†ä»»åŠ¡å¯è§†åŒ–ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£å…¶æ€§èƒ½ã€‚åœ¨ç»©æ•ˆæ¨¡å‹çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†åŸ¹è®­è¿‡ç¨‹çš„ä¸‰ä¸ªå…³é”®ä¼˜åŒ–ç­–ç•¥ã€‚ä¸ºäº†å‡å°‘ä¸“å®¶é€‰æ‹©ä¸å¹³è¡¡é€ æˆçš„ç©ºè½¬ï¼Œé‡‡ç”¨äº†åŠ¨æ€é˜´å½±ã€‚å¼•å…¥ç»†ç²’åº¦æ™ºèƒ½è°ƒåº¦ç­–ç•¥ï¼Œå®ç°è®¡ç®—å’Œé€šä¿¡æ“ä½œçš„å¼‚æ­¥æ‰§è¡Œï¼Œå……åˆ†åˆ©ç”¨äº†æ“ä½œé—´çš„å¹¶è¡Œæ€§ã€‚æœ€åï¼Œè®¾è®¡äº†ä¸€ç§é¿å…æ‹¥å¡çš„ä¸“å®¶é€‰æ‹©ç­–ç•¥æ¥é™ä½è¿­ä»£å»¶è¿Ÿï¼Œæ”¶æ•›æ•ˆæœè‰¯å¥½ã€‚

ä¸ZeRO Optimizerç›¸æ¯”ï¼ŒFasterMoEå®ç°äº†17.87å€çš„åŠ é€Ÿæå‡[25]ï¼Œå…·æœ‰æ•°å­¦ä¸Šçš„ç­‰æ•ˆæ€§ã€‚åœ¨å…è®¸ä¿®æ”¹ä¸“å®¶é€‰æ‹©çš„æƒ…å†µä¸‹ï¼ŒFasterMoEçš„æ”¶æ•›æ—¶é—´æ¯”GShardå¿«1.37å€ï¼Œæ¯”BASE Layerå¿«2.19å€ã€‚

æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ€§èƒ½æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥å‡†ç¡®åœ°ä¼°è®¡ç»™å®šçš„MoEæ¨¡å‹åœ¨ç‰¹å®šå¹¶è¡Œç­–ç•¥ä¸‹çš„æ€§èƒ½ã€‚

æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç±»ä¼¼å±‹é¡¶çº¿çš„æ¨¡å‹æ¥åˆ†æä¸åŒå¹¶è¡Œæ€§çš„æ€§èƒ½å’Œç†è®ºæé™ï¼Œä»¥åŠæˆ‘ä»¬ä¼˜åŒ–çš„æ”¹è¿›

åœ¨æˆ‘ä»¬çš„æ€§èƒ½æ¨¡å‹çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬å‘æ˜äº†ä¸€ç§åŠ¨æ€é˜´å½±æ–¹æ³•æ¥å‡å°‘ä¸“å®¶å—æ¬¢è¿ç¨‹åº¦çš„å½±å“

æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ™ºèƒ½çš„ç»†ç²’åº¦é€šä¿¡å’Œè®¡ç®—è°ƒåº¦ï¼Œä»¥å…±åŒå‡å°‘å®ƒä»¬çš„å»¶è¿Ÿã€‚

We design an adjusted expert selection strategy at
runtime for faster communication with less congestion,
whereas the loss is decreasing in promising slope.

# Background and Challenges

Transformer [34] is the state-of-the-art structure to process
sequences. Most pre-trained models are based on sequences,

The most time-consuming computation of a
transformer block is general matrix multiplication (GeMM)
that occurs in the MLP layer.

å› ä¸ºç›´è§‰è®¤ä¸ºä¸åŒçš„å°æ¨¡å‹æ˜¯ä¸åŒé¢†åŸŸçš„ä¸“å®¶ï¼Œåªæœ‰åœ¨è¾“å…¥å…¶é¢†åŸŸçš„æ•°æ®æ—¶æ‰èƒ½æ¿€æ´»ã€‚

åœ¨émoeå˜å‹å™¨æ¨¡å‹ä¸­ï¼ŒMLPä¸­æœ‰ä¸¤ä¸ªç›¸é‚»çš„FCå±‚ã€‚å½“æ‰©å¤§æ¨¡å‹å°ºå¯¸æ—¶ï¼Œè¿™äº›å¯†é›†çš„å±‚å˜å¾—å·¨å¤§ï¼Œä½¿GeMMè®¡ç®—è¿‡äºç¹é‡ã€‚åœ¨MoEæ¨¡å‹ä¸­ï¼ŒGeMMçš„æƒé‡çŸ©é˜µæ²¿ç€ä¸€å®šçš„ç»´åº¦è¢«åˆ†å‰²ï¼Œè¿™æ ·æ¯ä¸ªéƒ¨åˆ†ä»ç„¶äº§ç”Ÿç›¸åŒå¤§å°çš„è¾“å‡ºï¼Œè€ŒGeMMçš„è®¡ç®—é‡ä»ç„¶å¾ˆå°ã€‚

MoEå…è®¸åœ¨ä¸å¢åŠ è®¡ç®—é‡çš„æƒ…å†µä¸‹å¢åŠ æ¨¡å‹å‚æ•°ï¼Œä½¿å…¶æˆä¸ºç›®å‰æœ€å¯è¡Œçš„æ–¹æ³•æ¥äº§ç”Ÿæ•°ä¸‡äº¿è§„æ¨¡ç”šè‡³æ›´å¤§çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚

æ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œå’Œä¸“å®¶å¹¶è¡Œæ˜¯åˆ†å¸ƒå¼è®­ç»ƒä¸­å¸¸ç”¨çš„ä¸‰ç§å¹¶è¡Œç­–ç•¥ã€‚(çœ‹æ¥æµæ°´çº¿ä¸è¡Œ)

Workers synchronize gradients globally
and update the model after each iteration. Although there
is no communication within each iteration, the size of the
model must not exceed the capacity of a single worker, making it impossible to scale up to large models.

All workers process the global batch
together, and compute using its corresponding partition of
weight. After each layer, the embedding vectors are aggregated and re-distributed.æ¨¡å‹å¹¶è¡Œæ€§ä¸èƒ½é«˜æ•ˆåœ°æ‰©å±•åˆ°éå¸¸å¤§çš„æ¨¡å‹ï¼Œå› ä¸ºå®ƒå—åˆ°åˆ†åŒºç»´åº¦ï¼ˆåˆ‡å¾—å¤ªç»†è®¡ç®—è®­ç»ƒä½ï¼‰å’Œå±‚ä¹‹é—´å­˜åœ¨çš„å¤§é‡é€šä¿¡å¼€é”€çš„é™åˆ¶ã€‚

ä¸“å®¶è¢«å®‰ç½®åœ¨ä¸åŒçš„å·¥äººèº«ä¸Šï¼Œæ¯ä¸ªå·¥äººæ¥å—ä¸åŒæ‰¹æ¬¡çš„è®­ç»ƒæ ·æœ¬ã€‚å¯¹äºémoeå±‚ï¼Œä¸“å®¶å¹¶è¡Œæ€§çš„è¡Œä¸ºä¸æ•°æ®å¹¶è¡Œæ€§ç›¸åŒã€‚åœ¨MoEå±‚ä¸­ï¼Œåºåˆ—ä¸­çš„ä»¤ç‰Œè¢«å‘é€ç»™ä»–ä»¬æƒ³è¦çš„ä¸“å®¶æ‰€åœ¨çš„å·¥ä½œäººå‘˜ã€‚ä¸æ¨¡å‹å¹¶è¡Œæ€§ç±»ä¼¼ï¼Œæ¯ä¸ªMoEå±‚çš„è¾“å‡ºå†æ¬¡äº¤æ¢ï¼Œä»¥ç»„ç»‡å›åŸå§‹åºåˆ—ï¼Œç”¨äºä¸‹ä¸€å±‚çš„è®¡ç®—ã€‚ç”±äºMoEæ¨¡å‹é€šå¸¸æœ‰è®¸å¤šä¸“å®¶ï¼Œä¸“å®¶å¹¶è¡Œæ€§å¯ä»¥æ¯”æ¨¡å‹å¹¶è¡Œæ€§æ›´å¥½åœ°éšæ¨¡å‹å¤§å°è€Œæ‰©å±•ã€‚

åœ¨ä½¿ç”¨ä¸“å®¶å¹¶è¡Œè®­ç»ƒå˜å‹å™¨æ—¶ï¼Œä¸€ç³»åˆ—çš„æŒ‘æˆ˜æå¤§åœ°å½±å“äº†è®­ç»ƒæ•ˆç‡ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æè¿°è¿™äº›æŒ‘æˆ˜ã€‚

ä¸“å®¶é€‰æ‹©åå·®å¯¼è‡´åŠ¨æ€è´Ÿè½½ä¸å¹³è¡¡ã€‚ä¸“å®¶0æ”¶åˆ°3ä¸ªä»£å¸ï¼Œæ¯”ä¸“å®¶2å¤š3å€çš„å·¥ä½œé‡ã€‚ç»“æœï¼Œworker 2åœ¨ä¸‹ä¸€æ¬¡é€šä¿¡å¼€å§‹ä¹‹å‰ç©ºé—²äº†å¾ˆé•¿æ—¶é—´ï¼Œæ²¡æœ‰å……åˆ†åˆ©ç”¨å…¶å¯ç”¨çš„è®¡ç®—èƒ½åŠ›ã€‚è€ƒè™‘åˆ°è®­ç»ƒæ•°æ®è‡ªç„¶éµå¾ªå€¾æ–œåˆ†å¸ƒï¼Œä¸€äº›ä¸“å®¶æ¯”å…¶ä»–ä¸“å®¶æ›´æœ‰å¯èƒ½è¢«é€‰ä¸­ã€‚

![](f3.png)

åœ¨å‰500æ¬¡è¿­ä»£ä¸­è§‚å¯Ÿåˆ°å¿«é€Ÿå˜åŒ–çš„ä¸å‡åŒ€åˆ†å¸ƒã€‚åœ¨å›¾4aæ‰€ç¤ºçš„MoEå±‚ä¸­ï¼Œä¸“å®¶çš„å—æ¬¢è¿ç¨‹åº¦åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­å˜åŒ–ã€‚å›¾4bæ˜¾ç¤ºäº†å¦ä¸€ä¸ªä¸åŒæ¨¡å‹ä¸­çš„å¦ä¸€å±‚ï¼Œå…¶ä¸­äººæ°”æ›´ç¨³å®šï¼Œä½†ä»ç„¶æœ‰è®¸å¤šä¸å—æ¬¢è¿çš„ä¸“å®¶ã€‚äº‹å®ä¸Šï¼Œæ”¾å¤§å›¾ï¼Œå¯ä»¥çœ‹åˆ°è®¸å¤šå°æ¡çº¹ï¼Œè¡¨æ˜è¿™äº›ä¸“å®¶ä¸å—æ¬¢è¿ï¼Œå°½ç®¡ä»–ä»¬ä»ç„¶å¿ å®åœ°å¤„ç†ç‰¹å®šé¢†åŸŸçš„æ•°æ®ã€‚ä¸æ­¤åŒæ—¶ï¼Œ16ä½ä¸“å®¶ä¸­æœ‰4ä½å¤„ç†äº†çº¦20%çš„ä»£å¸ï¼Œæ˜¯å¹³å‡æ°´å¹³çš„3.2å€ã€‚è¿™ç§åŠ¨æ€è¡Œä¸ºå½±å“äº†ç¡¬ä»¶åˆ©ç”¨ç‡ï¼Œé™ä½äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ï¼Œå› ä¸ºå®ƒæ²¡æœ‰å……åˆ†åˆ©ç”¨å¯ç”¨çš„è®¡ç®—èµ„æºã€‚

![](f4.png)

åŒæ­¥æ‰§è¡Œæ¨¡å¼çš„æ“ä½œæ•ˆç‡å¾ˆä½ã€‚ä¸“å®¶å¹¶è¡Œä¸­çš„å…¨å¯¹å…¨æ“ä½œé€šå¸¸ç”±é€šä¿¡åº“æä¾›çš„åŒæ­¥æ“ä½œç¬¦å®ç°ï¼Œå¦‚MPI[6]æˆ–NCCL[8]ã€‚è€ƒè™‘åˆ°ä¸“å®¶é€‰æ‹©ä¸ç»Ÿä¸€å¯¼è‡´è®¡ç®—å’Œé€šä¿¡çš„ä¸å¹³è¡¡ï¼Œè¿™ç§åŒæ­¥æ‰§è¡Œæ–¹æ³•å¯¼è‡´äº†æ›´é«˜çš„èµ„æºæµªè´¹ã€‚å½“æ‰§è¡Œé€šä¿¡æˆ–è®¡ç®—æ—¶ï¼Œå…¶ä»–ç¡¬ä»¶æœ€ç»ˆæ²¡æœ‰å¾—åˆ°å……åˆ†åˆ©ç”¨ï¼Œè€Œå®ƒä»¬å¯ä»¥ç”¨äºå¤„ç†å…¶ä»–æ“ä½œã€‚ç„¶è€Œï¼Œåˆ†å‰²å…¨å¯¹å…¨é€šä¿¡å¹¶ä¸å®¹æ˜“ï¼Œå› ä¸ºä¸åŒçš„é€šä¿¡å’Œè®¡ç®—ä»»åŠ¡ä¹‹é—´å­˜åœ¨ä¾èµ–å…³ç³»ã€‚å¦‚æœæ•°æ®ä¼ è¾“çš„é¡ºåºè®¾è®¡ä¸å½“ï¼Œå¾ˆå®¹æ˜“å¼•å…¥æ­»é”ã€‚å› æ­¤ï¼Œç¬¬äºŒä¸ªæŒ‘æˆ˜æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°ç»„ç»‡å¹¶è¡Œæ‰§è¡Œçš„é€šä¿¡å’Œè®¡ç®—ä»»åŠ¡ã€‚

ä¸“å®¶å¹¶è¡Œå¯¼è‡´ä¸¥é‡çš„ç½‘ç»œäº‰ç”¨ã€‚

æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†ä¸“å®¶åˆ†é…ä¸ç½‘ç»œæ‹“æ‰‘ä¹‹é—´çš„ä¸å…¼å®¹æ€§ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒåŒæ—¶æ‰§è¡Œå¤šä¸ªé€šä¿¡æ“ä½œï¼Œç”±äºå°‘é‡é¥±å’Œé“¾è·¯ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ Since the expert assignment of the
tokens dictates the load balance and communication path,
performing a smart assignment of tokens can help to lower
the end-to-end latency of training without affecting the quality of the models. ç¬¬ä¸‰ä¸ªæŒ‘æˆ˜æ˜¯å¦‚ä½•è®¾è®¡ä¸€ä¸ªç½‘ç»œæ‹“æ‰‘æ„ŸçŸ¥ä»¤ç‰Œåˆ†é…ç­–ç•¥ï¼Œä»¥é¿å…ä¸¥é‡çš„ç½‘ç»œäº‰ç”¨ã€‚

# Performance Modeling

ä¸ºäº†è¯„ä¼°å’Œåˆ†æè®­ç»ƒä»»åŠ¡çš„æ€§èƒ½ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†åˆ«ä¸ºè®¡ç®—å’Œé€šä¿¡å»ºç«‹æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç±»ä¼¼å±‹é¡¶çº¿çš„æ¨¡å‹æ¥ç ”ç©¶é€šä¿¡å»¶è¿Ÿå’Œè®¡ç®—å»¶è¿Ÿå¦‚ä½•å…±åŒå†³å®šæ•´ä½“è®­ç»ƒæ•ˆç‡ã€‚

![](t1.png)

## Load-aware Computation Modeling

ç°ä»£å¤§è§„æ¨¡è®¡ç®—è®¾å¤‡ï¼Œå¦‚gpuï¼Œå¯¹å¸¸è§„è®¡ç®—è¿›è¡Œäº†é«˜åº¦ä¼˜åŒ–ï¼Œå¦‚GeMMï¼Œå®ç°äº†éå¸¸é«˜çš„æ€§èƒ½ã€‚According
to our measurement, an NVIDIA Tesla V100 GPU can achieve
more than 90% of its peak throughput when running GeMMs for typical model sizes and batch sizes in transformers. æˆ‘ä»¬ç”¨ä¸‹é¢çš„å…¬å¼æ¥é¢„æµ‹å˜å‹å™¨å—ä¸­MLPå±‚çš„æ­£å‘è®¡ç®—å»¶è¿Ÿ(PæŒ‡ä¸€æ¬¡ç®€å•æ“ä½œçš„æ¬¡æ•°)

![](latcomp.png)

(FMAè®¡ç®—ä¸¤ä¸ªæ•°å­—çš„ä¹˜ç§¯å¹¶å°†å…¶æ·»åŠ åˆ°ä¸€ä¸ªç´¯åŠ å™¨ä¸­)([B,H]*[H,aH])

The end-to-end latency is the maximum
latency of each single worker, as all the workers have to
exchange features after computation. As a result, load imbalance in computation is reflected by this formula.

ä¸€ä¸ªæ½œåœ¨çš„é—®é¢˜æ˜¯ï¼Œå¯¹äºé‚£äº›Bğ‘¤éå¸¸å°çš„å·¥äººï¼Œå®ƒå¯èƒ½æ— æ³•å¾ˆå¥½åœ°åˆ©ç”¨å®ƒçš„è®¡ç®—è®¾å¤‡ï¼Œä»è€Œå¯¼è‡´ä¸æ­£ç¡®çš„å»¶è¿Ÿä¼°è®¡ã€‚ç„¶è€Œï¼Œè™½ç„¶æ²¡æœ‰è¾¾åˆ°å³°å€¼æ€§èƒ½ï¼Œä½†è¾ƒå°çš„å˜é‡å˜é‡Bğ‘¤çš„è®¡ç®—å»¶è¿Ÿé€šå¸¸ä¸ä¼šå¤§äºè¾ƒå¤§çš„Bğ‘¤ã€‚ç”±äºå·¨å¤§çš„Bğ‘¤æ§åˆ¶äº†æ•´ä¸ªè®¡ç®—å»¶è¿Ÿï¼Œè¿™ç§é¢„æµ‹çš„ä¸å‡†ç¡®æ€§å¹¶ä¸ä¼šå½±å“å…¶æœ‰æ•ˆæ€§ã€‚

## Topology-aware Communication Modeling

æ ¹æ®LogPæ¨¡å‹[2]ï¼Œé€šä¿¡çš„æ€»å»¶è¿Ÿç”±å¼€é”€å’Œå»¶è¿Ÿç»„æˆã€‚æ¯ä¸ªä»¤ç‰Œçš„ç‰¹å¾å‘é‡ä¸€èˆ¬å¤§äº1024ï¼Œè¯´æ˜ä¼ è¾“æ•°æ®çš„æœ€å°ç²’åº¦å¤§äº4KBã€‚Therefore, we simplify the model by regarding
the overhead in communication as negligible. Bandwidth of
interconnections can be fully utilized if we assume that there
is no congestion.Given that there are commonly multiple
accelerators within a node, each being a worker, we should
not only consider inter-node connection, but also intra-node
connection, such as PCIe, UPI, and NVLink.å‡è®¾ä¸€æ¡é“¾è·¯ğ‘™çš„å•å‘å¸¦å®½ä¸ºğ‘Šğ‘™ï¼Œæµé‡å¤§å°ä¸ºğ‘‡ğ‘™ã€‚é€šä¿¡çš„ç«¯åˆ°ç«¯å»¶è¿Ÿè®¡ç®—å¦‚ä¸‹ã€‚

![](latcomm.png)

ğ‘Šğ‘™å¯ä»¥é€šè¿‡ç¡¬ä»¶è§„æ ¼å’Œæ‰§è¡Œç‚¹å¯¹ç‚¹å¸¦å®½åŸºå‡†æ¥ç¡®å®šã€‚æˆ‘ä»¬å¼ºè°ƒç½‘ç»œæ‹“æ‰‘å›¾æ˜¯æœ‰å‘çš„ã€‚ä¸ºäº†å¾—åˆ°ğ‘‡ğ‘™ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªé“¾æ¥å»ºæ¨¡ä¸ºå›¾ä¸­çš„ä¸€æ¡è¾¹ã€‚ä½¿ç”¨ä¸¤ä¸ªæœ‰å‘è¾¹æ¥è¡¨ç¤ºåŒå·¥é“¾è·¯ï¼Œåˆ†åˆ«è€ƒè™‘ä¸¤ä¸ªæ–¹å‘çš„æµé‡ã€‚å› ä¸ºåœ¨è´Ÿè½½ä¸å‡è¡¡çš„æƒ…å†µä¸‹ï¼Œä¸€æ¡é“¾è·¯çš„ä¸¤ä¸ªæ–¹å‘ä¸Šçš„æµé‡å¯èƒ½ç›¸å·®å¾ˆå¤§ã€‚é“¾è·¯çš„æœ‰æ•ˆå¸¦å®½å¹¶ä¸ç›´æ¥ç­‰äºä¸¤ä¸ªæ–¹å‘éƒ½å¿™æ—¶çš„æœ‰æ•ˆå¸¦å®½ã€‚æ¯æ¡é“¾è·¯ä¸Šçš„æµé‡å–å†³äºç®—æ³•å’Œè·¯ç”±ç­–ç•¥ã€‚ä¸åŒçš„æ“ä½œä½¿ç”¨ä¸åŒçš„æ–¹æ³•æ¥è®¡ç®—æ¯æ¡è¾¹ç¼˜ä¸Šçš„æµé‡ã€‚

All-to-all-v is used to route tokens from its position in the
sequence to their desired experts.Due to the flexibility of expert selection, traffic between each pair of workers is highly
variable. We assume that all-to-all operations simply create
links between all pairs of workers,ï¼ˆé€»è¾‘ï¼‰ and transfer data simultaneously. The path between each pair of workers is calculated
by an algorithm according to the type of topology.ï¼ˆç‰©ç†ï¼‰ For each
pair of workers, the traffic between them is accumulated on
all directed edges along the path.

Applying ring all-reduce [30] on a tensor of size ğ‘† on each
of ğ‘› workers results in having each of them sending a total
of 2ï¼ˆğ‘›âˆ’1ï¼‰/ğ‘›*ğ‘† to its neighbor in a pipeline.

Broadcast and reduce are as regular as all-reduce, utilizing ring connection ï¼ˆæ€ä¹ˆåšï¼Ÿï¼‰and pipeline to lower its latency. But
different from all-reduce, they only send messages of total
size ğ‘† through each link

##  DDL-Roofline Model

æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ (DDL) roolineæ¨¡å‹æ¥æè¿°ç»™å®šé›†ç¾¤ä¸Šç‰¹å®šè®­ç»ƒä»»åŠ¡çš„æ€§èƒ½ã€‚

, we define the ratio of computation-communication ğ‘…ğ¶ğ¶, presenting on the X axis of the DDLRoofline, as follows.ğ¿ğ‘ğ‘¡compå’Œğ¿ğ‘ğ‘¡commåˆ†åˆ«è¡¨ç¤ºæˆ‘ä»¬çš„é¢„æµ‹å™¨ä¼°è®¡çš„è®¡ç®—å’Œé€šä¿¡å»¶è¿Ÿã€‚ğ‘…cc.è¡¨ç¤ºä»»åŠ¡æ˜¯å¦å—è®¡ç®—æˆ–é€šä¿¡çº¦æŸã€‚å½“ğ‘…ğ‘…> 1æ—¶ï¼Œè®¡ç®—æ—¶é—´å ç«¯åˆ°ç«¯å»¶è¿Ÿçš„ä¸»è¦éƒ¨åˆ†ï¼Œå¦åˆ™é€šä¿¡æ—¶é—´å ç«¯åˆ°ç«¯å»¶è¿Ÿçš„å¤§éƒ¨åˆ†ã€‚è¿™ä¸ªæ¯”ç‡æŒ‡ç¤ºäº†åº”ç”¨ä¸åŒä¼˜åŒ–çš„æ–¹å‘ã€‚The variable of the Y axis is ğ‘ƒ, the average computation
throughput of all workers. When training an MoE MLP layer,
it can be calculated as follows.(ä¸ºä»€ä¹ˆæ˜¯12ï¼Ÿ4*3,4çœ‹å‰é¢ï¼Œå‰å‘1æ¬¡ï¼Œåå‘ä¸¤æ¬¡ï¼Œä¸€æ¬¡è®¡ç®—å‚æ•°æ¢¯åº¦ï¼Œä¸€æ¬¡è®¡ç®—è¾“å…¥æ¢¯åº¦)

![](p.png)

in synchronous expert parallelism, we estimate it by ğ¿ğ‘ğ‘¡e2e = 3ğ¿ğ‘ğ‘¡comp + 4ğ¿ğ‘ğ‘¡comm,
as there are in total 3 rounds of computation in both forward
and backward, and 4 rounds of communicationï¼ˆå‰å‘2æ¬¡ï¼Œåå‘2æ¬¡ï¼Œæ²¡æœ‰è€ƒè™‘æ¢¯åº¦åŒæ­¥ï¼Ÿï¼‰. ğ‘ƒ intuitively
reflects the average utilization of all worker devices, and can
also directly indicate the scalability of a system.ğ‘ƒ intuitively
reflects the average utilization of all worker devices, and can
also directly indicate the scalability of a system.

ç†æƒ³æƒ…å†µä¸‹ï¼Œé€šä¿¡å’Œè®¡ç®—åŒæ—¶è¿›è¡Œï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªç±»ä¼¼å±‹é¡¶çº¿çš„æŠ˜çº¿ä½œä¸ºç†è®ºä¸Šç•Œï¼Œå¦‚å›¾5ä¸­çš„å®çº¿æ‰€ç¤ºã€‚è®¡ç®—æ–¹æ³•å¦‚ä¸‹ã€‚

![](pidea.png)

![](f5.png)

In the semi-ideal case, the end-to-end latency is the sum of
communication latency and computation latency. Different
from the original roofline model [37] that depicts a program
on a single device, where memory access and computation
are naturally executed simultaneously, distributed training
programs commonly requires significant optimizations on
the system to execute them at the same time.

ç»™å®šä¸€ä¸ªè®­ç»ƒä»»åŠ¡åŠå…¶å¹¶è¡Œé…ç½®ï¼Œddlroolineæœ‰åŠ©äºæ›´å¥½åœ°ç†è§£æ¨¡å‹çš„è®­ç»ƒååé‡ã€‚

Data parallelism is shown by 2 points on the left part of the
ideal polyline. As synchronizing gradients for an MoE MLP
layer involves performing all-reduce on 2ğ‘ ğ›¼$ğ»^2$
elements, (ä¸¤ä¸ªå‰é¦ˆï¼‰it is too expensive, resulting in a poor ğ‘…ğ¶ğ¶. However, as the
all-reduce may be overlapped with backward computation,
it can move slightly above the semi-ideal curve.

Model parallelism has larger ğ‘…ğ¶ğ¶ as it introduces less communication.(è®¡ç®—ä¹Ÿå°‘äº†ï¼Œè¿™æ€ä¹ˆæ¯”ï¼Ÿ) It performs 2 all-reduce on an embedding matrix of ğµ tokens, totally sized 2ğ‘ ğµğ». Compared to data parallelism, it reduces communication of ğ›¼ğ»/ğµ>1ã€‚But when synchronizing embedding vectors, no other computation can be performed. This characteristic forces model parallelism to be performed synchronously, and stops the point from moving
above the semi-ideal curve.

Expert parallelism introduces more latency on computation than communication, due to load imbalance, so it has
large ğ‘…ğ¶ğ¶ but poor ğ‘ƒ, far below the semi-ideal curve. Optimizations in FasterMoE are also presented in Figure 5. We
indicate their characteristics in DDL-Roofline in the following Section.

 # Model-Guided Optimization Approaches

## Light-Weight Dynamic Shadowing Strategy

In MoE models, popular experts can be selected by more than half the input tokens, causing severe load imbalance, as Figure 4 suggests.

Although the embedding of a single input
token is orders of magnitude smaller than model parameters,
batches of input coming from all other workers may be equal
or even larger than model parameters. Therefore, a tradeoff comes that whether the latency of transferring so many
embedding vectors can be replaced by replicating model
parameters of popular experts.

As shown in Figure 6, some experts are replicated on all
workers, namely shadowed experts, so that their parameters, instead of their input tokens, are transferred over the
network.(é€šä¿¡æœ‰å‡å°‘å—ï¼Ÿ)

![](f6.png)

å®ƒä»¬çš„ç›¸å…³è®¡ç®—åœ¨æœ¬åœ°æ‰§è¡Œï¼Œç›´è§‚åœ°å‡å°‘äº†åŒ…å«çƒ­é—¨ä¸“å®¶çš„å·¥ä½œäººå‘˜çš„è´Ÿè½½ã€‚ç„¶è€Œï¼ŒåŠ¨æ€é˜´å½±æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå› ä¸ºä¸“å®¶çš„å—æ¬¢è¿ç¨‹åº¦éšç€è®­ç»ƒè¿‡ç¨‹çš„å˜åŒ–è€Œå˜åŒ–ï¼Œå¹¶ä¸”æ¯æ¬¡è¿­ä»£æ‰€åšçš„å†³å®šå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚æ­¤å¤–ï¼Œå‚æ•°ä¸èƒ½åƒæ™®é€šçš„åˆ†å¸ƒå¼å†…å­˜ç³»ç»Ÿé‚£æ ·ç¼“å­˜ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½ä¼šæ›´æ–°ï¼Œå¹¶ä¸”éœ€è¦åœ¨æ›´æ–°è¿‡ç¨‹ä¸­å…¨å±€æ”¶é›†æ¢¯åº¦ã€‚å¦‚æœå®ƒä»¬ç¼“å­˜åœ¨æ¯ä¸ªworkerä¸Šï¼Œåˆ™åº”è¯¥åœ¨æ¯æ¬¡è¿­ä»£ä¸­æ›´æ–°å®ƒä»¬ï¼Œä»è€Œå¼•å…¥å¤§é‡é¢å¤–å¼€é”€ã€‚

ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æˆ‘ä»¬çš„æ€§èƒ½æ¨¡å‹æ¥åˆ†ææ˜¯å¦åº”è¯¥åœ¨è¿è¡Œæ—¶è·Ÿè¸ªä¸“å®¶ã€‚æˆ‘ä»¬é¢„æµ‹è®­ç»ƒè¿­ä»£çš„ç«¯åˆ°ç«¯å»¶è¿Ÿæ¥æ£€æŸ¥æ€§èƒ½å¢ç›Šï¼Œå¹¶é‡‡å–ç›¸åº”çš„è¡ŒåŠ¨ã€‚

In the original imbalance situation, the communication
and computation are dominated by a set of popular experts.
We model the workload of a single worker ğ‘¤ from a total of
ğ‘ workers by calculating its batch size ğµğ‘¤ as ğµğ‘¤ =$\sum^ğ‘_{ğ‘–=1} ğ‘‡_{ğ‘–ğ‘¤}$
where ğ‘‡ğ‘–ğ‘¤ tokens are sent from worker ğ‘– to worker ğ‘¤.

A single iteration of training an MLP layer contains 1
GeMM in forward, and 2 in backward to compute the gradient of the inputs. There are 4 rounds of all-to-all communication, 2 in each forward and backward. In a simplified
case where the network has a fixed bandwidth, the training
latency is calculated as follows.

![](latimbl.png)

ä¸ºäº†è·Ÿè¸ªä¸€ä¸ªå—æ¬¢è¿çš„ä¸“å®¶ï¼Œæˆ‘ä»¬å¿…é¡»é¦–å…ˆå°†å…¶å‚æ•°å¹¿æ’­ç»™æ‰€æœ‰å·¥ä½œäººå‘˜ï¼Œç„¶åä½¿ç”¨è·å–çš„æ¨¡å‹åœ¨æœ¬åœ°ä»¤ç‰Œä¸Šè¿è¡Œè®¡ç®—ã€‚åœ¨åå‘é˜¶æ®µï¼Œæ¯ä¸ªworkeråˆ†åˆ«è®¡ç®—å…¶è·å–çš„ä¸“å®¶çš„æ¢¯åº¦ï¼Œç„¶åè¿›è¡Œçº¦ç®€æ“ä½œã€‚æœ€åï¼Œåœ¨æœ€åˆæ”¾ç½®çƒ­é—¨ä¸“å®¶çš„workerä¸Šæ‰§è¡Œå‚æ•°æ›´æ–°æ“ä½œã€‚

In this scenario, the overhead of performing imbalanced
computation is replaced by 2 collective communication operators over 2 parameters of size $ğ›¼ğ»^2$
each. Since multiple
popular experts are being sent to many other workers, load
imbalance is less likely to happen. The latency of shadowing
ğ‘Ÿ models is calculated as follows.

![](latshadow.png)

The first condition suggests that the total overhead of
transferring the input is higher than transferring the model.
The second condition indicates that the reduced computation latency is more than the increased communication overhead. In either case, dynamic shadowing is enabled to reduce
end-to-end latency. Otherwise, the communication is too
expensive and model exchange does not bring benefit to
reducing latency, thus not performed. Such situation typically occurs when the workload is balanced across different
workers

![](f5.png)

Represented by arrow (1) in Figure 5, the latency of computation is shortened thanks to the reduced idling,
resulting in a lower ğ‘…ğ¶ğ¶ and higher ğ‘ƒ.

æˆ‘ä»¬åœ¨æ¯æ¬¡è¿­ä»£çš„è¿è¡Œæ—¶é€‰æ‹©ä¸“å®¶è¿›è¡Œé˜´å½±ã€‚å¯¹æ¯ä¸ªå·¥äººæ‰§è¡Œè½»é‡çº§ç®—æ³•ï¼Œå¦‚ç®—æ³•1æ‰€ç¤ºã€‚ç”±äºçŸ©é˜µğ‘‡æ€»æ˜¯å¿…é¡»åœ¨æ‰€æœ‰å·¥äººä¹‹é—´å¯ç”¨(å…±äº«å˜é‡ï¼‰ï¼Œå› æ­¤æ²¡æœ‰å¼•å…¥é¢å¤–çš„é€šä¿¡ã€‚æ ¹æ®ä¸Šé¢çš„å…¬å¼ï¼Œå®ƒè¿”å›ä¸€ç»„è¦é˜´å½±çš„ä¸“å®¶

ï¼ˆä»è´Ÿè½½æœ€å¤§çš„å¼€å§‹çœ‹ï¼‰

ï¼ˆ6ï¼šåªä¿ç•™æœ¬åœ°ï¼ŒTjiæ˜¯ä»jåˆ°i)

![](algo1.png)

## Asynchronous Fine-Grained Smart Scheduling

As our DDL-Roofline shows, when communication and computation are performed separately, a program cannot move
beyond the semi-ideal curve. Besides, due to the inherently
large amount of communication, increasing ğ‘…ğ¶ğ¶ is hard.

æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ™ºèƒ½è°ƒåº¦æ–¹æ³•ï¼Œå°†ä»»åŠ¡åˆ’åˆ†ä¸ºæ›´å°çš„éƒ¨åˆ†ï¼Œå¹¶åœ¨è€ƒè™‘æ•ˆç‡çš„æƒ…å†µä¸‹é‡æ–°è°ƒåº¦ç»†ç²’åº¦çš„é€šä¿¡å’Œè®¡ç®—æ“ä½œã€‚ç»†ç²’åº¦è°ƒåº¦å…è®¸å¼‚æ­¥æ‰§è¡Œè®¡ç®—å’Œé€šä¿¡ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨ç¡¬ä»¶ï¼Œä»è€Œå®ç°è¶…è¶Šå›¾5ä¸­ç®­å¤´(2)æ‰€ç¤ºçš„åŠç†æƒ³æ›²çº¿çš„é£è·ƒã€‚

In expert parallelism, communication follows a complicated all-to-all pattern. We first split up all-to-all communication by dividing workers into fine-grained groups. We
use a grouped pairwise exchange algorithm [33] to perform
all-to-all. The groups form a ring of size ğ‘›ï¼ˆnä¸ªgroup), and send data to
other groups with a stride increasing from 0 to ğ‘› âˆ’ 1. For
group assignment, we follow a heuristic that closely connected workers are placed in the same group, resulting in
faster connections among workers of the same group. The
group size is jointly determined by connection topology, and
related computation granularity

In either forward or backward stage of an MoE layer, 2
symmetric all-to-all are involved, with computation between
them. We split computation according to the pairwise exchanging, making room for re-organizing them.(?) 3ğ‘› operations are performed in ğ‘› steps, by all workers in the ğ‘› groups.
In step ğ‘—, workers in group ğ‘– perform the following 3 operations, instantiated by an example in Figure 7.(jä»0å¼€å§‹ï¼Œè®¡ç®—è‡ªå·±groupï¼Œå†è®¡ç®—group+1...)

![](sij.png)

![](f7.png)

![](schedule.png)

Recall that the goal of the schedule is to perform the operations in parallel. A communication and a computation
stream are created for each worker to execute different types
of operators. As shown in Figure 8b, in its communication
stream, it first executes ğ‘†ğ‘–,0, ğ‘†ğ‘–,1, . . . , ğ‘†ğ‘–,ğ‘›âˆ’1 and then from ğ‘…ğ‘–,0
to ğ‘…ğ‘–,ğ‘›âˆ’1. Its computation stream executes from ğ¶ğ‘–,0 to ğ¶ğ‘–,ğ‘›âˆ’1.
By performing the operations in parallel, the end-to-end latency is significantly reduced. However, all operations must respect their data dependencies and wait for previous tasks
to be executed before starting itself

We illustrate our approach to minimize overhead with
a two-stream schedule. We make the assumption that the
computation stream is busy most of the time. We highlight
that, in the opposite case that communication takes most of
the time, the optimization would have little effect, according
to the DDL-Roofline. As the computation stream is fully
occupied, the main opportunity for optimization is to reduce
the latency of the first ğ‘† and the last ğ‘…. An example is given
by Figure 8c. As group 2 introduces lower overheadï¼ˆé€šä¿¡ï¼‰ than
group 3, placing it at the last place of the schedule lowers the
end-to-end latency. Note that ğ‘†ğ‘–,0 receives tokens from the
local group, which is expected to be the fastest operation, as
no upper level connection is involved. ğ‘…ğ‘–,ğ‘›âˆ’1 ï¼ˆRi,1ä¹Ÿæ˜¯é˜¿ï¼Ÿï¼‰only exchanges
data with neighbors of group ğ‘–. From a global view, all groups
in step ğ‘›âˆ’1 are organized as a ring, and exchange data along
the ring. This makes the best use of the network bandwidth
among all steps other than step 0. As a result, the fastest 2
operations, i.e. ğ‘†ğ‘–,0 and ğ‘…ğ‘–,ğ‘›âˆ’1, are placed at the first and the
last in the smart schedule, minimizing overhead.

## Contention-Avoiding Expert Selection Strategy

åœ¨MoEæ¨¡å‹ä¸­ï¼Œæœ€ç»ˆç›®æ ‡æ˜¯ç”¨è¶³å¤Ÿå¤šçš„è¾“å…¥æ ·æœ¬è®­ç»ƒæ‰€æœ‰ä¸“å®¶ï¼Œè€Œä¸æ˜¯ç”¨ä»–ä»¬æœ€æœŸæœ›çš„ä¸“å®¶æ¥å¤„ç†æ¯ä¸ªä»¤ç‰Œã€‚ç”±äºæ‹Ÿåˆåˆ†æ•°è¢«ç”¨ä½œåŠ æƒæ¥æ±‡æ€»æ¯ä¸ªä¸“å®¶çš„è¾“å‡ºï¼Œå› æ­¤æ”¹å˜ä¸“å®¶çš„é€‰æ‹©ä¸ä¼šå¼•å…¥æ•°å€¼è¯¯å·®ã€‚GShard[11]å’ŒBASE Layer[12]éƒ½æ”¹å˜äº†ä¸“å®¶é€‰æ‹©ç­–ç•¥æ¥å®ç°ç‰¹å®šçš„ç›®çš„ã€‚ï¼ˆï¼Ÿï¼‰æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸“å®¶é€‰æ‹©ç­–ç•¥å¯ä»¥ä¸è®­ç»ƒç³»ç»ŸååŒè®¾è®¡ä»¥æé«˜æ•ˆç‡ã€‚ç„¶è€Œï¼Œæ¨¡å‹çš„å‡†ç¡®æ€§ä¼šå—åˆ°é€‰æ‹©ç­–ç•¥çš„å½±å“ã€‚ä¸“å®¶ä»¬å¯èƒ½ä¼šå¾—åˆ°ä¸å…¶ä¸“ä¸šçŸ¥è¯†ä¸å¤ªç›¸å…³çš„ä»£å¸ï¼Œä»è€Œå‰Šå¼±å…¶å½±å“åŠ›ã€‚å› æ­¤ï¼Œé™¤äº†ååé‡ä¹‹å¤–ï¼Œtokenå’Œä¸“å®¶ä¹‹é—´æ›´å¥½çš„åŒ¹é…æ˜¯å€¼å¾—èµèµçš„ã€‚

æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ‹“æ‰‘æ„ŸçŸ¥é—¨ï¼Œä»¥è¾ƒä½çš„å»¶è¿Ÿå°†è¾“å…¥å¼•å¯¼åˆ°ä¸“å®¶ã€‚é€šè¿‡è€ƒè™‘ç‰¹å®šç¡¬ä»¶çš„ç½‘ç»œæ‹“æ‰‘ç»“æ„ï¼Œå¯ä»¥æé«˜è®­ç»ƒååé‡ã€‚åœ¨æ ‘çŠ¶æ‹“æ‰‘çš„æ™®é€šé›†ç¾¤ä¸­ï¼Œä¸Šå±‚è¿æ¥çš„å¸¦å®½é€šå¸¸ä½äºæœ¬åœ°è¿æ¥çš„å¸¦å®½ã€‚ä¸å…¶ä»–å¸¸è§„çš„é›†ä½“é€šä¿¡ä¸åŒï¼Œæ‰€æœ‰å¯¹æ‰€æœ‰å¯¼è‡´è¿™äº›è¿æ¥ä¸Šæ›´é«˜çš„äº‰ç”¨ã€‚

Assume that a switch connects ğ‘ nodes with ğ‘€ workers
on each node. The traffic between a worker and the host(wokeræ‰€åœ¨çš„node)
is roughly ğ‘‡ğ‘¤ =(ğ‘€ğ‘ âˆ’1)/(ğ‘€ğ‘)* ğµğ».(æ€»å…±BHï¼Œå‘mn-1ä»½ï¼Œä¸€ä»½ç•™ç»™è‡ªå·±) Meanwhile, traffic across the
network interface of each node is ğ‘‡ğ‘› =ğ‘€ (ğ‘ âˆ’1)/ğ‘*ğµğ»ï¼ˆæ€»å…±MBHï¼Œå‘N-1åˆ†ï¼Œç•™ä¸€ä»½ï¼‰, about ğ‘€Ã— larger than ğ‘‡ğ‘¤.To reduce congestion, we allow up to ğ¿ =ğ‘Šnet/(ğ‘€ğ‘Šlocal)\*ğµ tokens
to be directed to another node. Here, ğ‘Šnet and ğ‘Šlocal denotes the communication bandwidth inter- and intra- nodes, respectively. Specifically, if there are more than ğ¿ tokens whose
best-fit selection is on another node, ğ¿ of them with the highest score are allowed to go. The rest of them are left together
with other tokens to re-select their desired experts within
the local node. The traffic across the network is reduced to
ğ‘Šnet/ğ‘Šlocal\*ğµğ», taking the same time as local communication does.
As a result, congestion of upper-level links is less likely to
happen, and the communication overhead is decreased. With
reduced communication overhead, the model can be trained
for more iterations in the same amount of time. Besides, the
best-fit pairs of expert and token are preserved, reducing the
impact of limited room for selection of others.

è¯·æ³¨æ„ï¼Œå¯¹äºå…¶ä»–ç±»å‹çš„æ‹“æ‰‘ï¼Œåº”è¯¥è®¾è®¡å¦ä¸€ä¸ªä¸“é—¨çš„æ‹“æ‰‘æ„ŸçŸ¥é—¨æ¥æé«˜æ€§èƒ½ã€‚é€šè¿‡åœ¨ç‰¹å®šæ ‘æ‹“æ‰‘ä¸Šæ¼”ç¤ºè¿™ç§æ‹“æ‰‘æ„ŸçŸ¥é—¨ä½œä¸ºå®ä¾‹ï¼Œæˆ‘ä»¬æå€¡ä¸€ç§ååŒè®¾è®¡æ–¹æ³•ã€‚With the guidance of the DDL-Roofline model,
gates with high-throughput can be easily designed, and their
performance patterns can be better understood.

#  Evaluation

johnnyæ˜¯ä¸€ä¸ªåœ¨2ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸Šæœ‰16ä¸ªgpuçš„é›†ç¾¤ã€‚æ¯ä¸ªå·¥ä½œèŠ‚ç‚¹æœ‰8ä¸ªNVIDIA Tesla V100-PCIEå›¾å½¢å¤„ç†å™¨ï¼Œé€šè¿‡PCIeäº¤æ¢æœºè¿æ¥åˆ°2ä¸ªCPUæ’åº§ã€‚

æˆ‘ä»¬åœ¨trevorä¸Šçš„å®éªŒåˆ†é…äº†16ä¸ªèŠ‚ç‚¹çš„64ä¸ªgpuã€‚

åœ¨æ•°æ®å¹¶è¡Œæ–¹æ³•ä¸­ï¼ŒMoEæ¨¡å‹è¢«å¤åˆ¶åˆ°æ‰€æœ‰å·¥äººï¼Œç„¶åç”±ZeRO Optimizerè¿›è¡Œä¼˜åŒ–ã€‚ZeROçš„é˜¶æ®µ3ï¼Œå³åœ¨æ‰€æœ‰workerä¸Šåˆ’åˆ†æ¯ä¸ªå¼ é‡ï¼Œè¢«ç”¨ä½œåŸºçº¿ã€‚We also show the results of stage 1
and 2 to get a wider vision of data parallelism, although they
have larger memory footprint, thus infeasible for the large
models that stage 3 or FasterMoE targets.

ä½¿ç”¨FastMoEçš„ä¸“å®¶å¹¶è¡Œå®ç°è¢«ç”¨ä½œå¦ä¸€ä¸ªåŸºçº¿ï¼Œå®ƒå…·æœ‰ä¸ZeROé˜¶æ®µ3ç›¸ä¼¼çš„å†…å­˜å ç”¨ï¼Œä½†æ¨¡å‹çš„åˆ†åŒºä¸åŒã€‚

ä¸è¿™äº›åŸºçº¿ç³»ç»Ÿç›¸æ¯”ï¼ŒFasterMoEä¸­çš„åŠ¨æ€é˜´å½±å’Œæ™ºèƒ½è°ƒåº¦æ˜¯å¯ç”¨çš„ã€‚

åœ¨å…¶ä»–æœ€å…ˆè¿›çš„ä¸“å®¶å¹¶è¡Œç³»ç»Ÿä¸­ï¼ŒåŒ…æ‹¬GShard[11]å’ŒBASE Layer[12]ï¼Œä¸“å®¶é€‰æ‹©è¢«ä¿®æ”¹ï¼Œç±»ä¼¼äºFasterMoEä¸­çš„æ‹“æ‰‘æ„ŸçŸ¥é—¨ã€‚ä¸åŸå§‹é€‰æ‹©ç›¸æ¯”ï¼Œä¸“å®¶å°†åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œå¯¼è‡´ä¸åŒçš„æ¨¡å‹å‚æ•°ã€‚Therefore, comparison between
FasterMoE and them are made by training MoE-GPT on
johnny cluster, and checking their training loss.

æˆ‘ä»¬åŸºäºFastMoEå®ç°äº†FastMoE[7]ã€‚é€šè¿‡æ‰©å±•å…¶åŠŸèƒ½å®ç°åŠ¨æ€é˜´å½±å’Œæ™ºèƒ½è°ƒåº¦ã€‚åœ¨FastMoEä¸­ï¼Œæ‹“æ‰‘æ„ŸçŸ¥é—¨å’ŒGShardçš„[11]è´Ÿè½½å‡è¡¡ç­–ç•¥éƒ½æ˜¯ä½œä¸ºè‡ªå®šä¹‰é—¨å®ç°çš„ã€‚DeepSpeedâ€™s [27] implementation
of ZeRO Optimizer [25] is used over a single-worker version
of FastMoE. To train models, we use Megatron-LM [20] as a
baseline, altering its MLP module for MoE training. BASE
Layer [12] is implemented by FairSeq [21], and it is used as
a plugin layer

## Overall Speedup

å›¾10æ˜¾ç¤ºäº†fastermoeç›¸å¯¹äºZeROä¼˜åŒ–å™¨ç¬¬3é˜¶æ®µçš„æ€»ä½“åŠ é€Ÿæƒ…å†µã€‚FasterMoEåœ¨ä¸¤ä¸ªé›†ç¾¤ä¸Šåˆ†åˆ«å®ç°äº†6.63å€å’Œ17.87å€çš„åŠ é€Ÿæå‡ã€‚ZeROçš„é˜¶æ®µ1å’Œé˜¶æ®µ2å…·æœ‰æ¯”é˜¶æ®µ3æ›´ä½çš„å»¶è¿Ÿï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰å¯¹å‚æ•°è¿›è¡Œåˆ†åŒºï¼Œè¿™ä¼šå¸¦æ¥æ˜¾è‘—çš„å¼€é”€ã€‚ ZeRO stage 2 is faster than stage 1 for better overlapped backward computation and gradient synchronization, as mentioned in Section 3. However, FasterMoE
still achieves up to 3.94Ã— speedup against ZeRO stage 2.åœ¨æœ¬å®éªŒä¸­ä¹Ÿæµ‹è¯•äº†æœªç»é¢å¤–ä¼˜åŒ–çš„FastMoEï¼Œè¡¨æ˜ä¸“å®¶å¹¶è¡Œæ€§çš„åŸºçº¿æ€§èƒ½ï¼Œå¹¶ä¸”ä¼˜äºZeROé˜¶æ®µ3ã€‚è¿™è¡¨æ˜æ•°æ®å¹¶è¡Œæ€§æœ¬è´¨ä¸Šæ˜¯ä½æ•ˆçš„ï¼Œè¿™åœ¨ddl - roolineä¸­å·²ç»è¯´æ˜äº†ã€‚

![](f10.png)

## Dynamic Shadowing and Smart Scheduling Analysis
è¿™äº›ä¼˜åŒ–æ˜¯å•ç‹¬æµ‹è¯•çš„ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£æ¯ä¸ªä¼˜åŒ–å¸¦æ¥çš„æ€§èƒ½å¢ç›Šã€‚

æˆ‘ä»¬åœ¨ç‰¹å®šè¿­ä»£ä¸­æ£€æŸ¥å»¶è¿Ÿå’Œé˜´å½±ä¸“å®¶ï¼Œå¦‚å›¾11æ‰€ç¤ºã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸“å®¶çš„å—æ¬¢è¿ç¨‹åº¦æ˜¯é«˜åº¦åŠ¨æ€çš„ï¼Œå¹³å‡æœ‰19%çš„ä¸“å®¶è¢«é˜´å½±ã€‚é®è”½ç­–ç•¥ç›¸åº”åœ°æˆåŠŸåœ°å‡å°‘äº†è¿­ä»£çš„å»¶è¿Ÿã€‚å½“1ä¸ªä¸“å®¶è¢«é®è”½æ—¶ï¼Œæœ€å¤§åŠ é€Ÿè¾¾åˆ°1.97å€ã€‚

The theoretical upper bound of the smart scheduleâ€™s speedup
in every layer, calculated as ğ¿ğ‘ğ‘¡comm+ğ¿ğ‘ğ‘¡comp/max{ğ¿ğ‘ğ‘¡comm,ğ¿ğ‘ğ‘¡comp }
, is compared
with the actual speedup in Figure 12.æˆ‘ä»¬å»ºè®®ç†æƒ³çš„åŠ é€Ÿæ˜¯1.71å€ï¼Œæˆ‘ä»¬å®ç°äº†1.42å€ã€‚åœ¨æŸäº›å±‚ï¼Œæˆ‘ä»¬çš„å®ç°è¾¾åˆ°äº†ç†è®ºåŠ é€Ÿçš„99%ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œç”±äºç›¸å¯¹è¾ƒä½çš„å¯åŠ¨å¼€é”€ï¼Œåœ¨è¾ƒå¤§çš„æ¨¡å‹å’Œæ›´å¤šçš„å·¥ä½œäººå‘˜ä¸­ï¼Œå®é™…åŠ é€Ÿæ›´æ¥è¿‘ä¸Šç•Œã€‚

å¦‚å›¾13æ‰€ç¤ºï¼Œæ‰§è¡ŒåŠ¨æ€é˜´å½±ï¼Œjohnnyçš„åŠ é€Ÿå¯è¾¾1.95å€ï¼Œtrevorçš„åŠ é€Ÿå¯è¾¾4.74å€ã€‚

æ™ºèƒ½è°ƒåº¦ä½¿è®­ç»ƒé€Ÿåº¦æé«˜1.40å€ã€‚ä¸¤è€…è”åˆä½¿ç”¨æ—¶ï¼Œjohnnyçš„åŠ é€Ÿé€Ÿåº¦ä¸º2.20å€ï¼Œtrevorçš„åŠ é€Ÿé€Ÿåº¦ä¸º5.72å€ã€‚

## Speedup of the Topology-aware Gate

![](f14t3.png)

GShard, although with the lowest iteration time, takes 2.38Ã—
more steps than a faithful top-2 gate in FastMoE. BASE Layer [12] also takes significantly more steps to converge.
It even takes more time in each iteration due to its strict
matching algorithm and extra communication overhead.å½“ä»…å¯ç”¨åŠ¨æ€é˜´å½±å’Œæ™ºèƒ½è°ƒåº¦æ—¶ï¼Œç”±FasterMoE w/o topoè¡¨ç¤ºã€‚é—¨ï¼ŒæŸè€—æ›²çº¿å‡ ä¹ä¸FastMoEç›¸åŒï¼Œæ¯æ¬¡è¿­ä»£å…·æœ‰1.33å€çš„åŠ é€Ÿã€‚è™½ç„¶æ¯æ¬¡è¿­ä»£çš„å»¶è¿Ÿä¸åƒå…¶ä»–æœ€å…ˆè¿›çš„ç³»ç»Ÿé‚£ä¹ˆä½ï¼Œä½†è¿™äº›ä¼˜åŒ–ä¸ä¿®æ”¹ä¸“å®¶é€‰æ‹©ï¼Œå› æ­¤ä¸ä¼šå¯¼è‡´é¢å¤–çš„æ”¶æ•›æ­¥éª¤ã€‚å½“æ‹“æ‰‘æ„ŸçŸ¥é—¨ä¸åŠ¨æ€é˜´å½±å’Œæ™ºèƒ½è°ƒåº¦ä¸€èµ·å¯ç”¨æ—¶ï¼Œç”±FasterMoE w/ topoè¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿­ä»£é€Ÿåº¦å¿«äº†9.4%ï¼Œè€Œé‡‡å–çš„æ­¥éª¤å¤šäº†18%ï¼Œç±»ä¼¼äºä¿®æ”¹é€‰æ‹©çš„å…¶ä»–åŸºçº¿ã€‚æ€»çš„æ¥è¯´ï¼Œfastmoeæ¯”GShardå’ŒBASE Layersåˆ†åˆ«å¿«1.37å€å’Œ2.19å€(æ€ä¹ˆæ›´æ…¢äº†ï¼Ÿ)

# Related Work

å‚æ•°æœåŠ¡å™¨[10,13,14]æ˜¯æœ€æ—©æ”¯æŒæ•°æ®å¹¶è¡Œçš„ç³»ç»Ÿï¼Œå¾ˆå¿«è¢«Horovod[30]å–ä»£ï¼Œä½¿ç”¨allreduceä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚å¼•å…¥å¼‚æ­¥éƒ¨åˆ†æ¨¡å‹æ›´æ–°æ–¹æ³•[15,16,18,32]æ¥åŠ å¿«å¼‚æ„ç¯å¢ƒä¸‹æ•°æ®å¹¶è¡Œçš„æ”¶æ•›é€Ÿåº¦ã€‚SuperNeurons[35]é€šè¿‡ç»†ç²’åº¦å†…å­˜ç®¡ç†æ–¹æ³•å°†å¤§å‹æ¨¡å‹æ”¾åœ¨å•ä¸ªGPUä¸Šã€‚ZeRO Offload[28]é€šè¿‡å°†æ•°æ®äº¤æ¢åˆ°ä¸»æœºå†…å­˜æ¥å‡å°‘æ•°æ®å¹¶è¡Œæ€§çš„å†…å­˜æ¶ˆè€—ï¼Œå¹¶è¿›ä¸€æ­¥å°†æ•°æ®å¸è½½åˆ°ç£ç›˜[26]ã€‚Megatron-LM[20]æ˜¯ä¸€æ¬¾ç”¨äºé¢„è®­ç»ƒçš„ä¸“ç”¨è®­ç»ƒç³»ç»Ÿï¼Œå¯¹å˜å‹å™¨å—å…·æœ‰ç‹¬åˆ°çš„æ¨¡å‹å¹¶è¡ŒåŒ–æ–¹æ³•ã€‚è±†è…[36]å’ŒFlexFlow[9]æ˜¯é€šè¿‡æ‰§è¡Œæ¨¡æ‹Ÿå™¨å’Œæœç´¢æä¾›æ•°æ®å’Œæ¨¡å‹å¹¶è¡Œæ€§çš„æœ€ä½³æ··åˆçš„é€šç”¨ç³»ç»Ÿã€‚å¦ä¸€ç§èŠ‚çœå†…å­˜çš„æ–¹æ³•æ˜¯ç®¡é“å¹¶è¡Œ[19]ï¼Œå®ƒå¯ä»¥ä¸æ•°æ®å¹¶è¡Œ[4]æ··åˆä½¿ç”¨ã€‚

åŸºäºMesh TensorFlow[31]çš„GShard[11]é¦–å…ˆå¼•å…¥äº†ä¸“å®¶å¹¶è¡Œã€‚BASE layers[12]ï¼Œä½œä¸ºFairSeq[21]çš„ä¸€éƒ¨åˆ†ï¼Œæ˜¯æ¥è‡ªPyTorch[23]ç¤¾åŒºçš„å¦ä¸€ä¸ªMoEè®­ç»ƒç³»ç»Ÿï¼Œä½¿ç”¨åŒ¹é…ç®—æ³•è¿›è¡Œä¸“å®¶åˆ†é…ã€‚

# Conclusion

åœ¨æ€§èƒ½æ¨¡å‹çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€é˜´å½±æ–¹æ³•ï¼Œå¯ä»¥å‡å°‘è´Ÿè½½ä¸å¹³è¡¡å¸¦æ¥çš„å¼€é”€ã€‚ The synchronous operators are split into smaller tasks between worker groups, and smartly scheduled to execute concurrently, minimizing communication overhead.æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§ä¸“å®¶é€‰æ‹©æ–¹æ³•æ¥é¿å…ç½‘ç»œä¸­çš„æ‹¥å¡ï¼Œåœ¨ä¿è¯æ”¶æ•›é€Ÿåº¦çš„åŒæ—¶å®ç°äº†æ›´é«˜çš„ååé‡ã€‚FasterMoEä½¿è®­ç»ƒå¤§å‹åŠ¨æ€MoEæ¨¡å‹çš„æ•ˆç‡æé«˜17.87å€ã€‚

å¾ˆä¸å¹³è¡¡æ—¶å¤åˆ¶ä¸“å®¶

åˆ†æˆnä¸ªgroupï¼Œç¯çŠ¶è®¡ç®—ï¼ˆåˆ†ç»„ï¼Œç»„å¯¹ç»„é€šä¿¡å†è®¡ç®—ï¼‰ï¼Œé€šä¿¡ä¸è®¡ç®—é‡å 

é™åˆ¶èŠ‚ç‚¹é—´ä¼ è¾“æ•°æ®ï¼Œä½¿èŠ‚ç‚¹é—´å’ŒèŠ‚ç‚¹å†…é€šä¿¡æ—¶é—´ç›¸åŒ
