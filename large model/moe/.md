Adaptive mixtures of local experts, Neural Computation'1991

Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, ICLR'17

GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding, ICLR'21

Scaling Vision with Sparse Mixture of Experts, NeurlPS'21

Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity, JMLR'22

GLaM: Efficient Scaling of Language Models with Mixture-of-Experts, ICML'2021

Go Wider Instead of Deeper, AAAI'22

MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation, NAACL'22

FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models, PPoPP'22

A Review of Sparse Expert Models in Deep Learning, arxiv'22

On the Representation Collapse of Sparse Mixture of Experts, NeurIPS'22

STABLEMOE: Stable Routing Strategy for Mixture of Experts, ACL'22

Taming Sparsely Activated Transformer with Stochastic Experts, ICLR'22

Hash Layers For Large Sparse Models, NeurlPS'22

PATHWAYS: ASYNCHRONOUS DISTRIBUTED DATAFLOW FOR ML, mlsys'22

BaGuaLu: Targeting Brain Scale Pretrained Models with over 37 Million Cores, PPoPP'22

MEGABLOCKS: EFFICIENT SPARSE TRAINING WITH MIXTURE-OF-EXPERTS, arxiv'22

Brainformers: Trading Simplicity for Efficiency, ICML'23

SMARTMOE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization, ATC'23

Accelerating Distributed MoE Training and Inference with Lina, ATC'23

PipeMoE: Accelerating Mixture-of-Experts through Adaptive Pipelining

[codecaution/Awesome-Mixture-of-Experts-Papers: A curated reading list of research in Mixture-of-Experts(MoE). (github.com)](https://github.com/codecaution/Awesome-Mixture-of-Experts-Papers)

[XueFuzhao/awesome-mixture-of-experts: A collection of AWESOME things about mixture-of-experts (github.com)](https://github.com/XueFuzhao/awesome-mixture-of-experts)



Adaptive mixtures of local experts：训练快，可扩展

Outrageously Large Neural Networks：moe层，dp+tp(专家)，负载均衡（loss，门控函数）

GShard：负载均衡（expert有计数器），自动划分tp(专家)

Switch Transformers: k=1

GLaM：decoder, few shot

Go Wider Instead of Deeper：参数共享（moe，attention）

MoEBERT：expert共享重要参数，总损失=蒸馏损失+标签损失

FasterMoE：很不平衡时复制专家；分成n个group，环状计算，通信与计算重叠；限制节点间传输数据，使节点间和节点内通信时间相同

A Review：token和专家的关系用点乘

Brainformers：分解矩阵乘法，搜索网络架构

STABLEMOE：学习一个均衡的内聚路由策略，冻结蒸馏的路由器

Taming Sparsely：一个数据进行两次计算，每次选一个专家，减少输出分布差异

Hash Layers：通过不同哈希选不同参数向量，拼接

Lina：all-to-all优先，切分通信；根据专家流行性分配设备
