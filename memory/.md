ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. SC’20

ZeRO-Offload: Democratizing Billion-Scale Model Training. ATC'21

ZeRO-infinity: breaking the GPU memory wall for extreme scale deep learning. SC’21

DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. SC’22

STRONGHOLD: Fast and Affordable Billion-Scale Deep Learning Model Training. SC’22

WholeGraph: A Fast Graph Neural Network Training Framework with Multi-GPU Distributed Shared Memory Architecture. SC’22

LightSeq2: Accelerated Training for Transformer-Based Models on GPUs. SC'22

Mobius: Fine Tuning Large-Scale Models on Commodity GPU Servers. ASPLOS’23

PatrickStar: Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory Management. TPDS'23



ZeRO:zero-dp(将模型状态平分，只更新自己负责的部分)+zero-r(MP删除冗余激活)

ZeRO-Offload:cpu(模型状态，参数更新，adam优化), gpu(参数，向前和向后计算), zero-2, MP只有对应参数在cpu上更新

ZeRO-infinity:gpu-cpu-NVMe，tile，聚合带宽，通信重叠

Mobius:每个GPU负责多个阶段的执行(减少通信量)，通过在GPU和DRAM之间交换阶段，预取下一阶段(重叠)，交叉映射减少通信争用