ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. SC’20

ZeRO-Offload: Democratizing Billion-Scale Model Training. ATC'21

ZeRO-infinity: breaking the GPU memory wall for extreme scale deep learning. SC’21

STRONGHOLD: Fast and Affordable Billion-Scale Deep Learning Model Training. SC’22

WholeGraph: A Fast Graph Neural Network Training Framework with Multi-GPU Distributed Shared Memory Architecture. SC’22

LightSeq2: Accelerated Training for Transformer-Based Models on GPUs. SC'22

Mobius: Fine Tuning Large-Scale Models on Commodity GPU Servers. ASPLOS’23

PatrickStar: Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory Management. TPDS'23