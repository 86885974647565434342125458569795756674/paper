ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. SC’20

ZeRO-Offload: Democratizing Billion-Scale Model Training. ATC'21

ZeRO-infinity: breaking the GPU memory wall for extreme scale deep learning. SC’21

DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. SC’22

STRONGHOLD: Fast and Affordable Billion-Scale Deep Learning Model Training. SC’22

WholeGraph: A Fast Graph Neural Network Training Framework with Multi-GPU Distributed Shared Memory Architecture. SC’22

LightSeq2: Accelerated Training for Transformer-Based Models on GPUs. SC'22

Mobius: Fine Tuning Large-Scale Models on Commodity GPU Servers. ASPLOS’23

PatrickStar: Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory Management. TPDS'23

Efficient Memory Management for Large Language Model Serving with PagedAttention

FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU

MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism. HPCA'23



ZeRO:zero-dp(将模型状态平分，只更新自己负责的部分)+zero-r(MP删除冗余激活)

ZeRO-Offload:cpu(模型状态，参数更新，adam优化), gpu(参数，向前和向后计算), zero-2, MP只有对应参数在cpu上更新

ZeRO-infinity:gpu-cpu-NVMe，tile，聚合带宽，通信重叠

STRONGHOLD:动态加载模型状态，通信重叠，cpu多优化器更新参数，为第一个工作窗口的层保留GPU缓冲区，多核数据并行单参数

Mobius:每个GPU负责多个阶段的执行(减少通信量)，通过在GPU和DRAM之间交换阶段，预取下一阶段(重叠)，交叉映射减少通信争用

PatrickStar:块通信，动态内存分配，重用块