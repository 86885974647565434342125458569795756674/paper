# Abstract

DeepSpeed Inference包括(1)一个多GPU推理解决方案，以最大限度地减少延迟，同时最大限度地提高密集和稀疏变压器模型在适合聚合GPU内存时的吞吐量，以及(2)一个异构推理解决方案，除了GPU内存和计算之外，还利用CPU和NVMe内存来实现不适合聚合GPU内存的大型模型的高推理吞吐量。在面向延迟的场景中，DeepSpeed Inference将延迟降低了7.3倍，在面向吞吐量的场景中，吞吐量提高了1.5倍以上。此外，它通过利用数百个gpu，在实时延迟限制下实现万亿参数规模的推理，这是前所未有的推理规模。

#  INTRODUCTION

目前变压器模型的格局越来越多样化:模型大小变化很大，最大的超过一万亿个参数;由于混合专家技术引入的稀疏性，模型特征有所不同;目标应用程序场景可以是延迟关键型或面向吞吐量的;部署硬件可以是单gpu或多gpu系统，具有不同类型的内存和存储等。

Using a transformer based model for online scenarios in production requires meeting stringent latency requirements, and thus the batch sizes used are generally small. 对于小批量，模型的推理延迟由将所有模型参数从内存加载到寄存器所需的时间来决定。因此，满足变压器模型推断的延迟需求相当于获得足够的总内存带宽。Maximizing effective memory bandwidth at small batch sizes requires reading memory at near peak memory bandwidth for fully-connected (or, linear) layers which contain the majority of the model weights, while also minimizing kernel launch and data movement overhead of other operators like layernorm and softmax. The GeMM implementations and other kernels designed for training primarily focus on maximizing compute utilization at very large batch sizes and are sub-optimal for latency-critical inference.

此外，对于大型模型，即使单个设备的峰值内存带宽也可能不足以满足推理延迟约束。它需要跨多个设备的聚合内存带宽，这需要最优的并行策略来跨设备划分模型计算，从而最小化跨设备的通信开销。这种并行策略必须适应变压器体系结构和硬件特性的变化。

关于变压器体系结构，我们将其分为两大类——密集型或稀疏型专家混合(MoE)变压器模型。最优并行策略取决于模型体系结构。例如，张量并行和管道并行只适用于密集变压器，而专家并行只适用于稀疏变压器。Moreover, transformer-based MoE models contain both dense and sparse transformer components, requiring a combination of different parallelism techniques to maximize the effective memory bandwidth across devices. 最后，就硬件特性而言，现代集群具有异构网络拓扑(例如。节点内NVLink/NVSwitch和节点间InfiniBand)，在开发并行策略时需要进一步考虑。

除了满足延迟之外，生产工作负载还具有满足成本预算的吞吐量目标。At small batch sizes, where the workload is still
memory bandwidth bound, the latency of the workload does not increase as long as the computation is entirely overlapped with model weight reads.（读取大于计算，增大计算不会增加延迟） Therefore, maximizing throughput while meeting the latency SLA requires not only maximizing the memory bandwidth utilization, but also overlapping compute with the model weight reads, and at the same time achieving high compute efficiency at small batch sizes to maximize the batch size whose compute can be overlapped with reading the
model weights.因此，推理内核必须在小批处理规模下实现高内存带宽利用率和高计算利用率，而训练内核只需要在更大的批处理规模下实现高计算利用率。

Moreover, even for throughput bound scenarios with large
batch sizes, inference workloads can differ from training
workloads in terms of data flow and computation dependencies, requiring novel solutions to achieve high throughput. 例如，生成转换器在每个生成的令牌和下一个令牌之间具有依赖关系，这在训练期间不存在。因此，在推理期间，为了跟踪以前生成的状态，它会产生更高的内存需求。对于可能需要管道并行性来适应内存中的模型的大型模型，与训练场景相比，这种跨生成令牌的依赖性还需要新的管道调度来保持所有设备繁忙

具有数百亿参数的模型太大了，无法容纳单个GPU设备的内存，而具有数千亿参数的模型甚至无法容纳单个节点的聚合GPU内存。

DeepSpeed Transformer是一个仅GPU的解决方案，旨在最大限度地减少延迟，同时最大限度地提高密集和稀疏变压器模型的吞吐量。它为各种尺寸的变压器模型实现了最先进的延迟和吞吐量，并支持在单个GPU上运行或扩展到数百个GPU以推断数万亿个参数模型。

DeepSpeed Transformer解决方案是一个三层系统架构，包括i)单个GPU变压器内核，针对低批量大小的内存带宽利用率和大批量大小的高吞吐量进行了优化，ii)多GPU密集变压器层，用于使用张量切片和推理优化的管道并行性跨GPU缩放密集变压器模型，iii)大规模GPU规模稀疏变压器层。使用并行技术和通信优化策略的组合，将MoE变压器层扩展到数百个GPU，同时还使用优化的稀疏核最小化单个GPU的稀疏计算开销。

By taking this layered approach, where each layer addresses a unique aspect of the latency challenge: batch size, scaling
dense models, and scaling sparse models, but are compatible
and built on top of each other, we create a comprehensive
system capable of achieving state-of-art latency and throughput at unprecedented scales for both dense and sparse transformer models despite the heterogeneity in batch size, model scale and model characteristics.

ZeRO-Inference是一种基于GPU+CPU+NVMe的异构解决方案，通过使用最少的GPU资源实现大规模模型推理来解决内存挑战。与DeepSpeed Transformer相比，对于延迟敏感性较低但资源受限的应用程序，ZeRO-Inference允许在单个或多个gpu上推断具有数千亿参数的模型，只要有足够的CPU或NVMe内存来存储模型参数。此外，即使模型适合聚合GPU内存，ZeRO-Inference通过支持更大的批处理大小，提供比DeepSpeed Transformer更好的每个GPU效率。

单GPU变压器内核，通过以内存带宽为中心的融合计划和GeMM内核来最小化延迟和最大化吞吐量(第III节)。

A many-GPU dense transformer inference system that
combines tensor-parallelism to minimize latency with
inference optimized pipeline parallelism schedules and
memory optimizations to maximize throughput

一个大规模gpu稀疏模型推理系统，它结合了:i)专家、数据和张量并行性，ii)新颖的通信优化，以及iii)在数百个gpu上对数万亿参数进行稀疏推理的稀疏内核优化(第V节)。

零推理:利用CPU、NVMe和GPU内存以及GPU计算，在有限的资源下实现大规模模型推理(第六节)。

i)对于延迟敏感的场景，DeepSpeed transformer显示出比最先进的延迟减少高达1:9 x的密集模型(高达175B参数)和7:2 x的稀疏模型(25毫秒以下的1T模型)，同时在33%的峰值内存带宽利用率下扩展到256个gpu，这是前所未有的推理规模。

ii)对于面向吞吐量的场景，DeepSpeed Transformer比最先进的方案显示了超过1:5倍的增益(第VII-C节)

iii)在GPU资源受限系统上对ZeRO-Inference的评估，表明ZeRO-Inference可以支持比仅GPU解决方案大25倍的模型，同时实现超过50%的峰值硬件性能。

# BACKGROUND AND RELATED WORK

缩放密集语言模型的成功促使研究人员和从业者进一步提出专家混合(MoE)技术，该技术在变压器模型中引入了稀疏性[10]。Typical
transformer model [11] architectures have transformer blocks
that consist of two consecutive sub-layers, a self-attention
sub-layer followed by a position-wise feed-forward (FF) block. MoE models add conditional computation by replacing the feedforward blocks with a Position-wise MoE layer with a variable number of experts and a top-k gating function. Increasing the number of experts allows scaling the MoE model size with only sublinear increase in computation cost, greatly reducing the training cost of the model. 然而，MoE模型可以比其质量等效的密集模型大8倍[12]，[13]，[14]，[15]，需要更高的聚合内存带宽才能在推理期间实现相当的延迟(模型大，计算小？)

扩展模型大小的主要挑战在于内存瓶颈。为了满足内存需求，前人提出了各种并行策略来使用节点内和节点间的聚合GPU内存。

As the number of GPU increases for tensorslicing, two primary trade-offs show up: (i) lower compute granularity due to the smaller local problem size, and (ii) all-reduce communications in each transformer layer to aggregate the partial activations. When scaling across node boundaries, the inter-node bandwidth is limited comparing to the fast intra-node connections, thus tensor parallelism can cause a significant latency degradation.在实践中，张量并行性通常仅限于共享节点内高带宽互连的gpu组(例如，NVIDIA NVLink)。

model splitting and micro-batching could pose functionality, performance and convergence related restrictions for pipeline parallelism。

3D并行性[21]将数据、张量和管道并行性有效地结合起来，扩展到数万亿参数的模型。

Expert parallelism [22] places different experts on different
GPUs and executes them in parallel. Each expert only processes a subset of tokens on each expert based on a learned top-k gating function. The classic all-to-all communication primitive has been used to implement expert parallelism [23], [15], [22].

上述并行策略主要是为了最大限度地提高训练吞吐量而设计的，在推理过程中，由于推理中批量小的并行性不足，它们的有效性会受到限制。

也有一组工作专注于加速变压器内核的性能[24]，[25]，[26]。BERT的训练时间记录是通过随机变压器核实现的，该核融合了算子并减少了激活内存以支持大批量[24]。Ianov等[25]使用转换数据流图来融合元素和约简算子，加速训练。TurboTransformers[26]类似地融合了用于变压器推理的元素算子和约简算子。E.T.[27]将融合、自定义GeMM和剪枝结合在一起，加快了变形金刚的推理速度。The kernel optimizations presented in this work fuse a wider variety
of operators, such as head-wise transformation that requires
additional data layout transformation and layers beyond the selfattention sublayers, such as the intermediate layers and MoE specific layers.此外，本工作中提出的内核还支持自回归生成模型，这些模型需要在推理期间使用kv缓存[8]来提高性能，而上述工作并未考虑对kv缓存的支持。

存在一些编译器和运行时来促进模型的部署，如TVM [28]， ONNXRuntime[29]和TensorRT[30]。这些平台主要专注于优化可以适合单个GPU的深度神经网络模型，例如具有数亿个参数的小型变压器。相比之下，我们的工作目标是十亿规模甚至万亿规模的变压器，这些变压器不容易安装在单个GPU设备上。与我们最相关的工作是FastTransformer[31]，它支持变压器模型的多gpu推理，我们将在第七节中提供更详细的比较。最后，已经有许多工作通过模型压缩技术(如蒸馏、量化和稀疏化)改进DNN模型的部署，这可以在很小的精度代价下减少计算时间和内存消耗。我们的工作是对这些的补充

# INFERENCE-OPTIMIZED TRANSFORMER KERNELS

我们将讨论能够实现小批量和大批量高性能推理的变压器内核的挑战、设计和优化。

小批量性能受到读取模型权重时内存带宽利用率的限制。There are three main challenges to optimizing for memory
bandwidth at small-batch inference. First, due to limited work
at different kernels performing the operations of a transformer layer using small batch, inference performance suffers from the kernel-invocation overhead. （内核调用开销）Second, each kernel-invocation writes data to global memory which is read by GPU cores during the next kernel invocation, and this data-transfer between GPU cores and global memory adds an additional overhead.（数据写到global再读入）Finally, neither cuBLAS nor CUTLASS GeMM libraries are well tuned for extremely small batch sizes, and cannot achieve
good memory-bandwidth utilization.（带宽利用率低）

另一方面，大批推理性能受到计算利用率的限制，虽然变压器层内的GeMM等计算繁重的操作可以使用CUBLAS和CUTLASS库实现非常好的计算利用率，但总体利用率仍然受到内核启动开销和GPU内核之间的数据传输以及跨GeMM以外的不同内核的全局内存的限制。

为了应对这些挑战，我们引入了两种技术:i)深度融合，通过融合元素操作之外的多个内核来减少内核调用和数据移动开销;ii)定制GeMM内核，用于在批处理规模相对较小时提高内存带宽利用率，同时允许使用深度融合进行融合。

虽然算子融合是深度学习中用于减少内核启动和数据移动开销的常用技术，但它主要限于元素运算符[32]，[28]，[29]。相反，transformer由数据布局转换、缩减和gemm等操作符组成，这些操作符跨线程块创建数据依赖关系，使它们难以融合。这是因为在GPU上，如果一个线程块产生的数据被另一个线程块消耗，则需要全局内存同步，从而调用新的内核。

为了避免全局同步的需要，Deep-Fusion将计算空间沿着迭代空间的维度进行平铺，这不会产生跨平铺的数据依赖，并在不同的线程块上并行执行它们。包含数据依赖关系的计算空间的维度不被平铺，而是由相同的线程块处理。在此平铺之后，如果第二个算子的每个平铺依赖于第一个算子的一个输出平铺，则可以使用DeepFusion融合两个算子。

只要没有交叉依赖，Deep-Fusion不仅可以融合元素操作，还可以融合约简、数据转换和gem。此外，每个tile产生的数据要么保存在寄存器中，要么在可能的情况下保存在共享内存中，以允许跨操作符重用数据，而不会产生全局内存数据传输开销。

Our custom GeMM implementation is designed to be
fusable with Deep-Fusion while achieving maximum memory
bandwidth utilization. Its design can be viewed in three parts:
tiling strategies, cooperative-group reduction, and data-layout
transformation for better memory bandwidth utilization.

# INFERENCE-ADAPTED DENSE TRANSFORMER MODELS ON MANY-GPU SYSTEMS
# MASSIVE SCALE SPARSE MODEL INFERENCE

# DEMOCRATIZATION OF LARGE MODEL INFERENCE

