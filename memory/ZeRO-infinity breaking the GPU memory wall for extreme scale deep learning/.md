# ABSTRACT

å®ƒåˆ©ç”¨GPUã€CPUå’ŒNVMeå†…å­˜ï¼Œåœ¨æœ‰é™çš„èµ„æºä¸Šå®ç°å‰æ‰€æœªæœ‰çš„æ¨¡å‹æ‰©å±•ï¼Œè€Œä¸éœ€è¦æ¨¡å‹ä»£ç é‡æ„ã€‚åŒæ—¶ï¼Œå®ƒå®ç°äº†å‡ºè‰²çš„è®­ç»ƒååé‡å’Œå¯æ‰©å±•æ€§ï¼Œä¸å—æœ‰é™çš„CPUæˆ–NVMeå¸¦å®½çš„é˜»ç¢ã€‚å®ƒå¯ç”¨äºåœ¨å•ä¸ªNVIDIA DGX-2èŠ‚ç‚¹ä¸Šå¾®è°ƒä¸‡äº¿å‚æ•°æ¨¡å‹ï¼Œä½¿å¤§å‹æ¨¡å‹æ›´æ˜“äºè®¿é—®ã€‚åœ¨è®­ç»ƒååé‡å’Œå¯æ‰©å±•æ€§æ–¹é¢ï¼Œå®ƒåœ¨512ä¸ªNVIDIA V100 gpu(å³°å€¼çš„40%)ä¸Šç»´æŒè¶…è¿‡25åƒä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—ï¼ŒåŒæ—¶ä¹Ÿå±•ç¤ºäº†è¶…çº§çº¿æ€§å¯æ‰©å±•æ€§ã€‚ZeRO-Infinityçš„å¼€æºå®ç°å¯ä»¥é€šè¿‡DeepSpeed 1è·å¾—ã€‚

# EXTENDED INTRODUCTION

è¶…è¿‡1000äº¿ä¸ªå‚æ•°(GPT-3[4])ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œå•ä¸ªGPUå†…å­˜ä»…å¢åŠ äº†5å€(ä»16 GBå¢åŠ åˆ°80 GB)ã€‚

ç›®å‰æœ€å…ˆè¿›çš„å¤§å‹æ¨¡å‹è®­ç»ƒæŠ€æœ¯æ˜¯ä¸‰ç»´å¹¶è¡Œ(3D parallelism[13,14])ï¼Œå®ƒå°†æ¨¡å‹(å¼ é‡åˆ‡ç‰‡)å’Œç®¡é“å¹¶è¡Œä¸æ•°æ®å¹¶è¡Œç›¸ç»“åˆï¼Œå¯ä»¥åœ¨æ•°ç™¾æˆ–æ•°åƒä¸ªgpuä¸Šæœ‰æ•ˆåœ°å°†æ·±åº¦å­¦ä¹ è®­ç»ƒæ‰©å±•åˆ°æ•°ä¸‡äº¿ä¸ªå‚æ•°ã€‚å°½ç®¡3Då¹¶è¡Œæ€§å…·æœ‰ç”¨äºå¤§å‹æ¨¡å‹è®­ç»ƒçš„èƒ½åŠ›ï¼Œä½†æˆ‘ä»¬ç°åœ¨åˆ°è¾¾äº†GPUå†…å­˜å¢™[16]ã€‚æ€»çš„GPUå†…å­˜ä¸è¶³ä»¥æ”¯æŒæ¨¡å‹å¤§å°çš„å¢é•¿ã€‚å³ä½¿ä½¿ç”¨å…·æœ‰80gbå†…å­˜çš„æœ€æ–°NVIDIA A100 GPU, 3Då¹¶è¡Œæ€§ä¹Ÿéœ€è¦320ä¸ªGPUæ‰èƒ½é€‚åº”ç”¨äºè®­ç»ƒçš„ä¸‡äº¿å‚æ•°æ¨¡å‹ï¼Œå¹¶ä¸”å³ä½¿æˆ‘ä»¬å‡è®¾GPUå†…å­˜åœ¨æœªæ¥å‡ å¹´å†…å¢åŠ 5å€ï¼Œæ‰©å±•åˆ°æœªæ¥çš„100ä¸‡äº¿å‚æ•°æ¨¡å‹ä¹Ÿéœ€è¦è¶…è¿‡6Kä¸ªGPUã€‚ä»¥GPUå†…å­˜ä¸ºç“¶é¢ˆï¼Œæˆ‘ä»¬æ— æ³•å†ç»´æŒæ¨¡å‹è§„æ¨¡çš„æŒç»­å¢é•¿ã€‚

è™½ç„¶é¢„è®­ç»ƒå…·æœ‰æ•°åƒäº¿ä¸ªå‚æ•°çš„æ¨¡å‹å¯èƒ½éœ€è¦æ•°ç™¾ä¸‡ä¸ªGPUè®¡ç®—å°æ—¶ï¼Œä½†å¯¹å…¶è¿›è¡Œå¾®è°ƒè¦ä¾¿å®œå¾—å¤šï¼Œéœ€è¦çš„GPUè®¡ç®—å°æ—¶è¦å°‘å¾—å¤šï¼Œå¹¶ä¸”å¯ä»¥åœ¨å•ä¸ªè®¡ç®—èŠ‚ç‚¹ä¸Šä½¿ç”¨å°‘é‡GPUå®Œæˆã€‚è™½ç„¶è®¸å¤šä¼ä¸šå’Œç”¨æˆ·éƒ½å¯ä»¥è®¿é—®è¿™äº›è®¡ç®—èµ„æºï¼Œä½†ä¸å¹¸çš„æ˜¯ï¼Œå®ƒä»¬å—åˆ°è¿™äº›è®¡ç®—èŠ‚ç‚¹ä¸Šå¯ç”¨å†…å­˜çš„é™åˆ¶ï¼Œè¿™åè¿‡æ¥åˆé™åˆ¶äº†å¯ä»¥å¾®è°ƒçš„æ¨¡å‹çš„å¤§å°ã€‚è¿™ä½¿å¾—å¤§å¤šæ•°æ— æ³•è®¿é—®å¤§è§„æ¨¡GPUé›†ç¾¤çš„ç ”ç©¶äººå‘˜å’Œå…¬å¸æ— æ³•è¿›è¡Œå¤§å‹æ¨¡å‹å¾®è°ƒã€‚ä¾‹å¦‚ï¼Œå¾®è°ƒGPT-3å°†éœ€è¦è¶…è¿‡8ä¸ªå…·æœ‰3Då¹¶è¡Œæ€§çš„DGX-2èŠ‚ç‚¹(128ä¸ªgpu)æ¥é€‚åº”è®­ç»ƒæ¨¡å‹ï¼Œå³ä½¿å•ä¸ªDGX-2èŠ‚ç‚¹(16ä¸ªgpu)æœ‰è¶³å¤Ÿçš„è®¡ç®—èƒ½åŠ›åœ¨åˆç†çš„æ—¶é—´å†…å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚

é™¤äº†GPUå†…å­˜å¢™ä¹‹å¤–ï¼Œç”¨äºè®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹çš„æœ€å…ˆè¿›æŠ€æœ¯åœ¨å¯ç”¨æ€§å’Œçµæ´»æ€§æ–¹é¢ä¹Ÿå—åˆ°é™åˆ¶ã€‚

å¦‚ä¸Šæ‰€è¿°ï¼Œ3Då¹¶è¡Œæ€§éœ€è¦ä»¥å¤æ‚çš„æ–¹å¼å°†æ•°æ®ã€æ¨¡å‹å’Œç®¡é“å¹¶è¡Œæ€§ç»“åˆèµ·æ¥ï¼Œä»¥è·å¾—æ•°åƒäº¿æˆ–æ•°ä¸‡äº¿ä¸ªå‚æ•°ã€‚è™½ç„¶è¿™æ ·çš„ç³»ç»Ÿå¯ä»¥éå¸¸é«˜æ•ˆï¼Œä½†å®ƒéœ€è¦æ•°æ®ç§‘å­¦å®¶æ‰§è¡Œä¸»è¦çš„æ¨¡å‹ä»£ç é‡æ„ï¼Œç”¨å¼ é‡åˆ‡ç‰‡ç‰ˆæœ¬æ›¿æ¢å•ä¸ªGPUæ“ä½œç¬¦ï¼Œå¹¶å°†æ¨¡å‹åˆ’åˆ†ä¸ºè´Ÿè½½å¹³è¡¡çš„ç®¡é“é˜¶æ®µã€‚è¿™ä¹Ÿä½¿å¾—3Då¹¶è¡Œæ€§åœ¨å®ƒå¯ä»¥æ”¯æŒçš„æ¨¡å‹ç±»å‹ä¸­ä¸çµæ´»ã€‚å…·æœ‰å¤æ‚ä¾èµ–å…³ç³»çš„æ¨¡å‹ä¸å®¹æ˜“è½¬æ¢ä¸ºè´Ÿè½½å¹³è¡¡çš„ç®¡é“

è€ƒè™‘åˆ°å¤§å‹æ¨¡å‹è®­ç»ƒçš„å‰æ™¯ï¼Œå‡ºç°äº†3ä¸ªé—®é¢˜:â€¢å±•æœ›æœªæ¥ï¼Œæˆ‘ä»¬å¦‚ä½•æ”¯æŒæ¨¡å‹è§„æ¨¡çš„ä¸‹ä¸€ä¸ª1000å€å¢é•¿ï¼Œä»åƒGPT-3è¿™æ ·æ‹¥æœ‰1750äº¿ä¸ªå‚æ•°çš„æ¨¡å‹å‘å±•åˆ°æ‹¥æœ‰æ•°ç™¾ä¸‡äº¿ä¸ªå‚æ•°çš„æ¨¡å‹?â€¢æˆ‘ä»¬å¦‚ä½•è®©æ›´å¤šæ— æ³•ä½¿ç”¨æ•°ç™¾ä¸ªgpuçš„æ•°æ®ç§‘å­¦å®¶èƒ½å¤Ÿè®¿é—®ä»Šå¤©çš„å¤§å‹æ¨¡å‹?â€¢æˆ‘ä»¬æ˜¯å¦å¯ä»¥é€šè¿‡æ¶ˆé™¤æ¨¡å‹é‡æ„å’Œå¤šç§å½¢å¼çš„å¹¶è¡Œæ€§æ¥ç®€åŒ–å¤§å‹æ¨¡å‹çš„è®­ç»ƒ?

æˆ‘ä»¬ä»3Då¹¶è¡Œæ€§è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œæå‡ºäº†ZeRO-Infinityï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿè§£å†³ä¸Šè¿°æ‰€æœ‰å¤§å‹æ¨¡å‹è®­ç»ƒæŒ‘æˆ˜çš„æ–°ç³»ç»Ÿã€‚

ZeRO-Infinityæ˜¯ä¸€ç§æ•°æ®å¹¶è¡Œè®­ç»ƒçš„å½¢å¼ï¼Œä½†ä¸æ ‡å‡†æ•°æ®å¹¶è¡Œè®­ç»ƒä¸åŒçš„æ˜¯ï¼Œåœ¨æ ‡å‡†æ•°æ®å¹¶è¡Œè®­ç»ƒä¸­ï¼Œæ¨¡å‹çŠ¶æ€(å¦‚å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€)åœ¨æ‰€æœ‰æ•°æ®å¹¶è¡Œè¿›ç¨‹ä¸­è¢«å¤åˆ¶ï¼ŒZeRO-Infinityå°†å®ƒä»¬åˆ†åŒºï¼Œä»¥å……åˆ†åˆ©ç”¨æ‰€æœ‰æ•°æ®å¹¶è¡Œè¿›ç¨‹çš„èšåˆå†…å­˜ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒZeRO-Infinityä½¿ç”¨é€šä¿¡é›†åˆæ¥æ”¶é›†å½“å‰éœ€è¦çš„æ¨¡å‹çŠ¶æ€ã€‚è¿™ä¸è¢«ç§°ä¸ºZeROçš„æ•°æ®å¹¶è¡Œçš„å†…å­˜æ•ˆç‡å½¢å¼ç±»ä¼¼[11]ã€‚ç„¶è€Œï¼Œä¸åªåœ¨GPUä¸Šä¿ç•™æœ¬åœ°åˆ†åŒºçš„ZeROä¸åŒï¼ŒZeRO- infinityå°†å®ƒä»¬å¸è½½åˆ°CPUæˆ–NVMeå†…å­˜ï¼Œæ ¹æ®éœ€è¦åœ¨GPU, CPUå’ŒNVMeä¹‹é—´ç§»åŠ¨å®ƒä»¬ï¼Œå…è®¸ZeRO- infinityå……åˆ†åˆ©ç”¨CPUå’ŒNVMeå†…å­˜ä»¥åŠGPUå†…å­˜ã€‚

ZeRO-Infinity extends the ZeRO family of technology [11, 12] with new innovations in heterogeneous memory access called the infinity offload engine. This allows ZeRO-Infinityé€šè¿‡åŒæ—¶åˆ©ç”¨CPUå’ŒNVMeå†…å­˜ï¼Œåœ¨æœ‰é™çš„GPUèµ„æºä¸Šæ”¯æŒå¤§è§„æ¨¡æ¨¡å‹å¤§å°ã€‚

ZeRO-Infinity also introduces a novel GPU memory
optimization technique called memory-centric tiling to support extremely large individual layers that would otherwise not fit in GPU memory even one layer at a time. With the infinity offload engine and memory-centric tiling, ZeRO-Infinity not only supports the next 1000x growth in model size, but also makes large models accessible to data scientists with limited GPU resources.

ZeRO-Infinity introduces a novel data partitioning strategy for leveraging aggregate memory bandwidth across all devices, which we refer to as bandwidth-centric partitioning, and combines it with powerful communication overlap-centric design, as well as optimizations for high performance NVMe
access in the infinity offload engine. Together, ZeRO-Infinity offers excellent training efficiency, despite offloading data to CPU or NVMe, unencumbered by their limited bandwidth.

æœ‰äº†ZeRO-Infinityï¼Œæ•°æ®ç§‘å­¦å®¶ä¸å†éœ€è¦è°ƒæ•´ä»–ä»¬çš„æ¨¡å‹æ¥é€‚åº”å¤šç§å½¢å¼çš„å¹¶è¡Œï¼Œæ¯”å¦‚3Då¹¶è¡Œã€‚è¿™æ˜¯å¯èƒ½çš„ï¼Œå› ä¸ºä¸Šé¢è®¨è®ºçš„ZeRO-Infinityä¸­çš„ä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„å¹³é“ºæ—¨åœ¨å‡å°‘å¤§å‹å•ä¸ªå±‚çš„GPUå†…å­˜éœ€æ±‚ï¼Œå¦åˆ™å°†éœ€è¦æ¨¡å‹å¹¶è¡Œæ€§(å¼ é‡åˆ‡ç‰‡)æ¥é€‚åº”GPUå†…å­˜ä¸­çš„å±‚ã€‚ZeRO-Infinityæ¶ˆé™¤äº†æ‰‹åŠ¨æ¨¡å‹ä»£ç é‡æ„çš„éœ€è¦ï¼Œå³ä½¿æ˜¯åœ¨æ‰©å±•åˆ°æ•°ä¸‡äº¿ä¸ªå‚æ•°æ—¶ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„å®ç°ï¼Œè‡ªåŠ¨å®Œæˆè®­ç»ƒä»»æ„æ¨¡å‹æ¶æ„æ‰€éœ€çš„æ‰€æœ‰é€šä¿¡å’Œæ•°æ®åˆ†åŒºã€‚

æè¿°å¤§å‹æ¨¡å‹è®­ç»ƒçš„ä¸åŒç»„ä»¶çš„å†…å­˜éœ€æ±‚(ç¬¬3èŠ‚)ä»¥åŠå®ƒä»¬çš„å¸¦å®½éœ€æ±‚(ç¬¬4èŠ‚)ï¼Œ

ä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„å¹³é“ºï¼Œåœ¨ä¸éœ€è¦æ¨¡å‹å¹¶è¡Œçš„æƒ…å†µä¸‹å¤„ç†å¤§é‡æ“ä½œ

ä»¥å¸¦å®½ä¸ºä¸­å¿ƒçš„åˆ†åŒºï¼Œåœ¨æ‰€æœ‰å¹¶è¡Œè®¾å¤‡ä¸Šåˆ©ç”¨èšåˆå†…å­˜å¸¦å®½

åœ¨ç›¸åŒçš„ç¡¬ä»¶ä¸Šå®ç°è¶…è¿‡25åƒä¸‡äº¿æ¬¡çš„ååé‡

åœ¨å•ä¸ªDGX-2èŠ‚ç‚¹ä¸Šå¾®è°ƒå¤šè¾¾ä¸‡äº¿å‚æ•°çš„æ¨¡å‹ï¼Œè€Œä¸ä½¿ç”¨ä»»ä½•æ¨¡å‹å¹¶è¡Œæ€§æˆ–æ¨¡å‹ä»£ç é‡æ„

# BACKGROUND AND RELATED WORK

å¯¹äºé€‚åˆè®¾å¤‡å†…å­˜è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨æ•°æ®å¹¶è¡Œæ€§(DP)å°†è®­ç»ƒæ‰©å±•åˆ°å¤šä¸ªè®¾å¤‡ã€‚å½“æ¨¡å‹ä¸é€‚åˆè®¾å¤‡å†…å­˜æ—¶ï¼Œæ¨¡å‹å¹¶è¡Œ2 (MP)[7,17,18]å’Œç®¡é“å¹¶è¡Œ2 (PP)[7 - 9]å¯ä»¥åˆ†åˆ«åœ¨å‚ç›´å’Œæ°´å¹³æ–¹å‘å°†æ¨¡å‹æ‹†åˆ†åˆ°è¿›ç¨‹ä¹‹é—´ã€‚è™½ç„¶3Då¹¶è¡Œå¯ä»¥éå¸¸é«˜æ•ˆï¼Œä½†å®ƒéœ€è¦i)å¤§é‡çš„æ¨¡å‹ä»£ç é‡æ„æ¥å°†æ¨¡å‹æ‹†åˆ†ä¸ºæ¨¡å‹å’Œç®¡é“å¹¶è¡Œç»„ä»¶ï¼Œii)å…·æœ‰å¤æ‚ä¾èµ–å›¾çš„æ¨¡å‹éš¾ä»¥è¡¨ç¤ºä¸ºè´Ÿè½½å¹³è¡¡çš„ç®¡é“é˜¶æ®µï¼Œiii)æ¨¡å‹å¤§å°å—é™äºå¯ç”¨çš„GPUå†…å­˜æ€»é‡ã€‚

ZeROæ¶ˆé™¤äº†æ•°æ®å¹¶è¡Œè¿›ç¨‹ä¹‹é—´çš„å†…å­˜å†—ä½™ã€‚é€šè¿‡è¿™æ ·åšï¼Œä¸ä¼ ç»Ÿçš„æ•°æ®å¹¶è¡Œæ€§ç›¸æ¯”ï¼Œå®ƒæé«˜äº†å†…å­˜æ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™äº†å…¶è®¡ç®—ç²’åº¦å’Œé€šä¿¡æ•ˆç‡ã€‚

ZeROOffload[12]æ˜¯åœ¨å¤šgpuä¸Šè¿›è¡Œå¤§å‹æ¨¡å‹è®­ç»ƒçš„æœ€å…ˆè¿›(SOTA)æ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶éœ€è¦å°†å‚æ•°å­˜å‚¨åœ¨GPUå†…å­˜ä¸­ï¼Œå¹¶åœ¨æ‰€æœ‰è®¾å¤‡ä¸Šå¤åˆ¶ã€‚the model scale with ZeRO-Offload is limited to the total number of parameters that the memory on a single GPU device can host. ç”±äºæ¬¡ä¼˜æ•°æ®åˆ†åŒº(?)å’Œæœ‰é™çš„PCIeå¸¦å®½ï¼ŒZeRO-Offloadè¿˜éœ€è¦å¤§çš„æ‰¹å¤„ç†å¤§å°æ¥ä¿æŒæ•ˆç‡ã€‚

é€šè¿‡å‹ç¼©[28]ã€æ¿€æ´»æ£€æŸ¥ç‚¹[29,30]æˆ–å®æ—¶åˆ†æ[31]ï¼Œå¤šç§åŠªåŠ›éƒ½é›†ä¸­åœ¨å‡å°‘æ¿€æ´»æ‰€éœ€çš„å†…å­˜ä¸Šã€‚ZeRO-Infinityä¸æ¿€æ´»æ£€æŸ¥ç‚¹ä¸€èµ·å·¥ä½œä»¥å‡å°‘æ¿€æ´»å†…å­˜ã€‚

# MEMORY REQUIREMENTS

æè¿°äº†GPUä¸Šå¿…é¡»å¯ç”¨çš„æœ€å°å†…å­˜é‡æ¥æ”¯æŒè®­ç»ƒï¼Œå‡è®¾æ¨¡å‹å’Œå‰©ä½™çŠ¶æ€å¯ä»¥æˆåŠŸåœ°ä»GPUå†…å­˜ä¸­å¸è½½ã€‚

Adamä¼˜åŒ–å™¨è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒæ—¶ï¼Œå‚æ•°å’Œæ¢¯åº¦å­˜å‚¨åœ¨FP16ä¸­ï¼Œä¼˜åŒ–å™¨çŠ¶æ€ç”±FP32åŠ¨é‡ã€æ–¹å·®ã€å‚æ•°å’Œæ¢¯åº¦(?)ç»„æˆã€‚æ¯ä¸ªå‚æ•°æ€»å…±éœ€è¦20å­—èŠ‚çš„å†…å­˜ã€‚åŸºäºTransformerçš„æ¨¡å‹ä¸­å‚æ•°çš„æ€»æ•°ä¸»è¦å–å†³äºéšè—ç»´åº¦(hğ‘‘)å’ŒTransformerå±‚çš„æ•°é‡(ğ‘›ğ‘™)ã€‚Nearly all
the parameters in a Transformer block come from four linear layers within each block with sizes: (â„ğ‘‘, 3â„ğ‘‘), (â„ğ‘‘, â„ğ‘‘), (â„ğ‘‘, 4â„ğ‘‘) and (4â„ğ‘‘, â„ğ‘‘), respectively. åŸºäºTransformerçš„æ¨¡å‹å’Œä¸­çš„æ€»å‚æ•°å¯ä»¥è¿‘ä¼¼ä¸º12 Ã—ğ‘›ğ‘™Ã— $hğ‘‘^2$ï¼Œéœ€è¦æ€»å†…å­˜240 Ã—ğ‘›ğ‘™Ã— $hğ‘‘^2$å­—èŠ‚æ¥å­˜å‚¨æ¨¡å‹çŠ¶æ€ã€‚

å‰©ä½™çŠ¶æ€ä¸»è¦ç”±æ¿€æ´»å†…å­˜ç»„æˆï¼Œå®ƒå–å†³äºæ¨¡å‹ä½“ç³»ç»“æ„ã€æ‰¹å¤„ç†å¤§å°(ğ‘ğ‘ )å’Œåºåˆ—é•¿åº¦(ğ‘ ğ‘’ğ‘)ï¼Œå¹¶ä¸”å¯èƒ½ç›¸å½“å¤§ã€‚ä»ç§¯æçš„æ–¹é¢æ¥çœ‹ï¼Œæ¿€æ´»æ‰€éœ€çš„å†…å­˜å¯ä»¥é€šè¿‡æ¿€æ´»æ£€æŸ¥ç‚¹æ˜¾è‘—å‡å°‘[29]ï¼Œè¿™ä»¥0.33å€çš„é¢å¤–é‡æ–°è®¡ç®—ä¸ºä»£ä»·æ¢å–äº†æ¿€æ´»å†…å­˜ã€‚å›¾çµ- nlg 17.2Bå’ŒGPT-3 175Bç­‰å¤§å‹æ¨¡å‹å‡ä½¿ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹è¿›è¡Œè®­ç»ƒã€‚å­˜å‚¨æ¿€æ´»æ£€æŸ¥ç‚¹æ‰€éœ€çš„å†…å­˜ä¼°è®¡ä¸º $2 Ã— ğ‘ğ‘ ğ‘§ Ã— ğ‘ ğ‘’ğ‘ Ã— â„ğ‘‘ Ã— ğ‘›ğ‘™/ğ‘i$ bytes, ğ‘ğ‘– is the number of Transformer blocks between two activation checkpoints, and ğ‘ğ‘ ğ‘§ Ã— ğ‘ ğ‘’ğ‘ Ã— â„ğ‘‘ is the size of the input to each Transformer block. Many modern GPU clusters have 8-16 GPUs per node, and so we chose a batch size of 2-4 per GPU, resulting in a batch size of 32 as a conservative estimate of activation within each node. While the resulting activation checkpoints are orders of magnitude smaller than the full set of activations (column 6) , beyond a trillion parameters they still get too large to fit in GPU memory for the batch size and sequence length under consideration.

Model State Working Memory (MSWM) is the minimum
amount of GPU memory required to perform forward or backward propagation on the largest single operator in the model after all the model states have been offloaded to CPU or NVMe. This is approximately given by the size of the parameters and gradients of that operator in the model, since there must be at least enough memory to hold the parameter and its gradient for backward propagation(æ¿€æ´»ä¸ç”¨å—ï¼Ÿ)

For a Transformer based model, the largest operator is a linear layer that transforms hidden states from â„ğ‘‘ to 4â„ğ‘‘. The size of the parameter and gradients of this linear layer is 4 Ã— â„ğ‘‘ Ã— 4â„ğ‘‘ bytes

MSWM(å›¾2aåˆ—8)åœ¨è¶…è¿‡1000äº¿ä¸ªå‚æ•°çš„æƒ…å†µä¸‹æ˜¾è‘—å¢é•¿ï¼Œéœ€è¦å¤šä¸ªgbçš„è¿ç»­å†…å­˜ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨è®­ç»ƒæœŸé—´å†…å­˜è€—å°½ã€‚3D Parallelismç­‰æœ€å…ˆè¿›çš„æ–¹æ³•é€šè¿‡æ¨¡å‹å¹¶è¡Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œå°†å•ä¸ªè¿ç®—ç¬¦æ‹†åˆ†åˆ°å¤šä¸ªgpuä¸Šã€‚åœ¨ç¬¬5.1.3èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³è¿™äº›å¤§é‡çš„æ¨¡å‹çŠ¶æ€å·¥ä½œè®°å¿†ï¼Œè€Œä¸éœ€è¦æ¨¡å‹å¹¶è¡Œæ€§ã€‚

Activation Working Memory (AWM) is the memory required
in the backward propagation for recomputing the activations before performing the actual backward propagation.  This is the size of the activations between two consecutive activation checkpoints. For example, if we create one activation checkpoint per Transformer block, the memory is given by the size of the total activation per Transformer block. This is given in bytes by approximately ğ‘ğ‘ ğ‘§ Ã—ğ‘ ğ‘’ğ‘ Ã— ğ‘ğ‘– Ã— $(16 Ã— â„ğ‘‘ + 2 Ã— ğ‘ğ‘¡ğ‘¡ğ‘›\_â„ğ‘’ğ‘ğ‘‘ğ‘  Ã— ğ‘ ğ‘’ğ‘)$. AWMéšåºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡å¢é•¿ã€‚åœ¨æŸäº›å·¥ä½œè´Ÿè½½ä¸­ï¼Œåºåˆ—é•¿åº¦å¯ä»¥è¾¾åˆ°æ•°ä¸‡ï¼ŒAWMå¯ä»¥è¾¾åˆ°æ•°ç™¾GBï¼Œè¶…å‡ºäº†å•ä¸ªGPUçš„å¯ç”¨èŒƒå›´ã€‚å¯¹äºè¿™ç±»æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ç¨€ç–å…³æ³¨[39,40]æ¥æœ‰æ•ˆåœ°é™ä½AWMã€‚ç¨€ç–æ³¨æ„åŠ›ä¼˜åŒ–æ˜¯å¯¹æˆ‘ä»¬å·¥ä½œçš„è¡¥å……ï¼Œå¹¶ä¸”è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ã€‚

å³ä½¿ğ‘â‰¥1,AWMçš„å‚æ•°ä¹Ÿä¼šè¶…è¿‡10ä¸‡äº¿ä¸ªã€‚Unlike MSWM that is only composed of a single parameter and gradient, AWM is composed of dozens of activations, and does not cause memory issues due to lack of contiguous memory as long as the total AWM can fit in GPU memory.

# BANDWIDTH REQUIREMENTS

Assuming a workload execution without any compute and communication overlap, we can use the peak computational throughput (ğ‘ğ‘’ğ‘ğ‘˜ğ‘¡ğ‘ ), data movement bandwidth (ğ‘ğ‘¤) and its arithmetic intensity (ğ‘ğ‘–ğ‘¡) to estimate the training efficiency.å·¥ä½œè´Ÿè½½çš„ç®—æœ¯å¼ºåº¦(AIT)æ˜¯æ€»è®¡ç®—é‡ä¸è®¡ç®—æ‰€éœ€æ•°æ®é‡ä¹‹æ¯”ã€‚å®ƒæè¿°äº†æ¯æ¬¡æ•°æ®ç§»åŠ¨çš„è®¡ç®—é‡ã€‚è¾ƒé«˜çš„AITæ„å‘³ç€å¯¹æ•°æ®ç§»åŠ¨å¸¦å®½çš„è¦æ±‚è¾ƒä½ï¼Œå› ä¸ºå¯¹äºåŠ è½½çš„æ¯ä¸ªæ•°æ®ï¼ŒåŠ é€Ÿå™¨å¯ä»¥æ‰§è¡Œæ›´å¤šçš„è®¡ç®—ã€‚

![](time.png)

![](effice.png)

##  Quantifying AIT in DL training

æ¯æ¬¡è¿­ä»£çš„æ€»è®¡ç®—é‡ç”±Transformer çš„çº¿æ€§å±‚ä¸­çš„è®¡ç®—é‡ä¸»å¯¼ã€‚ In fact, for large NLP models, it is common that â„ğ‘‘ >> ğ‘ ğ‘’ğ‘, and thus the attention computation is a negligible part of the computation cost. For the forward propagation this can be approximated as a function of the number of parameters, sequence length, and batch size, given by 2 Ã— ğ‘ğ‘ ğ‘§ Ã— ğ‘ ğ‘’ğ‘ Ã— ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘ . The cost of backward propagation is approximately twice that of forward propagation. Additionally, activation checkpointing requires an additional forward computation as part of recomputation during backward propagation. Therefore, the total computation per iteration is:

![](com.png)

åœ¨å‘å‰å’Œå‘åä¼ æ’­æœŸé—´ï¼Œæ¨¡å‹å‚æ•°å¿…é¡»ä»æºä½ç½®åŠ è½½åˆ°GPUå¯„å­˜å™¨è‡³å°‘ä¸¤æ¬¡ï¼Œi)åœ¨å‘å‰æœŸé—´ï¼Œii)åœ¨å®é™…å‘åæœŸé—´ï¼Œå¯¼è‡´2å€çš„æ•°æ®ç§»åŠ¨ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ ã€‚åœ¨æ¿€æ´»æ£€æŸ¥ç‚¹å­˜åœ¨çš„æƒ…å†µä¸‹ï¼Œå‚æ•°å¯èƒ½ä¼šè¢«åŠ è½½ä¸€æ¬¡ï¼Œä»¥ä¾¿åœ¨å‘åä¼ é€’æœŸé—´é‡æ–°è®¡ç®—ï¼Œè¿™åˆå¢åŠ äº†1å€ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ ã€‚æ­¤å¤–ï¼Œæ¢¯åº¦å¿…é¡»ä»GPUå¯„å­˜å™¨å­˜å‚¨åˆ°å…¶æœ€ç»ˆä½ç½®è‡³å°‘ä¸€æ¬¡ï¼Œåœ¨æ•°æ®ç§»åŠ¨ä¸­æ·»åŠ æœ€ç»ˆçš„1 Ã—ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ ã€‚

å› æ­¤ï¼Œå‡è®¾å‚æ•°å’Œæ¢¯åº¦å­˜å‚¨åœ¨ç›¸åŒçš„æœ€ç»ˆä½ç½®ï¼Œåœ¨å‘å‰å’Œå‘åä¼ é€’æœŸé—´çš„æ€»æ•°æ®ç§»åŠ¨å°†æ˜¯4Ã—ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ ï¼Œå³2Ã—4Ã—ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ å­—èŠ‚ã€‚Therefore the ğ‘ğ‘–ğ‘¡ w.r.t parameter and gradients is ğ‘ ğ‘’ğ‘ Ã— ğ‘ğ‘ z

åœ¨ä¼˜åŒ–å™¨æ­¥éª¤æœŸé—´ï¼Œå¿…é¡»è‡³å°‘è¯»å–ä¸€æ¬¡ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå¹¶ä¸”å¿…é¡»è‡³å°‘å†™å…¥ä¸€æ¬¡ä¼˜åŒ–å™¨çŠ¶æ€ã€‚æ‰€ä»¥æ€»æ•°æ®ç§»åŠ¨2Ã—ğ‘œğ‘ğ‘¡ğ‘–ğ‘šğ‘–ğ‘§ğ‘’ğ‘Ÿ_ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘ ,è¿™å¤§çº¦æ˜¯2Ã—16Ã—ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ å­—èŠ‚ã€‚Thereforemğ‘ğ‘–ğ‘¡ w.r.t optimizer states during a full training iteration is ğ‘ ğ‘’ğ‘Ã—ğ‘ğ‘ ğ‘§/4.

During the forward propagation activation checkpoints must be saved to their final location, and must be retrieved during the backward propagation. Therefore, the total data movement w.r.t activation checkpoints in bytes is
given by 2Ã—ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™\_ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›\_ğ‘â„ğ‘’ğ‘ğ‘˜ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘ \_ğ‘–ğ‘›\_ğ‘ğ‘¦ğ‘¡ğ‘’ğ‘  which is given by 4Ã—ğ‘›ğ‘™/ğ‘ğ‘– Ã—â„ğ‘‘ Ã—ğ‘ ğ‘’ğ‘ Ã—ğ‘ğ‘ ğ‘§ from Eq. (1). The total computation per iteration is given by Sec. 4.1. So the ğ‘ğ‘–ğ‘¡ w.r.t activation checkpoints is given by 24 Ã— â„ğ‘‘ Ã— ğ‘ğ‘–.

## Bandwidth Requirements

ç”±äºAITçš„å˜åŒ–ï¼Œæ¨¡å‹çŠ¶æ€å’Œæ¿€æ´»æ£€æŸ¥ç‚¹å…·æœ‰éå¸¸ä¸åŒçš„å¸¦å®½éœ€æ±‚ï¼Œä»¥å®ç°è‰¯å¥½çš„æ•ˆç‡ã€‚å‰è€…ä»…å–å†³äºæ‰¹å¤§å°å’Œåºåˆ—é•¿åº¦ï¼Œåè€…ä»…å–å†³äºæ¿€æ´»æ£€æŸ¥ç‚¹çš„é¢‘ç‡å’Œæ¨¡å‹çš„éšè—ç»´åº¦å¤§å°ã€‚

Besides AIT, the bandwidth requirement for efficiency also depends on ğ‘ğ‘’ğ‘ğ‘˜ğ‘¡ğ‘ .Using ğ‘ğ‘’ğ‘ğ‘˜ğ‘¡ğ‘ , and ğ‘ğ‘–ğ‘¡ we first
show how efficiency varies with bandwidth w.r.t to different model and residual states, and then discuss the bandwidth requirements on these states for DL training to be efficient. 

![](3.png)

A small batch size per GPU is used when running on large number of GPUs, while a large batch size per GPU is used when training on relatively fewer GPUs to maintain a
reasonable effective batch size for training.

ğ‘ğ‘’ğ‘ğ‘˜ğ‘¡ğ‘ is not the theoretical hardware peak, but instead the achievable peak in the absence of any communication bottleneck. We ran models with aforementioned configurations on a single NVIDIA V100 DGX-2 box with all non-GPU communication turned off to simulate a zero external communication overhead scenario, or equivalently a virtually infinite external bandwidth scenario. The performance achieved ranged from 62-78 TFlops/GPU
based on the hidden size of 8K-64K, respectively. We used the average of 70 TFlops/GPU to represent ğ‘ğ‘’ğ‘ğ‘˜ğ‘¡ğ‘ for the purpose of this analysis.

å›¾3aæ˜¾ç¤ºï¼Œå½“å‚æ•°å’Œæ¢¯åº¦çš„å¸¦å®½è¶…è¿‡70 GB/sæ—¶ï¼Œå³ä½¿æ˜¯æœ€å°çš„æ‰¹å¤„ç†å¤§å°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å®ç°è¶…è¿‡50%çš„æ•ˆç‡ã€‚åœ¨è¿™ä¸ªå¸¦å®½ä¸‹ï¼Œç†è®ºä¸Šæ•°æ®ç§»åŠ¨å¯ä»¥ä¸è®¡ç®—å®Œå…¨é‡åˆï¼Œè¾¾åˆ°100%çš„æ•ˆç‡ã€‚

å›¾3bæ˜¾ç¤ºï¼Œä¸å‚æ•°å’Œæ¢¯åº¦ç›¸æ¯”ï¼Œä¼˜åŒ–å™¨çŠ¶æ€éœ€è¦è¿‘4å€çš„å¸¦å®½æ‰èƒ½è¾¾åˆ°50%çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œä¼˜åŒ–å™¨çŠ¶æ€åœ¨å‘å‰å’Œå‘åä¼ æ’­ç»“æŸæ—¶æ›´æ–°ï¼Œå¹¶ä¸”ä¸èƒ½ä¸è®¡ç®—é‡å ã€‚å› æ­¤ï¼Œå®ƒä»¬éœ€è¦æ›´å¤§çš„å¸¦å®½æ¥ä¿æŒæ•´ä¸ªDLå·¥ä½œè´Ÿè½½çš„æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œæ¯ä¸ªGPUçš„æ‰¹å¤„ç†å¤§å°ä¸º2ï¼Œè¦è¾¾åˆ°90%çš„æ•ˆç‡ï¼Œéœ€è¦è¿‘1.5 TB/sçš„æœ‰æ•ˆå¸¦å®½ï¼Œè¿™ç”šè‡³å¤§äºGPUçš„å†…å­˜å¸¦å®½ã€‚

å›¾3cè¿˜æ˜¾ç¤ºï¼Œå¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹åï¼Œå³ä½¿éšè—å¤§å°ä¸º2ğ¾ï¼Œ2 GB/sçš„å¾®è–„å¸¦å®½ä¹Ÿèƒ½å¤Ÿç»´æŒ50%ä»¥ä¸Šçš„æ•ˆç‡ã€‚å½“éšè—å¤§å°è¶…è¿‡8kæ—¶ï¼Œå¸¦å®½éœ€æ±‚ä¸‹é™åˆ°1 GB/sä»¥ä¸‹ã€‚

ï¼ˆå¸¦å®½éœ€æ±‚ï¼šä¼˜åŒ–å™¨>å‚æ•°=æ¢¯åº¦>æ¿€æ´»ï¼‰

#  ZERO-INFINITY DESIGN OVERVIEW

![](4.png)

## Design for Unprecedented Scale

ZeRO-Infinityæ”¯æŒæ¯ä¸ªNVIDIA V100 DGX-2èŠ‚ç‚¹1ä¸‡äº¿ä¸ªå‚æ•°ï¼Œæ¯”3Då¹¶è¡Œåº¦æé«˜äº†50å€ã€‚

ZeRO-Infinity is built on top of ZeRO-3 [11] which partitions all model states to remove memory redundancy as discussed in Sec. 2. Unlike any of the existing ZeRO family of technology, ZeRO-Infinity is designed with a powerful offload mechanism called the infinity offload engine which can offload all of the partitioned model states to CPU or NVMe memory, or keep them on the GPU based on the memory requirements.

é™¤äº†æ¨¡å‹çŠ¶æ€ä¹‹å¤–ï¼ŒZeRO-Infinityè¿˜å¯ä»¥åœ¨å¿…è¦æ—¶å°†æ¿€æ´»å†…å­˜å¸è½½åˆ°CPUå†…å­˜ä¸­ã€‚è¯·æ³¨æ„ï¼Œ10ä¸‡äº¿å‚æ•°æ¨¡å‹æ‰€éœ€çš„æ¿€æ´»æ£€æŸ¥ç‚¹(0.76 TB)å¯ä»¥è½»æ¾åœ°è£…å…¥DGX-2ç³»ç»Ÿä¸Šå¯ç”¨çš„1.5TB CPUå†…å­˜ä¸­ï¼Œè€Œ100ä¸‡äº¿å‚æ•°æ‰€éœ€çš„3 TBæ¿€æ´»æ£€æŸ¥ç‚¹åœ¨ä¸‹ä¸€ä»£ç¡¬ä»¶çš„CPUå†…å­˜èŒƒå›´å†…ã€‚é€šè¿‡å°†æ¿€æ´»æ£€æŸ¥ç‚¹å¸è½½åˆ°CPUå†…å­˜ä¸­ï¼ŒZeRO-Infinityå¯ä»¥æ‹Ÿåˆå…·æœ‰æ•°ä»¥ä¸‡äº¿è®¡å‚æ•°çš„æ¨¡å‹çš„æ¿€æ´»æ£€æŸ¥ç‚¹ã€‚

To reduce the working memory requirements of DL training for large models, ZeRO-Infinity introduces a novel technique called memory-centric tiling that exploits the data fetch and release pattern of ZeRO-3 to reduce the working memory requirements by breaking down a large operator into smaller tiles that can be executed sequentially.(æŠŠå¤§å¹¶è¡Œæ‹†æˆå°ä¸²è¡Œï¼Ÿ)

For example, to reduce the working memory for a large linear
operator, ZeRO-Infinity represents the operator as a mathematically equivalent sequence of smaller linear operators consisting of tiles of parameters from the original operator, and executes them sequentially. When combined with ZeRO-3, the parameter and gradients of each tile can be fetched and released one at a time, reducing the working memory proportional to the number of tiles.ï¼ˆåˆ†æˆå¤šå°‘tileå°±å‡å°‘å¤šå°‘æ¯”ä¾‹ï¼‰ Therefore, ZeRO-Infinity can support operators of arbitrary sizes, without relying on model parallelism to fit them in limited GPU memory.

## Design for Excellent Training Efficiency

Offloading all model states and activations to CPU or NVMe is only practical if ZeRO-Infinity can achieve high efficiency despite the offload.åœ¨ç°å®ä¸­ï¼Œè¿™æ˜¯æå…·æŒ‘æˆ˜æ€§çš„ï¼Œå› ä¸ºCPUå†…å­˜æ¯”GPUå†…å­˜å¸¦å®½æ…¢ä¸€ä¸ªæ•°é‡çº§ï¼ŒNVMeå¸¦å®½æ¯”CPUå†…å­˜å¸¦å®½è¿˜è¦æ…¢ä¸€ä¸ªæ•°é‡çº§ã€‚ä»GPUè¯»å–å’Œå†™å…¥è¿™äº›å†…å­˜ç”šè‡³æ›´æ…¢(è§å›¾2b)ã€‚æ ¹æ®æˆ‘ä»¬åœ¨ç¬¬4èŠ‚ä¸­çš„åˆ†æï¼Œåœ¨DGX-2è¿™æ ·çš„ç³»ç»Ÿä¸Šï¼Œå¸¦å®½å¿…é¡»å¤§äº70GB/sã€1.5TB/så’Œ1.4 GB/s w.r.t.å‚æ•°å’Œæ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œæ‰èƒ½æœ‰æ•ˆåœ°è¿›è¡Œæ·±åº¦å­¦ä¹ è®­ç»ƒã€‚

å‚æ•°å’Œæ¢¯åº¦çš„æ•°æ®ç§»åŠ¨å¸¦å®½å¿…é¡»å¤§äº70GB/sï¼Œæ¥è¿‘DGX-2é›†ç¾¤ä¸Šå¯ç”¨çš„GPU-GPUå¸¦å®½[42]ã€‚å› æ­¤ï¼ŒåƒZeRO3[11]è¿™æ ·çš„æ·±åº¦å­¦ä¹ å¹¶è¡Œè®­ç»ƒè§£å†³æ–¹æ¡ˆï¼Œåœ¨å‘å‰æˆ–å‘åä¼ æ’­ä¹‹å‰å°†å‚æ•°ä»æ‰€æœ‰è€…GPUå¹¿æ’­ç»™å…¶ä»–GPUï¼Œåªè¦é€šä¿¡æ˜¯é‡å çš„ï¼Œå°±å¯ä»¥é«˜æ•ˆè¿è¡Œã€‚

ç›¸åï¼Œä»å•ä¸ªGPUåˆ°CPUå†…å­˜æˆ–NVMeçš„å¾®è–„çš„12 GB/s PCIeå¸¦å®½(è§å›¾2b)æˆ–åä¹‹äº¦ç„¶ï¼Œæ ¹æœ¬ä¸è¶³ä»¥æ”¯æŒå¤§è§„æ¨¡çš„å¼‚æ„è®­ç»ƒã€‚CPUå’ŒNVMeå¸¦å®½åˆ†åˆ«ä¸º100gB /så’Œ25gB /sï¼Œä½†ä»CPUæˆ–NVMeè¯»å–æ•°æ®åˆ°å•ä¸ªGPUå—åˆ°å¯å®ç°çš„PCIeå¸¦å®½çš„é™åˆ¶ï¼Œå¤§çº¦ä¸º10- 12gB /s. Therefore, existing heterogeneous solutions like ZeRO-Offload where the parameters must be first moved from CPU to owner GPU before broadcasting requires significantly large batch sizes per GPU to achieve enough ğ‘ğ‘–ğ‘¡ necessary to be efficient under the limited bandwidth. This poses two problems: i) for massive models the activation memory will get too large to fit even in CPU memory, and ii) the effective batch size becomes too large when scaling to hundreds or thousands of GPUs for effective convergence.(ä¸€ä¸ªgpuä»£è¡¨ä¸€è·¯dp)

ZeRO-Infinityä»¥ä¸¤ç§æ–¹å¼è§£å†³äº†è¿™äº›æŒ‘æˆ˜:i)ä»¥å¸¦å®½ä¸ºä¸­å¿ƒçš„åˆ†åŒº:ä¸€ç§æ–°çš„æ•°æ®æ˜ å°„å’Œå¹¶è¡Œæ•°æ®æ£€ç´¢ç­–ç•¥ï¼Œç”¨äºå¸è½½å‚æ•°å’Œæ¢¯åº¦ï¼Œå…è®¸ZeRO-Infinityå®ç°å‡ ä¹æ— é™çš„å¼‚æ„å†…å­˜å¸¦å®½(è¯¦ç»†ä¿¡æ¯è§ç¬¬6.1èŠ‚) and ii) an overlap centric design that allows
ZeRO-Infinity to overlap not only GPU-GPU communication with computation but also NVMe-CPU and CPU-GPU communications over the PCIe (details in Sec. 5.1.3).

Unlike parameters and gradients that are consumed and produced sequentially during the forward and backward propagation, optimizer states can be updated in parallel, all at once. This property is leveraged by both ZeRO-3 and ZeRO-Offload, that store and update the optimizer
states in GPU and CPU memory, respectively, in parallel across all available GPUs and CPUs.(?) As a result the aggregate GPU or CPU memory bandwidth can get much higher than the required 1.5TB/s with increase in GPU or CPU count.

Since ZeRO-Infinity is built upon ZeRO-3, it can also leverage
the aggregate GPU and CPU memory bandwidth as well as the
aggregate CPU compute for optimizer step, when offloading optimizer states to CPU memory.ç„¶è€Œï¼Œä½¿ç”¨NVMeå¸è½½ï¼Œæœ‰å¿…è¦å°†æ•°æ®ä»NVMeå¸¦åˆ°CPUå†…å­˜ä¸­ï¼Œç„¶åä»¥é€‚åˆCPUå†…å­˜çš„å—çš„å½¢å¼è¿”å›ï¼Œä»¥æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ï¼Œæ¯æ¬¡ä¸€ä¸ªå—ã€‚å› æ­¤ï¼Œä¼˜åŒ–æ­¥éª¤å—åˆ°NVMe- cpuå†…å­˜å¸¦å®½çš„é™åˆ¶:è™½ç„¶ZeRO-Infinityå¯ä»¥è·¨å¤šä¸ªèŠ‚ç‚¹å®ç°èšåˆNVMeå¸¦å®½ï¼Œä½†å…³é”®æ˜¯è¦å®ç°æ¯ä¸ªèŠ‚ç‚¹æ¥è¿‘å³°å€¼çš„NVMeå¸¦å®½ï¼Œä»¥ä¾¿åœ¨å°½å¯èƒ½å°‘çš„èŠ‚ç‚¹å’Œå°½å¯èƒ½å°çš„æ‰¹é‡å¤§å°ä¸‹æ”¯æŒè¶…è¿‡1.5 TB/sçš„å¿…è¦å¸¦å®½ã€‚æ­¤å¤–ï¼Œå°†æ•°æ®ä»NVMeå¯¼å…¥åˆ°CPUå†…å­˜æˆ–ä»CPUå†…å­˜å¯¼å…¥åˆ°GPUå†…å­˜çš„è¿‡ç¨‹å¯èƒ½ä¼šå¯¼è‡´GPUå’ŒCPUä¸­çš„CPUå†…å­˜ç¢ç‰‡ï¼Œä»è€Œå¯¼è‡´å†…å­˜ä¸è¶³ï¼Œå³ä½¿ä»ç„¶æœ‰å¤§é‡å†…å­˜å¯ç”¨ã€‚

æ— é™å¸è½½å¼•æ“ä¸ä»…å¯ä»¥å®ç°æ¥è¿‘å³°å€¼çš„NVMeå¸¦å®½ï¼Œå®ƒè¿˜å¯ä»¥å…è®¸ZeRO-Infinityå°†NVMeåˆ°CPUçš„è¯»å–ä¸CPUåˆ°NVMeçš„å†™å…¥é‡å ï¼Œä»¥åŠä¼˜åŒ–å™¨æ­¥éª¤çš„CPUè®¡ç®—ï¼ŒåŒæ—¶å…è®¸ZeRO-Infinityåœ¨å°‘é‡gpuä¸Šä¿æŒé€‚åº¦çš„æ‰¹é‡å¤§å°ï¼Œå¹¶åœ¨å¤§é‡gpuä¸Šä¿æŒå°æ‰¹é‡å¤§å°ã€‚åŒæ—¶ï¼Œå®ƒé€šè¿‡å°å¿ƒåœ°ä¸ºæ•°æ®ç§»åŠ¨é‡ç”¨ä¸´æ—¶ç¼“å†²åŒºæ¥æœ€å°åŒ–å†…å­˜ç¢ç‰‡ã€‚

On a DGX-2 node, each GPU can read and write data at about 3 GB/s to CPU memory in parallel over the PCIe allowing activation checkpoints to be offloaded to CPU memory while retaining over 80% efficiency for hidden size larger 8ğ¾ or larger. ä¸ºäº†åœ¨è¾ƒå°çš„éšè—å°ºå¯¸ä¸‹å®ç°é«˜æ•ˆç‡ï¼ŒZeRO-Infinityå¯ä»¥å‡å°‘æ¿€æ´»æ£€æŸ¥ç‚¹çš„é¢‘ç‡ï¼Œå¹¶æœ‰æ•ˆåœ°å°†æ¿€æ´»æ£€æŸ¥ç‚¹ä¸CPUå†…å­˜ä¹‹é—´çš„é€šä¿¡ä¸GPUä¸Šçš„å‰å‘å’Œåå‘è®¡ç®—é‡å ã€‚

## Design for Ease of Use

æœ‰äº†ZeRO-Infinityï¼Œæ•°æ®ç§‘å­¦å®¶ä¸å†éœ€è¦è°ƒæ•´ä»–ä»¬çš„æ¨¡å‹æ¥é€‚åº”å¤šç§å½¢å¼çš„å¹¶è¡Œï¼Œæ¯”å¦‚3Då¹¶è¡Œã€‚ä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„ZeRO-Infinityå¹³é“ºï¼Œæ—¨åœ¨å‡å°‘å¤§å‹å•ä¸ªå±‚çš„GPUå†…å­˜éœ€æ±‚ï¼Œå¦åˆ™éœ€è¦æ¨¡å‹å¹¶è¡Œæ€§(å¼ é‡åˆ‡ç‰‡)æ¥é€‚åº”GPUå†…å­˜ä¸­çš„å±‚ã€‚

ZeRO-Infinityåœ¨PyTorchä¸­å®ç°çš„æ–¹å¼æ¶ˆé™¤äº†æ‰‹åŠ¨æ¨¡å‹ä»£ç é‡æ„çš„éœ€è¦ï¼Œå³ä½¿æ‰©å±•åˆ°æ•°ä¸‡äº¿ä¸ªå‚æ•°ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ªç®€å•çš„å®ç°å’Œä¸‰ä¸ªè‡ªåŠ¨åŒ–åŠŸèƒ½å®ç°çš„:

I)è‡ªåŠ¨æ•°æ®ç§»åŠ¨ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦ä¹‹å‰æ”¶é›†å‚æ•°ã€‚ZeRO-Infinityé€šè¿‡åœ¨PyTorchå­æ¨¡å—ä¸­æ³¨å…¥å‰å‘/åå‘é’©å­æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¿™äº›é’©å­ä¼šè§¦å‘allgatheré›†åˆæ¥æ”¶é›†å‰å‘/åå‘ä¼ é€’æ‰€éœ€çš„å‚æ•°ã€‚

Ii)åœ¨å­æ¨¡å—çš„å‘å‰/å‘åä¼ é€’ç»“æŸæ—¶ï¼Œå½“ä¸å†éœ€è¦å±äºå­æ¨¡å—çš„å‚æ•°æ—¶ï¼Œè‡ªåŠ¨è¿›è¡Œå‚æ•°åˆ’åˆ†ã€‚å†ä¸€æ¬¡ï¼ŒZeRO-Infinityå°†post forward/backward hookæ³¨å…¥å­æ¨¡å—ï¼Œå¯¹å‚æ•°è¿›è¡Œåˆ†åŒºï¼Œå¹¶å¯é€‰æ‹©å°†å®ƒä»¬å¸è½½ç»™CPUæˆ–NVMeã€‚

iii)åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è‡ªåŠ¨åˆ’åˆ†æ¨¡å‹ï¼Œè¿™æ ·ï¼Œä¸èƒ½åœ¨å•ä¸ªGPUæˆ–CPUå†…å­˜ä¸­é€‚åˆçš„æ¨¡å‹ä»ç„¶å¯ä»¥åˆå§‹åŒ–ï¼Œè€Œä¸éœ€è¦è·¨æ•°æ®å¹¶è¡Œè¿›ç¨‹å¯¹æ¨¡å‹è¿›è¡Œæ‰‹åŠ¨åˆ’åˆ†ã€‚ZeRO-Infinityé€šè¿‡åŒ…è£…æ‰€æœ‰æ¨¡å—ç±»çš„æ„é€ å‡½æ•°æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä»¥ä¾¿åœ¨åˆå§‹åŒ–æœŸé—´åˆ›å»ºæ¯ä¸ªå­æ¨¡å—çš„å‚æ•°åç«‹å³å¯¹å…¶è¿›è¡Œåˆ†åŒºå’Œå¸è½½ã€‚æ•´ä¸ªæ¨¡å‹æ°¸è¿œä¸ä¼šåœ¨å•ä¸ªæ•°æ®å¹¶è¡Œè¿›ç¨‹ä¸Šå®Œå…¨å®ä¾‹åŒ–ã€‚

# EFFICIENCY OPTIMIZATIONS

## Bandwidth-Centric Partitioning

ZeRO-Infinityå®ç°äº†ä¸€ç§æ–°çš„æ•°æ®æ˜ å°„å’Œæ£€ç´¢ç­–ç•¥ï¼Œä»¥è§£å†³NVMeå’ŒCPUå†…å­˜å¸¦å®½çš„é™åˆ¶ã€‚Unlike ZeRO [11] and ZeRO-Offload [12], where parameters of each layer are owned by a single data parallel process, which broadcasts them to the rest when needed, ZeRO-Infinity partitions individual parameters across all the data parallel process, and uses an allgather
instead of a broadcast when a parameter needs to be accessed.(æœ‰ä»€ä¹ˆåŒºåˆ«)è¯·æ³¨æ„ï¼Œå¦‚æœæ•°æ®ä½äºGPUä¸Šï¼Œé‚£ä¹ˆbroadcastå’Œallgatheré€šä¿¡é›†åˆåœ¨æ•°æ®ç§»åŠ¨é‡æ–¹é¢å…·æœ‰ç›¸åŒçš„é€šä¿¡æˆæœ¬ã€‚å› æ­¤ï¼Œè¿™å¯¹äºä»…ä½¿ç”¨gpuçš„è®­ç»ƒæ²¡æœ‰ä»€ä¹ˆåŒºåˆ«ã€‚ç„¶è€Œï¼Œå½“æ•°æ®ä½äºNVMeæˆ–CPUä¸­æ—¶ï¼Œè¿™å°†æ”¹å˜æ¸¸æˆè§„åˆ™ã€‚

In the broadcast-based approach, since each parameter is fully owned by one of the data parallel processes, the parameter must be first communicated from its source location (CPU or NVMe) to the GPU memory via the PCIe before the broadcast can happen. Since, the broadcast is done from the GPU device, this communication step is
needed to bring the data from NVMe/CPU to GPU. However, this communication step is not part of the broadcast itselfã€‚ï¼ˆå…ˆä¼ åˆ°gpuï¼Œå†æ€ä¹ˆä¼ ï¼Ÿï¼‰Note that only a single PCIe can be active for this process, while all the PCIe links connected to all the other GPUs are idle. On the contrary, with the partitioned parameter and allgather based approach in ZeRO-Infinity, all PCIe links are active in parallel, each bringing in 1/ğ‘‘ğ‘ğ‘¡â„ portion of the parameter where ğ‘‘ğ‘ is the data parallel degree. As a result, the effective communication bandwidth between NVMe or CPU to the GPU, increases linearly with the ğ‘‘ğ‘
degree.

For example, with broadcast-based approach, the CPU/NVMe to GPU bandwidth stays constant at about 12 GB/s with PCIe Gen 3, even with 16-way data parallelism on the DGX-2 box. However, with the all-gather-based approach, the effective achievable bandwidth increases to about 48/25 GB/s (3.0/1.6 GB/s per GPU), respectively (see Fig. 2b), limited only by the max aggregate PCIe bandwidth and max NVMe bandwidth per DGX-2 node. From here, the bandwidth grows linearly with more nodes. When training a massive model at massive scale, ZeRO-Infinity can therefore offer significantly more heterogeneous memory bandwidth than necessary (virtually
unlimited) for the training to remain efficient. For example, on
64 DGX-2 nodes, ZeRO-Infinity has access to over 3TB/s of CPU memory bandwidth and over 1.5TB/s of NVMe bandwidth.

## Overlap Centric Design

è™½ç„¶ZeRO-Infinityå¯ä»¥åœ¨å¤šèŠ‚ç‚¹è®¾ç½®ä¸Šåˆ©ç”¨è¶³å¤Ÿçš„å¼‚æ„å†…å­˜å¸¦å®½ï¼Œä½†å¸¦å®½ä»ç„¶å¯èƒ½æˆä¸ºå•ä¸ªGPUæˆ–å•ä¸ªèŠ‚ç‚¹è®¾ç½®çš„ç“¶é¢ˆã€‚ Even the GPU-GPU allgather communication has a big impact on efficiency when running with a small batch size (Fig. 3). Furthermore, accessing NVMe memory requires a three step process: i) read data from NVMe to CPU memory (nc-transfer), ii) copy the data from CPU memory to GPU memory (cg-transfer), iii) execute allgather to construct the full parameter on all GPUs (gg-transfer). è¿™äº›æ•°æ®ç§»åŠ¨çš„é¡ºåºæ€§è´¨æ„å‘³ç€ï¼Œå¦‚æœç®€å•åœ°è¿›è¡Œï¼Œæ€»é€šä¿¡æ—¶é—´å°†æ˜¯è¿™ä¸‰ä¸ªæ•°æ®ç§»åŠ¨æˆæœ¬çš„æ€»å’Œï¼Œå³ä½¿æ¯ä¸ªé˜¶æ®µçš„æ•°æ®ç§»åŠ¨å¸¦å®½éƒ½è¶³å¤Ÿï¼Œä¹Ÿä¼šå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

To address these issues, ZeRO-Infinity has an overlap engine
that not only overlaps GPU-GPU communication with GPU computation, but also overlaps the NVMe to CPU, and CPU to GPU communication, all at the same time. The overlap engine has two components: i) A dynamic prefetcher for overlapping the data movement required to reconstruct parameters before they are consumed in the forward or backward pass, andï¼ˆå–å‚æ•°å’Œè®¡ç®—é‡å ï¼‰ ii) a communication and offload overlapping mechanism for executing the data movement required by gradients in parallel with the backward computation.ï¼ˆå¸è½½æ¢¯åº¦å’Œè®¡ç®—é‡å ï¼‰

The dynamic prefetcher in ZeRO-Infinity traces the forward and backward computation on that fly, constructing an internal map of the operator sequence for each iteration. During each iteration, the prefetcher keeps track of where it is in the operator sequence and prefetches the parameter requires by the future operators.ï¼ˆé¢„å–å‚æ•°) The prefetcher is aware of the three step communication process, and
therefore can overlap the nc-transfer for one parameter, with cgtransfer and gg-transfer of other parameters. For instance, before executing the ğ‘–ğ‘¡â„ operator, the prefetcher can invoke nc, cg, and gg-transfer for parameters required by ğ‘– +3,ğ‘– +2, and ğ‘– +1 operators, respectively. Note that all of these data movement can happen in parallel with the execution of the ğ‘–
ğ‘¡â„ operator. Furthermore,ZeRO-Infinity can update the operator sequence map in case of dynamic workflow, allowing for appropriate prefetching even when the forward and backward propagation changes across iterations.

Similarly, in the backward pass, ZeRO-Infinity can overlap the
reduce-scatter for gradients of the parameters in (ğ‘– + 1)
ğ‘¡â„ operator with the computation of the ğ‘–ğ‘¡â„ operator, while simultaneous transferring the partitioned gradients from the reduce-scatter of the gradients of the (ğ‘– + 2)ğ‘¡â„ operator to the CPU or NVMe

é€šè¿‡è¿™ç§å¼ºå¤§çš„ä»¥é‡å ä¸ºä¸­å¿ƒçš„è®¾è®¡ï¼ŒZeRO-Infinityéšè—äº†æ•°æ®ç§»åŠ¨çš„é‡è¦éƒ¨åˆ†ï¼Œå³ä½¿åœ¨ä½¿ç”¨å°‘é‡GPUå’Œæ¯ä¸ªGPUçš„å°æ‰¹é‡å¤§å°è¿›è¡Œè®­ç»ƒæ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚

##  Infinity Offload Engine

DeepNVMe, a powerful C++ NVMe read/write library in the infinity offload engine that supports bulk read/write requests for asynchronous completion, and explicit synchronization requests to flush ongoing read/writes. The support for asynchrony allows ZeROInfinity to overlap these requests with GPU/GPU or GPU/CPU communication or computation.

Most importantly, DeepNVMe is capable of achieving near peak sequential read and write bandwidths on the NVMe storage device. It achieves this high performance through a number of optimizations, including aggressive parallelization of I/O requests (whether from a single user thread or across multiple user threads), smart work scheduling, avoiding data copying, and memory pinning.

To ensure high performance tensor reads (or writes) from (to) NVMe/CPU storage, the source (or destination) tensors must reside in pinned memory buffers. However, pinned memory buffers are scarce system resources, and their oversubscription by a single process can degrade
overall system performance or cause system instability. This layer manages the limited supply of pinned memory by reusing a small amount (tens of GBs) for offloading the entire model states (up to tens of TBs) to CPU or NVMe. The reuse of memory buffer prevents memory fragmentation in CPU and GPU memory. This layer also provides PyTorch tensors with pinned memory data, allowing inplace computation of the tensors so that they can then be written to NVMe without any further copies to improve bandwidth

# EVALUATION

å¯¹äºæ²¡æœ‰æ¨¡å‹å¹¶è¡Œæ€§çš„å®éªŒï¼Œæˆ‘ä»¬ä½¿ç”¨torchçš„åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ(DDP[43])ä½œä¸ºåŸºçº¿ã€‚å¯¹äºæ¨¡å‹å¹¶è¡Œæ€§çš„å®éªŒï¼Œæˆ‘ä»¬ä½¿ç”¨Megatron-LM[7]ã€‚ä½œä¸ºæ¯ä¸ªå®éªŒçš„åŸºçº¿ï¼Œæˆ‘ä»¬ä½¿ç”¨3D Parallelism[13]ã€ZeRO[11]æˆ–ZeRO- offload[12]ä¸­çš„ç›¸å…³æœ€å…ˆè¿›æ–¹æ³•ã€‚

æˆ‘ä»¬ä»…åœ¨èšåˆGPUå†…å­˜ä¸è¶³æ—¶å¸è½½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦å¸è½½åˆ°å…·æœ‰è¶³å¤Ÿå®¹é‡çš„æœ€å¿«å†…å­˜ï¼Œå› ä¸ºå®ƒä»¥æœ€å°çš„é€šä¿¡å¼€é”€èŠ‚çœäº†æœ€å¤§çš„å†…å­˜ï¼ˆï¼Ÿï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œåœ¨å‚æ•°å’Œæ¿€æ´»æ£€æŸ¥ç‚¹ä¹‹é—´ï¼Œå¦‚æœåªæœ‰ä¸€ä¸ªéœ€è¦å¸è½½åˆ°CPUå†…å­˜ï¼Œæˆ‘ä»¬æ ¹æ®ç»éªŒé€‰æ‹©å¸è½½æä¾›æ›´å¥½æ€§èƒ½çš„é‚£ä¸ªã€‚å½“ä¸¤è€…éƒ½éœ€è¦å¸è½½æ—¶ï¼Œæ¿€æ´»æ£€æŸ¥ç‚¹è¢«å¸è½½åˆ°CPUï¼Œå‚æ•°è¢«å¸è½½åˆ°å…·æœ‰è¶³å¤Ÿå®¹é‡çš„æœ€å¿«å†…å­˜ã€‚

ZeRO-Infinityå¯ä»¥è®­ç»ƒè¶…è¿‡32ä¸‡äº¿ä¸ªå‚æ•°çš„æ¨¡å‹ï¼Œè€Œ3Då¹¶è¡Œæ€§å¤§çº¦ä¸º650Bä¸ªå‚æ•°ï¼Œæä¾›äº†50å€çš„æ¨¡å‹è§„æ¨¡é£è·ƒã€‚

ZeROInfinityå¯ä»¥è®­ç»ƒå¤šè¾¾20ä¸‡äº¿ä¸ªå‚æ•°æ¨¡å‹(å¤§40å€)ï¼Œååé‡é«˜è¾¾49 (34?å†™é”™äº†ï¼‰TFlops/GPUã€‚è¿™å¤§çº¦æ˜¯è¯¥é›†ç¾¤ç†è®ºå³°å€¼æ€§èƒ½çš„40%ï¼Œä½†è¶…è¿‡äº†70 TFlopsçš„å¯å®ç°å³°å€¼æ€§èƒ½(ğ‘ğ‘’ğ‘ğ‘˜ğ‘¡ğ‘)çš„70%ã€‚At the extreme-scale, Figure 5a shows a performance drop from
10T (43 TFlops/GPU), and 20T (34 TFlops/GPU). This drop is not due to NVMe bandwidth as both model sizes use NVMe offloadï¼ˆç”¨æ¥å­˜å‚¨model state?ï¼‰, but instead due to an extremely small batch size per GPU (Table 1)at 20T scale as a result of limited CPU memory to store activation checkpoints. ï¼ˆå°cpuå†…å­˜å¯¼è‡´èƒ½å¤Ÿå­˜å‚¨çš„æ¿€æ´»å°å¯¼è‡´batchå°ï¼Œè®¡ç®—åˆ©ç”¨ç‡ä¸é«˜ï¼Ÿï¼‰This can be improved by increasing the CPU memory or offloading activation checkpoints to NVMe in a future implementation.

![](size.png)

å›¾5bæ˜¾ç¤ºï¼ŒZeRO-Infinityåœ¨è®­ç»ƒ1Tæ¨¡å‹æ—¶å®ç°äº†ä»4ä¸ªèŠ‚ç‚¹(64ä¸ªgpu)åˆ°32ä¸ªèŠ‚ç‚¹(512ä¸ªgpu)çš„è¶…çº¿æ€§å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬å°†æ¯ä¸ªèŠ‚ç‚¹çš„æ‰¹å¤§å°ä¿æŒä¸å˜ï¼Œå¹¶éšç€èŠ‚ç‚¹æ•°é‡çš„å¢åŠ è€Œå¢åŠ æ€»æ‰¹å¤§å°ã€‚ZeRO-Infinityé€šè¿‡æœ‰æ•ˆåœ°åˆ©ç”¨èšåˆPCIeå’ŒNVMeå¸¦å®½çš„çº¿æ€§å¢é•¿æ¥åŠ é€Ÿå‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„å¸è½½ï¼Œå¹¶åˆ©ç”¨æ¥è‡ªå…¶ä»–èŠ‚ç‚¹çš„CPUè®¡ç®—æ¥è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œä»è€Œè¶…è¶Šäº†å®Œç¾çš„çº¿æ€§æ‰©å±•ã€‚ZeRO-Infinityä»…ç”¨4ä¸ªèŠ‚ç‚¹å°±å·²ç»å®ç°äº†è¶…è¿‡2.8 petaflops (44 Tflops/GPU)ï¼Œè¿™è¡¨æ˜å³ä½¿åœ¨é€‚åº¦çš„è§„æ¨¡ä¸‹ï¼Œèšåˆçš„NVMeå¸¦å®½ä¹Ÿè¶³ä»¥å®ç°è‰¯å¥½çš„æ•ˆç‡ã€‚

![](super.png)

å›¾5cæ˜¾ç¤ºäº†åœ¨æ²¡æœ‰ä»»ä½•æ¨¡å‹å¹¶è¡Œæ€§çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨ZeRO-Infinityåœ¨å•ä¸ªèŠ‚ç‚¹(16ä¸ªgpu)ä¸Šè®­ç»ƒ10Båˆ°1Tæ¨¡å‹çš„æ€§èƒ½ã€‚å‡­å€Ÿé«˜è¾¾1000äº¿ä¸ªå‚æ•°çš„æ¨¡å‹ï¼ŒZeRO-Infinityå®ç°äº†è¶…è¿‡40 TFlops/GPUçš„å“è¶Šæ€§èƒ½ï¼Œä½¿å¾—åªéœ€ä¸€ä¸ªDGX-2ç›’å³å¯å¯¹GPT-3ç­‰æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ3Då¹¶è¡Œæ€§æ— æ³•æ‰©å±•åˆ°è¶…è¿‡200äº¿ä¸ªå‚æ•°çš„æ¨¡å‹ã€‚

![](single.png)

![](max.png)

æˆ‘ä»¬è¯„ä¼°äº†ä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„å¹³é“ºåœ¨å­˜åœ¨å†…å­˜ç¢ç‰‡çš„æƒ…å†µä¸‹å¯ç”¨å¤§çš„éšè—å¤§å°çš„å½±å“ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå…·æœ‰ä¸åŒéšè—å°ºå¯¸å’Œå¹³é“ºå› å­çš„å•å±‚å˜å‹å™¨æ¨¡å‹ï¼Œä»¥ç¡®å®šåœ¨å¹³é“ºå’Œä¸å¹³é“ºæƒ…å†µä¸‹å¯ä»¥è®­ç»ƒçš„æœ€å¤§éšè—å°ºå¯¸ã€‚ä¸ºäº†åœ¨æ‰€æœ‰å®éªŒä¸­ä¿æŒå†…å­˜ç¢ç‰‡ä¸€è‡´ï¼Œæˆ‘ä»¬å°†æ€»GPUå†…å­˜é¢„åˆ†å‰²ä¸º2GBè¿ç»­å—ï¼Œä»¥ä¾¿æ‰€æœ‰å¤§äº2GBçš„å†…å­˜åˆ†é…è¯·æ±‚éƒ½å°†å¤±è´¥ã€‚

![](hidden.png)

åœ¨æ²¡æœ‰ä»¥å†…å­˜ä¸ºä¸­å¿ƒå¹³é“ºçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥è®­ç»ƒçš„æœ€å¤§éšè—å¤§å°ä¸º8Kï¼Œè€Œæˆ‘ä»¬ç”šè‡³å¯ä»¥ä½¿ç”¨ä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„å¹³é“ºç³»æ•°ä¸º16æ¥è®­ç»ƒä¸€ä¸ªå·¨å¤§çš„éšè—å¤§å°ä¸º64Kã€‚é€šè¿‡ä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„å¹³é“ºï¼ŒZeRO-Infinityé€šè¿‡é¿å…å¯¹æ¨¡å‹å¹¶è¡Œæ€§çš„éœ€æ±‚ï¼Œæå¤§åœ°ç®€åŒ–äº†æ·±åº¦å­¦ä¹ ç³»ç»Ÿå †æ ˆï¼Œä½¿æ•°æ®ç§‘å­¦å®¶å¯ä»¥è½»æ¾åœ°è®­ç»ƒå¤§çš„éšè—å°ºå¯¸ã€‚

![](offload.png)

å›¾6cæ˜¾ç¤ºäº†0 - infinityå’Œ0 - offloadå¯¹CPUå†…å­˜çš„å¸è½½æ¢¯åº¦å¯¹8Bå‚æ•°æ¨¡å‹çš„åå‘ä¼ æ’­æ—¶é—´çš„å½±å“ã€‚ZeRO-Infinityåˆ©ç”¨gpuçš„èšåˆPCIeå¸¦å®½æ¥å¸è½½æ¢¯åº¦ï¼Œä¸å—å•ä¸ªPCIeå¸¦å®½é™åˆ¶çš„ZeRO-Offloadç›¸æ¯”ï¼Œ64ä¸ªgpuçš„é€Ÿåº¦æé«˜äº†è¿‘2å€ã€‚

![](overlap.png)

é¢„å–å’Œé‡å å¯¹äºåœ¨æ¯ä¸ªGPUçš„å°æ‰¹é‡å¤§å°ä¸‹è·å¾—è‰¯å¥½çš„æ€§èƒ½è‡³å…³é‡è¦ï¼Œè€Œåœ¨å¤§æ‰¹é‡å¤§å°ä¸‹å…¶å½±å“ä¼šå‡å¼±ã€‚(å¤§æ‰¹é‡è®¡ç®—æ•ˆç‡é«˜)

![](offload_ac.png)

å¯¹äºè¾ƒå°çš„éšè—å¤§å°ï¼ŒZeRO-Infinityä¸­æ¿€æ´»æ£€æŸ¥ç‚¹çš„CPUå¸è½½æœ€å¤šå¯å°†è®­ç»ƒååé‡é™ä½1.2å€ï¼Œä½†å¯¹äºéšè—å¤§å°32Kå’Œ64Kï¼Œå½±å“æœ€å°ï¼Œè¿™è¡¨æ˜å¯ä»¥å°†æ¿€æ´»æ£€æŸ¥ç‚¹å¸è½½åˆ°CPUå†…å­˜ä¸­ï¼Œè€Œä¸ä¼šå½±å“å¤§å‹éšè—å¤§å°çš„æ•ˆç‡

![](predict.png)

è¡¨4çš„å¯¹æ¯”ç»“æœè¡¨æ˜ï¼Œåœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼Œå®ç°çš„æ€§èƒ½éƒ½åœ¨é¢„æµ‹èŒƒå›´å†…ï¼ŒéªŒè¯äº†ç¬¬4èŠ‚å¸¦å®½åˆ†æçš„æ­£ç¡®æ€§ã€‚æ­¤å¤–ï¼Œå¯¹äºè¾ƒå°çš„æ¨¡å‹ï¼Œæ€§èƒ½æ›´æ¥è¿‘äºä¸‹èŒƒå›´ï¼Œè€Œå¯¹äºè¾ƒå¤§çš„æ¨¡å‹ï¼Œæ€§èƒ½æ›´æ¥è¿‘äºä¸ŠèŒƒå›´ã€‚è¿™æ˜¯å› ä¸ºğ‘ğ‘’ğ‘ğ‘˜ğ‘¡ğ‘éšç€éšè—å¤§å°ä»62åˆ°78 TFlopsçš„å¢åŠ è€Œå¢åŠ ã€‚

while it is clear that efficiency drops with the decrease
in batch size, note that at 512 GPU scale, the calculated efficiency degradation is larger w.r.t parameters and gradients than w.r.t optimizer states. The primary source of efficiency degradation w.r.t parameters and gradients is the limited GPU-GPU bandwidth of 70GB/s. Therefore, this bandwidth is the most prominent source of performance bottleneck in ZeRO-Infinity when batch size is small(?)

#  CONCLUSION & FUTURE IMPLICATIONS

é€šè¿‡åœ¨å¤šä¸ªè®¾å¤‡ä¸Šå¹¶è¡Œåˆ©ç”¨å»‰ä»·ã€ç¼“æ…¢ä½†å·¨å¤§çš„CPUæˆ–NVMeå†…å­˜æ¥å®ç°åœ¨å½“å‰ä¸€ä»£GPUé›†ç¾¤ä¸Šè¿›è¡Œæœ‰æ•ˆè®­ç»ƒæ‰€éœ€çš„èšåˆå¸¦å®½ï¼Œä»è€Œæœ‰å¯èƒ½è¶…è¶ŠGPUå†…å­˜å¢™ã€‚

å¾ˆæ˜æ˜¾ï¼Œæœ‰äº†ZeRO-Infinityï¼ŒåŠ é€Ÿå™¨è®¾å¤‡å†…å­˜ä¸å†æ˜¯æ¨¡å‹è§„æ¨¡æˆ–è®­ç»ƒæ•ˆç‡çš„é™åˆ¶ã€‚ç„¶è€Œï¼Œåœ¨åˆç†çš„æ—¶é—´å†…è®­ç»ƒå…·æœ‰æ•°åä¸‡äº¿æˆ–æ•°ç™¾ä¸‡äº¿å‚æ•°çš„æ¨¡å‹ä»ç„¶éœ€è¦è®¡ç®—èƒ½åŠ›çš„å·¨å¤§é£è·ƒï¼Œå¹¶ä¸”åœ¨è¿™äº›æœªæ¥è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œéœ€è¦è®¾å¤‡åˆ°è®¾å¤‡å¸¦å®½çš„æˆæ¯”ä¾‹é£è·ƒ(è¡¨3)ã€‚

# .

zero-3:æ¨¡å‹çŠ¶æ€åˆ’åˆ†

æ¯ä¸€å±‚ï¼šbroadcastå‚æ•°ï¼Œå‰å‘ï¼Œä¸¢å‚æ•°

æ¯ä¸€å±‚ï¼šbroadcastå‚æ•°ï¼Œåå‘ï¼Œreduceæ¢¯åº¦ï¼Œä¸¢å‚æ•°ã€æ¢¯åº¦

ç”¨æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€å’Œæ›´æ–°å‚æ•°

ZeRO-Infinityæ˜¯æ•°æ®å¹¶è¡Œè®­ç»ƒçš„ä¸€ç§å½¢å¼

ä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„å¹³é“º(memory-centric tiling)ï¼Œä»¥æ”¯æŒè¶…å¤§çš„å•ä¸ªå±‚ï¼Œå¦åˆ™GPUå†…å­˜ç”šè‡³æ— æ³•ä¸€æ¬¡å®¹çº³ä¸€å±‚ï¼Œé€šè¿‡å°†å¤§å‹æ“ä½œåˆ†è§£ä¸ºå¯ä»¥é¡ºåºæ‰§è¡Œçš„è¾ƒå°çš„å¹³é“ºæ¥å‡å°‘å·¥ä½œå†…å­˜éœ€æ±‚ã€‚When combined with ZeRO-3, the parameter and gradients
of each tile can be fetched and released one at a time, reducing the working memory proportional to the number of tiles.

ä½¿ç”¨Adamä¼˜åŒ–å™¨è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒæ—¶ï¼Œå‚æ•°å’Œæ¢¯åº¦å­˜å‚¨åœ¨FP16ä¸­ï¼Œä¼˜åŒ–å™¨çŠ¶æ€ç”±FP32åŠ¨é‡ã€æ–¹å·®ã€å‚æ•°å’Œæ¢¯åº¦ç»„æˆã€‚

å¿…é¡»è‡³å°‘æœ‰è¶³å¤Ÿçš„è¿ç»­å†…å­˜æ¥ä¿å­˜å‚æ•°åŠå…¶æ¢¯åº¦ä»¥è¿›è¡Œåå‘ä¼ æ’­ã€‚

åœ¨æ‰§è¡Œå®é™…çš„å‘åä¼ æ’­ä¹‹å‰é‡æ–°è®¡ç®—æ¿€æ´»æ‰€éœ€çš„å†…å­˜ã€‚è¿™æ˜¯ä¸¤ä¸ªè¿ç»­æ¿€æ´»æ£€æŸ¥ç‚¹ä¹‹é—´çš„æ¿€æ´»å¤§å°ã€‚

![](effice.png)

åœ¨æ¿€æ´»æ£€æŸ¥ç‚¹å­˜åœ¨çš„æƒ…å†µä¸‹ï¼Œå‚æ•°å¯èƒ½ä¼šè¢«å†åŠ è½½ä¸€æ¬¡ï¼Œæ¢¯åº¦å¿…é¡»ä»GPUå¯„å­˜å™¨å­˜å‚¨åˆ°å…¶æœ€ç»ˆä½ç½®è‡³å°‘ä¸€æ¬¡ï¼ˆå‚æ•°æˆ–æ¢¯åº¦ä¸€å…±ç§»åŠ¨4æ¬¡ï¼Œåœ¨cpuæ›´æ–°å‚æ•°ï¼Ÿï¼‰

ğ‘ğ‘–ğ‘¡ w.r.t parameter and gradients is ğ‘ ğ‘’ğ‘ Ã— ğ‘ğ‘ ğ‘§.

åœ¨ä¼˜åŒ–å™¨æ­¥éª¤æœŸé—´ï¼Œå¿…é¡»è‡³å°‘è¯»å–ä¸€æ¬¡ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå¹¶ä¸”å¿…é¡»è‡³å°‘å†™å…¥ä¸€æ¬¡ä¼˜åŒ–å™¨çŠ¶æ€ã€‚

ğ‘ğ‘–ğ‘¡ w.r.t optimizer states during a full training iteration is ğ‘ ğ‘’ğ‘Ã—ğ‘ğ‘ ğ‘§/4.

ğ‘ğ‘–ğ‘¡ w.r.t activation checkpoints is given by 24 Ã— â„ğ‘‘ Ã— ğ‘ğ‘–.

ç”±äºAITçš„å˜åŒ–ï¼Œæ¨¡å‹çŠ¶æ€å’Œæ¿€æ´»æ£€æŸ¥ç‚¹å…·æœ‰éå¸¸ä¸åŒçš„å¸¦å®½éœ€æ±‚ï¼Œä»¥å®ç°è‰¯å¥½çš„æ•ˆç‡ã€‚

å½“å‚æ•°å’Œæ¢¯åº¦çš„å¸¦å®½è¶…è¿‡70 GB/sæ—¶ï¼Œå³ä½¿æ˜¯æœ€å°çš„æ‰¹é‡å¤§å°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å®ç°50%ä»¥ä¸Šçš„æ•ˆç‡ã€‚åœ¨è¿™ä¸ªå¸¦å®½ä¸‹ï¼Œç†è®ºä¸Šæ•°æ®ç§»åŠ¨å¯ä»¥ä¸è®¡ç®—å®Œå…¨é‡åˆï¼Œè¾¾åˆ°100%çš„æ•ˆç‡

ä¸å‚æ•°å’Œæ¢¯åº¦ç›¸æ¯”ï¼Œä¼˜åŒ–å™¨çŠ¶æ€éœ€è¦è¿‘4å€çš„å¸¦å®½æ‰èƒ½è¾¾åˆ°50%çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œä¼˜åŒ–å™¨çŠ¶æ€åœ¨å‘å‰å’Œå‘åä¼ æ’­ç»“æŸæ—¶æ›´æ–°ï¼Œå¹¶ä¸”ä¸èƒ½ä¸è®¡ç®—ï¼ˆå‰åå‘ï¼‰é‡å ã€‚

å¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹åï¼Œå³ä½¿éšè—å¤§å°ä¸º2ğ¾ï¼Œ2 GB/sçš„å¾®è–„å¸¦å®½ä¹Ÿèƒ½å¤Ÿç»´æŒ50%ä»¥ä¸Šçš„æ•ˆç‡ã€‚

é€šè¿‡å°†æ¿€æ´»æ£€æŸ¥ç‚¹å¸è½½åˆ°CPUå†…å­˜ä¸­ï¼ŒZeRO-Infinityå¯ä»¥æ‹Ÿåˆå…·æœ‰æ•°ä»¥ä¸‡äº¿è®¡å‚æ•°çš„æ¨¡å‹çš„æ¿€æ´»æ£€æŸ¥ç‚¹ã€‚

åœ¨DGX-2è¿™æ ·çš„ç³»ç»Ÿä¸Šï¼Œå¸¦å®½å¿…é¡»å¤§äº70GB/sã€1.5TB/så’Œ1.4 GB/s w.r.t.å‚æ•°å’Œæ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œæ‰èƒ½æœ‰æ•ˆåœ°è¿›è¡Œæ·±åº¦å­¦ä¹ è®­ç»ƒã€‚

å‚æ•°å’Œæ¢¯åº¦çš„æ•°æ®ç§»åŠ¨å¸¦å®½å¿…é¡»å¤§äº70GB/sï¼Œæ¥è¿‘DGX-2é›†ç¾¤ä¸Šå¯ç”¨çš„GPU-GPUå¸¦å®½[42]ã€‚å› æ­¤ï¼ŒåƒZeRO3[11]è¿™æ ·çš„æ·±åº¦å­¦ä¹ å¹¶è¡Œè®­ç»ƒè§£å†³æ–¹æ¡ˆï¼Œåœ¨å‘å‰æˆ–å‘åä¼ æ’­ä¹‹å‰å°†å‚æ•°ä»æ‰€æœ‰è€…GPUå¹¿æ’­ç»™å…¶ä»–GPUï¼Œåªè¦é€šä¿¡æ˜¯é‡å çš„ï¼Œå°±å¯ä»¥é«˜æ•ˆè¿è¡Œã€‚

On the contrary, a meager 12 GB/s PCIe bandwidth from a single GPU to CPU memory or NVMe (see Fig. 2b) or vice-versa is simply not sufficient to support heterogeneous training at scale. Therefore, existing heterogeneous solutions like ZeRO-Offload where the parameters must be first moved from CPU to owner GPU before broadcasting requires significantly large batch sizes per GPU to achieve enough ğ‘ğ‘–ğ‘¡ necessary to be efficient under the limited bandwidth.(è¦è¾¾åˆ°æ•ˆç‡è¦ä¹ˆå¢å¤§å¸¦å®½ï¼Œè¦ä¹ˆå¢å¤§aitï¼ŒGPUåˆ°GPUä¸éœ€è¦å¤§batch sizeï¼ŒPCIeéœ€è¦å¤§batch size)ï¼ˆå¤§batch sizeï¼‰è¿™å¸¦æ¥äº†ä¸¤ä¸ªé—®é¢˜:1)å¯¹äºå¤§è§„æ¨¡æ¨¡å‹ï¼Œæ¿€æ´»å†…å­˜ä¼šå˜å¾—å¤ªå¤§ï¼Œç”šè‡³æ— æ³•å®¹çº³CPUå†…å­˜;2)å½“æ‰©å±•åˆ°æ•°ç™¾æˆ–æ•°åƒä¸ªgpuä»¥å®ç°æœ‰æ•ˆæ”¶æ•›æ—¶ï¼Œæœ‰æ•ˆæ‰¹å¤„ç†å¤§å°ä¼šå˜å¾—å¤ªå¤§ã€‚

ZeRO-Infinityä»¥ä¸¤ç§æ–¹å¼è§£å†³äº†è¿™äº›æŒ‘æˆ˜:

ä»¥å¸¦å®½ä¸ºä¸­å¿ƒçš„åˆ†åŒºï¼Œå…è®¸ZeRO-Infinityå®ç°å‡ ä¹æ— é™çš„å¼‚æ„å†…å­˜å¸¦å®½ï¼ˆä¸è¦å¤§ batch sizeï¼Œå¢å¤§å¸¦å®½ï¼‰

ä»¥é‡å ä¸ºä¸­å¿ƒçš„è®¾è®¡ï¼Œoverlap not only GPU-GPU communication with computation but also NVMe-CPU and CPU-GPU communications over the PCIeï¼ˆé€šè¿‡é¢„å–éšè—é€šä¿¡ï¼‰

Unlike parameters and gradients that are consumed and produced sequentiallyï¼ˆä¸åŒgpu?ï¼‰ during the forward and backward propagation, optimizer states can be updated in parallel(æ‰€æœ‰gpu?), all at once.  

store and update the optimizer states in GPU and CPU memory, respectively, in parallel across all available GPUs and CPUs. As a result the aggregate GPU or CPU memory bandwidth can get much higher than the required 1.5TB/s with increase in GPU or CPU count.ï¼ˆæ¯ä¸ªGPUè´Ÿè´£ä¸€éƒ¨åˆ†æ•°æ®ä¼ è¾“ï¼Œæ¯ä¸ªcpuè´Ÿè´£ä¸€éƒ¨åˆ†è®¡ç®—ï¼Œå¹¶è¡Œä¼ è¾“ï¼Œå¹¶è¡Œè®¡ç®—ï¼‰

with NVMe offload, it is necessary to bring the data from NVMe to CPU memory and back in chunks that can fit in the CPU memory to perform the optimizer step, one chunk at a time. The optimizer step is therefore limited by the NVMe-CPU memory bandwidth

the process of bringing data in and out of NVMe to CPU memory, or from CPU memory to GPU memory can cause CPU memory fragmentation in both GPU and CPU that can result in out of memory even with plenty of memory still available.

æ— é™å¸è½½å¼•æ“ä¸ä»…å¯ä»¥å®ç°æ¥è¿‘å³°å€¼çš„NVMeå¸¦å®½ï¼Œå®ƒè¿˜å¯ä»¥å…è®¸ZeRO-Infinityå°†NVMeåˆ°CPUçš„è¯»å–ä¸CPUåˆ°NVMeçš„å†™å…¥é‡å ï¼Œä»¥åŠä¼˜åŒ–å™¨æ­¥éª¤çš„CPUè®¡ç®—ï¼ŒåŒæ—¶å…è®¸ZeRO-Infinityåœ¨å°‘é‡gpuä¸Šä¿æŒé€‚åº¦çš„æ‰¹é‡å¤§å°ï¼Œå¹¶åœ¨å¤§é‡gpuä¸Šä¿æŒå°æ‰¹é‡å¤§å°ï¼ˆæ•°æ®å¹¶è¡Œçš„æ€»batchå¤§å°ä¿æŒï¼‰ã€‚åŒæ—¶ï¼Œå®ƒé€šè¿‡å°å¿ƒåœ°ä¸ºæ•°æ®ç§»åŠ¨é‡ç”¨ä¸´æ—¶ç¼“å†²åŒºæ¥æœ€å°åŒ–å†…å­˜ç¢ç‰‡ã€‚

åœ¨DGX-2èŠ‚ç‚¹ä¸Šï¼Œæ¯ä¸ªGPUå¯ä»¥é€šè¿‡PCIeå¹¶è¡Œåœ°ä»¥å¤§çº¦3gb /sçš„é€Ÿåº¦è¯»å–å’Œå†™å…¥æ•°æ®åˆ°CPUå†…å­˜ï¼Œå…è®¸æ¿€æ´»æ£€æŸ¥ç‚¹å¸è½½åˆ°CPUå†…å­˜ï¼ŒåŒæ—¶åœ¨éšè—å¤§å°å¤§äº8ğ¾æˆ–æ›´å¤§çš„æƒ…å†µä¸‹ä¿æŒ80%ä»¥ä¸Šçš„æ•ˆç‡ã€‚ä¸ºäº†åœ¨è¾ƒå°çš„éšè—å°ºå¯¸ä¸‹å®ç°é«˜æ•ˆç‡ï¼ŒZeRO-Infinityå¯ä»¥å‡å°‘æ¿€æ´»æ£€æŸ¥ç‚¹çš„é¢‘ç‡ï¼Œå¹¶æœ‰æ•ˆåœ°å°†æ¿€æ´»æ£€æŸ¥ç‚¹ä¸CPUå†…å­˜ä¹‹é—´çš„é€šä¿¡ä¸GPUä¸Šçš„å‰å‘å’Œåå‘è®¡ç®—é‡å ã€‚

ZeRO-Infinityé€šè¿‡åœ¨PyTorchå­æ¨¡å—ä¸­æ³¨å…¥å‰å‘/åå‘é’©å­æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¿™äº›é’©å­ä¼šè§¦å‘allgatheré›†åˆæ¥æ”¶é›†å‰å‘/åå‘ä¼ é€’æ‰€éœ€çš„å‚æ•°ã€‚

ZeRO-Infinityå°†post forward/backward hookæ³¨å…¥å­æ¨¡å—ï¼Œå¯¹å‚æ•°è¿›è¡Œåˆ†åŒºï¼Œå¹¶å¯é€‰æ‹©å°†å®ƒä»¬å¸è½½ç»™CPUæˆ–NVMeã€‚

ä¸ZeRO[11]å’ŒZeRO- offload[12]ä¸åŒï¼Œå…¶ä¸­æ¯å±‚çš„å‚æ•°ç”±å•ä¸ªæ•°æ®å¹¶è¡Œè¿›ç¨‹æ‹¥æœ‰ï¼ˆä¸€å±‚çš„æ‰€æœ‰å‚æ•°ï¼‰ï¼Œå¹¶åœ¨éœ€è¦æ—¶å°†å…¶å¹¿æ’­ç»™å…¶ä»–æ•°æ®å¹¶è¡Œè¿›ç¨‹ï¼ŒZeRO- infinityåœ¨æ‰€æœ‰æ•°æ®å¹¶è¡Œè¿›ç¨‹ä¸­åˆ’åˆ†å•ä¸ªå‚æ•°ï¼ˆä¸€ä¸ªå‚æ•°å¹³åˆ†æˆå¤šä¸ªï¼‰ï¼Œå¹¶åœ¨éœ€è¦è®¿é—®å‚æ•°æ—¶ä½¿ç”¨allgatherè€Œä¸æ˜¯å¹¿æ’­ã€‚

åœ¨åŸºäºå¹¿æ’­çš„æ–¹æ³•ä¸­ï¼Œç”±äºæ¯ä¸ªå‚æ•°å®Œå…¨ç”±ä¸€ä¸ªæ•°æ®å¹¶è¡Œè¿›ç¨‹æ‹¥æœ‰ï¼Œå› æ­¤åœ¨å¹¿æ’­å‘ç”Ÿä¹‹å‰ï¼Œå‚æ•°å¿…é¡»é¦–å…ˆä»å…¶æºä½ç½®(CPUæˆ–NVMe)é€šè¿‡PCIeé€šä¿¡åˆ°GPUå†…å­˜ã€‚è¯·æ³¨æ„ï¼Œåªæœ‰ä¸€ä¸ªPCIeå¯ä»¥æ¿€æ´»æ­¤è¿›ç¨‹ï¼Œè€Œè¿æ¥åˆ°æ‰€æœ‰å…¶ä»–gpuçš„æ‰€æœ‰PCIeé“¾è·¯éƒ½æ˜¯ç©ºé—²çš„ã€‚with the partitioned parameter and allgather based
approach in ZeRO-Infinity, all PCIe links are active in parallel, each bringing in 1/ğ‘‘ğ‘ğ‘¡â„ portion of the parameterï¼ˆä¸€ä¸ªå‚æ•°ï¼‰ where ğ‘‘ğ‘ is the data parallel degree. As a result, the effective communication bandwidth between NVMe or CPU to the GPU, increases linearly with the ğ‘‘ğ‘ degree.ï¼ˆæ¯äººä»cpuå–ä¸€éƒ¨åˆ†ï¼Œå†ä»gpu all-gather)

å¸¦å®½éšç€èŠ‚ç‚¹çš„å¢åŠ è€Œçº¿æ€§å¢é•¿ã€‚å› æ­¤ï¼Œåœ¨å¤§è§„æ¨¡è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹æ—¶ï¼ŒZeRO-Infinityå¯ä»¥æä¾›æ¯”å¿…è¦(å®é™…ä¸Šæ˜¯æ— é™çš„)æ›´å¤šçš„å¼‚æ„å†…å­˜å¸¦å®½ï¼Œä»¥ä¿æŒè®­ç»ƒçš„æ•ˆç‡ã€‚

è™½ç„¶ZeRO-Infinityå¯ä»¥åœ¨å¤šèŠ‚ç‚¹è®¾ç½®ä¸Šåˆ©ç”¨è¶³å¤Ÿçš„å¼‚æ„å†…å­˜å¸¦å®½ï¼Œä½†å¸¦å®½ä»ç„¶å¯èƒ½æˆä¸ºå•ä¸ªGPUæˆ–å•ä¸ªèŠ‚ç‚¹è®¾ç½®çš„ç“¶é¢ˆã€‚å³ä½¿GPU-GPU allgatheré€šä¿¡åœ¨å°æ‰¹é‡è¿è¡Œæ—¶ä¹Ÿä¼šå¯¹æ•ˆç‡äº§ç”Ÿå¾ˆå¤§å½±å“(å›¾3)ã€‚(å¸¦å®½å¤§è¿˜ä¸å¤Ÿæ•ˆç‡ï¼Œè¦åŠ ä¸Šé‡å )

è®¿é—®NVMeå†…å­˜éœ€è¦ä¸‰æ­¥è¿‡ç¨‹:i)å°†æ•°æ®ä»NVMeè¯»å–åˆ°CPUå†…å­˜(nc-transfer)ï¼Œ ii)å°†æ•°æ®ä»CPUå†…å­˜å¤åˆ¶åˆ°GPUå†…å­˜(cg-transfer)ï¼Œ iii)æ‰§è¡Œallgatherä»¥åœ¨æ‰€æœ‰GPUä¸Šæ„é€ å®Œæ•´å‚æ•°(gg-transfer)ã€‚

ZeRO-Infinityæœ‰ä¸€ä¸ªé‡å å¼•æ“ï¼Œå®ƒä¸ä»…å°†GPU-GPUé€šä¿¡ä¸GPUè®¡ç®—é‡å ï¼Œè¿˜å°†NVMeä¸CPUã€CPUä¸GPUé€šä¿¡é‡å ï¼Œæ‰€æœ‰è¿™äº›éƒ½åœ¨åŒä¸€æ—¶é—´è¿›è¡Œã€‚é‡å å¼•æ“æœ‰ä¸¤ä¸ªç»„æˆéƒ¨åˆ†: i) A dynamic prefetcher for overlapping the data movement required to reconstruct parameters before they are consumed
in the forward or backward pass, and ii) a communication and
offload overlapping mechanism for executing the data movement required by gradients in parallel with the backward computation.

é¢„å–å™¨çŸ¥é“ä¸‰æ­¥é€šä¿¡è¿‡ç¨‹ï¼Œå› æ­¤å¯ä»¥å°†ä¸€ä¸ªå‚æ•°çš„nc-transferä¸å…¶ä»–å‚æ•°çš„cgtransferå’Œgg-transferé‡å ã€‚ä¾‹å¦‚ï¼Œé¢„å–å™¨åœ¨æ‰§è¡Œopero -ğ‘¡- operatorä¹‹å‰ï¼Œå¯ä»¥åˆ†åˆ«å¯¹opero i+ 3ã€opero i+ 2å’Œopero i+ 1æ‰€éœ€è¦çš„å‚æ•°è°ƒç”¨ncã€cgå’Œgg-transferã€‚æ³¨æ„ï¼Œæ‰€æœ‰è¿™äº›æ•°æ®ç§»åŠ¨éƒ½å¯ä»¥ä¸æ‰§è¡Œğ‘¡è¿ç®—ç¬¦å¹¶è¡Œè¿›è¡Œã€‚(è°ƒç”¨å®Œåä¸‰æ­¥æ•°æ®å†è®¡ç®—)

Similarly, in the backward pass, ZeRO-Infinity can overlap the
reduce-scatter for gradients of the parameters in (ğ‘– + 1)
ğ‘¡â„ operator with the computation of the ğ‘–ğ‘¡â„ operator, while simultaneous transferring the partitioned gradientsï¼ˆæ¢¯åº¦åŒæ ·å¹³åˆ†ï¼Ÿï¼‰ from the reduce-scatter of the gradients of the (ğ‘– + 2)ğ‘¡â„ operator to the CPU or NVMe.

DeepNVMeï¼Œä¸€ä¸ªå¼ºå¤§çš„c++ NVMeè¯»/å†™åº“ï¼Œåœ¨æ— é™å¸è½½å¼•æ“ä¸­ï¼Œæ”¯æŒå¼‚æ­¥å®Œæˆçš„æ‰¹é‡è¯»/å†™è¯·æ±‚ï¼Œä»¥åŠæ˜¾å¼åŒæ­¥è¯·æ±‚ï¼Œä»¥åˆ·æ–°æ­£åœ¨è¿›è¡Œçš„è¯»/å†™ã€‚å¯¹å¼‚æ­¥çš„æ”¯æŒå…è®¸ZeROInfinityå°†è¿™äº›è¯·æ±‚ä¸GPU/GPUæˆ–GPU/CPUé€šä¿¡æˆ–è®¡ç®—é‡å ã€‚

DeepNVMeèƒ½å¤Ÿåœ¨NVMeå­˜å‚¨è®¾å¤‡ä¸Šå®ç°æ¥è¿‘å³°å€¼çš„é¡ºåºè¯»å†™å¸¦å®½ã€‚å®ƒé€šè¿‡è®¸å¤šä¼˜åŒ–å®ç°äº†è¿™ç§é«˜æ€§èƒ½ï¼ŒåŒ…æ‹¬I/Oè¯·æ±‚çš„ç§¯æå¹¶è¡ŒåŒ–(æ— è®ºæ˜¯æ¥è‡ªå•ä¸ªç”¨æˆ·çº¿ç¨‹è¿˜æ˜¯è·¨å¤šä¸ªç”¨æˆ·çº¿ç¨‹)ã€æ™ºèƒ½å·¥ä½œè°ƒåº¦ã€é¿å…æ•°æ®å¤åˆ¶å’Œå†…å­˜å›ºå®šã€‚

ä¸ºäº†ç¡®ä¿é«˜æ€§èƒ½å¼ é‡ä»NVMe/CPUå­˜å‚¨è¯»å–(æˆ–å†™å…¥)ï¼Œæº(æˆ–ç›®æ ‡)å¼ é‡å¿…é¡»é©»ç•™åœ¨å›ºå®šçš„å†…å­˜ç¼“å†²åŒºä¸­ã€‚This layer manages the limited supply of pinned memory by reusing a small
amount (tens of GBs) for offloading the entire model states (up to tens of TBs) to CPU or NVMe. ï¼ˆï¼Ÿï¼‰å†…å­˜ç¼“å†²åŒºçš„é‡ç”¨å¯ä»¥é˜²æ­¢CPUå’ŒGPUå†…å­˜ä¸­çš„å†…å­˜ç¢ç‰‡ã€‚è¿™ä¸€å±‚è¿˜ä¸ºPyTorchå¼ é‡æä¾›äº†å›ºå®šçš„å†…å­˜æ•°æ®ï¼Œå…è®¸å¯¹å¼ é‡è¿›è¡Œå°±åœ°è®¡ç®—ï¼Œè¿™æ ·å®ƒä»¬å°±å¯ä»¥è¢«å†™å…¥NVMeï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥å¤åˆ¶ä»¥æé«˜å¸¦å®½ã€‚ï¼ˆï¼Ÿï¼‰

# .

åœ¨è¿‡å»çš„ä¸‰å¹´é‡Œï¼Œæœ€å¤§çš„å¯†é›†æ·±åº¦å­¦ä¹ æ¨¡å‹å¢é•¿äº†1000å€ä»¥ä¸Šï¼Œè¾¾åˆ°äº†æ•°åƒäº¿ä¸ªå‚æ•°ï¼Œè€ŒGPUå†…å­˜åªå¢é•¿äº†5å€(16gbåˆ°80gb)ã€‚

(æ¨¡å‹å¢é•¿å¾ˆå¿«ï¼Œgpuå†…å­˜å¢é•¿å¾ˆæ…¢)

åˆ›æ–°å…è®¸å¤§å‹æ¨¡å‹é€‚åˆå¤šä¸ªGPUçš„èšåˆGPUå†…å­˜ã€‚ï¼ˆå¤§å¤šæ•°æŠ€æœ¯éƒ½æ˜¯ä½¿ç”¨å¤šä¸ªgpuçš„èšåˆgpuå†…å­˜ï¼Œtensorå¹¶è¡Œå’Œpipelineå¹¶è¡Œï¼‰

ç›®å‰æœ€å…ˆè¿›çš„å¤§å‹æ¨¡å‹è®­ç»ƒæŠ€æœ¯æ˜¯ä¸‰ç»´å¹¶è¡Œ(3D parallelism[13,14])ï¼Œå®ƒå°†æ¨¡å‹(å¼ é‡åˆ‡ç‰‡)å’Œç®¡é“å¹¶è¡Œä¸æ•°æ®å¹¶è¡Œç›¸ç»“åˆï¼Œå¯ä»¥åœ¨æ•°ç™¾æˆ–æ•°åƒä¸ªgpuä¸Šæœ‰æ•ˆåœ°å°†æ·±åº¦å­¦ä¹ è®­ç»ƒæ‰©å±•åˆ°æ•°ä¸‡äº¿ä¸ªå‚æ•°ã€‚é€šè¿‡å……åˆ†åˆ©ç”¨é›†ç¾¤çš„GPUæ€»å†…å­˜ï¼ŒDeepSpeed 3Då¹¶è¡Œå®ç°å¯ä»¥åœ¨800ä¸ªNVIDIA V100 GPUä¸Šæ‰©å±•åˆ°è¶…è¿‡1ä¸‡äº¿ä¸ªå‚æ•°[15]ã€‚

æˆ‘ä»¬æ­£åœ¨æ¥è¿‘GPUå†…å­˜å¢™ã€‚å®ƒéœ€è¦800ä¸ªNVIDIA V100 gpuæ¥æ‹Ÿåˆç”¨äºè®­ç»ƒçš„ä¸‡äº¿å‚æ•°æ¨¡å‹ï¼Œè€Œè¿™æ ·çš„é›†ç¾¤å¯¹äºå¤§å¤šæ•°æ•°æ®ç§‘å­¦å®¶æ¥è¯´ç®€ç›´æ˜¯é¥ä¸å¯åŠã€‚ï¼ˆåªä½¿ç”¨gpuå­˜å‚¨ï¼Œéœ€è¦å¤§é‡çš„gpu)

ä»¥GPUå†…å­˜ä¸ºç“¶é¢ˆï¼Œæˆ‘ä»¬æ— æ³•å†ç»´æŒæ¨¡å‹è§„æ¨¡çš„æŒç»­å¢é•¿ã€‚

åœ¨è¿™ç§è§„æ¨¡ä¸Šè®­ç»ƒæ¨¡å‹éœ€è¦å¤æ‚çš„å¹¶è¡ŒæŠ€æœ¯ç»„åˆï¼Œè¿™ç»™æ•°æ®ç§‘å­¦å®¶é‡æ„æ¨¡å‹å¸¦æ¥äº†å¾ˆå¤§çš„è´Ÿæ‹…ã€‚(å¹¶è¡ŒæŠ€æœ¯éœ€è¦é‡æ„æ¨¡å‹ï¼Œæœ‰éš¾åº¦)

ç”¨å¼ é‡åˆ‡ç‰‡ç‰ˆæœ¬æ›¿æ¢å•ä¸ªGPUæ“ä½œç¬¦ï¼Œå¹¶å°†æ¨¡å‹åˆ’åˆ†ä¸ºè´Ÿè½½å¹³è¡¡çš„ç®¡é“é˜¶æ®µã€‚

ZeRO-Infinity åˆ©ç”¨GPUã€CPUå’ŒNVMeå†…å­˜ï¼Œåœ¨æœ‰é™çš„èµ„æºä¸Šå®ç°å‰æ‰€æœªæœ‰çš„æ¨¡å‹æ‰©å±•ï¼Œè€Œä¸éœ€è¦æ¨¡å‹ä»£ç é‡æ„ã€‚

å®ƒå®ç°äº†å‡ºè‰²çš„è®­ç»ƒååé‡å’Œå¯æ‰©å±•æ€§ï¼Œä¸å—æœ‰é™çš„CPUæˆ–NVMeå¸¦å®½çš„é˜»ç¢ã€‚

å¯ä»¥æ‹Ÿåˆå…·æœ‰æ•°åç”šè‡³æ•°ç™¾ä¸‡äº¿å‚æ•°çš„æ¨¡å‹ï¼Œç”¨äºåœ¨å½“å‰ä¸€ä»£GPUé›†ç¾¤ä¸Šè¿›è¡Œè®­ç»ƒã€‚

å¯ç”¨äºåœ¨å•ä¸ªNVIDIA DGX-2èŠ‚ç‚¹ä¸Šå¾®è°ƒä¸‡äº¿å‚æ•°æ¨¡å‹

åœ¨512ä¸ªNVIDIA V100 gpu(å³°å€¼çš„40%)ä¸Šç»´æŒè¶…è¿‡25åƒä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—ï¼ŒåŒæ—¶ä¹Ÿå±•ç¤ºäº†è¶…çº§çº¿æ€§å¯æ‰©å±•æ€§ã€‚

ZeRO-Infinityæ˜¯æ•°æ®å¹¶è¡Œè®­ç»ƒçš„ä¸€ç§å½¢å¼ï¼ŒZeRO-Infinityå°†æ¨¡å‹çŠ¶æ€ï¼ˆå‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼‰åˆ’åˆ†åˆ°æ‰€æœ‰æ•°æ®å¹¶è¡Œè¿›ç¨‹ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒZeRO-Infinityä½¿ç”¨é€šä¿¡é›†åˆæ¥æ”¶é›†å½“å‰éœ€è¦çš„æ¨¡å‹çŠ¶æ€ã€‚æ¨¡å‹çŠ¶æ€åœ¨æœªä½¿ç”¨æ—¶å¸è½½åˆ°cpuå†…å­˜æˆ–NVMeå†…å­˜ï¼Œéœ€è¦æ—¶å†åŠ è½½ã€‚

Unprecedented Model Scale:

infinity offload engine:åŒæ—¶åˆ©ç”¨CPUå’ŒNVMeå†…å­˜ï¼Œåœ¨æœ‰é™çš„GPUèµ„æºä¸Šæ”¯æŒå¤§è§„æ¨¡æ¨¡å‹å¤§å°ã€‚

å¼•å…¥äº†ä¸€ç§æ–°çš„GPUå†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œç§°ä¸ºä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„å¹³é“º(memory-centric tiling)ï¼Œä»¥æ”¯æŒè¶…å¤§çš„å•ä¸ªå±‚ï¼Œå¦åˆ™GPUå†…å­˜ç”šè‡³æ— æ³•ä¸€æ¬¡å®¹çº³ä¸€å±‚ã€‚

Excellent Training Efficiency:

å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°æ®åˆ†åŒºç­–ç•¥ï¼Œç”¨äºåˆ©ç”¨æ‰€æœ‰è®¾å¤‡çš„èšåˆå†…å­˜å¸¦å®½ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºä»¥å¸¦å®½ä¸ºä¸­å¿ƒçš„åˆ†åŒºï¼Œå¹¶å°†å…¶ä¸å¼ºå¤§çš„é€šä¿¡é‡å ä¸ºä¸­å¿ƒçš„è®¾è®¡ç›¸ç»“åˆï¼Œä»¥åŠåœ¨æ— é™å¸è½½å¼•æ“ä¸­å¯¹é«˜æ€§èƒ½NVMeè®¿é—®çš„ä¼˜åŒ–ã€‚

å°½ç®¡å¸è½½æ•°æ®åˆ°CPUæˆ–NVMeï¼Œä¸å—å…¶æœ‰é™çš„å¸¦å®½çš„é˜»ç¢ã€‚

Ease of Use:

ä¸å†éœ€è¦è°ƒæ•´ä»–ä»¬çš„æ¨¡å‹æ¥é€‚åº”å¤šç§å½¢å¼çš„å¹¶è¡Œï¼Œä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„å¹³é“ºæ—¨åœ¨å‡å°‘å¤§å‹å•ä¸ªå±‚çš„GPUå†…å­˜éœ€æ±‚ï¼Œå¦åˆ™å°†éœ€è¦æ¨¡å‹å¹¶è¡Œæ€§(å¼ é‡åˆ‡ç‰‡)æ¥é€‚åº”GPUå†…å­˜ä¸­çš„å±‚ã€‚è‡ªåŠ¨å®Œæˆè®­ç»ƒä»»æ„æ¨¡å‹æ¶æ„æ‰€éœ€çš„æ‰€æœ‰é€šä¿¡å’Œæ•°æ®åˆ†åŒºã€‚

ç”±äº”ç§åˆ›æ–°æŠ€æœ¯ç»„æˆï¼Œä»¥æ»¡è¶³å†…å­˜å’Œå¸¦å®½è¦æ±‚ï¼Œæä¾›å‰æ‰€æœªæœ‰çš„æ¨¡å‹è§„æ¨¡ï¼Œæ˜“äºè®¿é—®å’Œä½¿ç”¨ï¼ŒåŒæ—¶å®ç°å“è¶Šçš„è®­ç»ƒæ•ˆç‡:i)æ— é™å¸è½½å¼•æ“ï¼Œé€šè¿‡åŒæ—¶åˆ©ç”¨GPUã€CPUå’ŒNVMeå†…å­˜ä»¥åŠGPUå’ŒCPUè®¡ç®—ï¼Œå……åˆ†åˆ©ç”¨ç°ä»£é›†ç¾¤ä¸Šçš„å¼‚æ„æ¶æ„;ii)ä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„å¹³é“ºï¼Œåœ¨ä¸éœ€è¦æ¨¡å‹å¹¶è¡Œçš„æƒ…å†µä¸‹å¤„ç†å¤§é‡æ“ä½œ;iii)ä»¥å¸¦å®½ä¸ºä¸­å¿ƒçš„åˆ†åŒºï¼Œåœ¨æ‰€æœ‰å¹¶è¡Œè®¾å¤‡ä¸Šåˆ©ç”¨èšåˆå†…å­˜å¸¦å®½;iv)ä»¥é‡å ä¸ºä¸­å¿ƒçš„è®¾è®¡ï¼Œç”¨äºé‡å è®¡ç®—å’Œé€šä¿¡ã€‚V)æ˜“äºå®ç°ï¼Œé¿å…æ¨¡å‹ä»£ç é‡æ„ã€‚

i)åœ¨32ä¸ªNVIDIA DGX-2èŠ‚ç‚¹(512ä¸ªV100 gpu)ä¸Šè¿è¡Œ32ä¸‡äº¿ä¸ªå‚æ•°çš„ç©ºå‰è§„æ¨¡ï¼Œii)åœ¨ç›¸åŒç¡¬ä»¶ä¸Šå®ç°è¶…è¿‡25åƒä¸‡äº¿æ¬¡æµ®ç‚¹ååé‡çš„å“è¶Šè®­ç»ƒæ•ˆç‡ï¼Œiii)ä¸‡äº¿å‚æ•°æ¨¡å‹çš„è¶…çº¿æ€§å¯æ‰©å±•æ€§

MEMORY REQUIREMENTSï¼š

å‡è®¾ä½¿ç”¨Adamä¼˜åŒ–å™¨è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒ

è®­ç»ƒæ‰€éœ€çš„è®°å¿†å¯ä»¥åˆ†ä¸ºä¸¤éƒ¨åˆ†:i)æ¨¡å‹çŠ¶æ€ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œæ¨¡å‹å‚æ•°;ii)æ®‹å·®çŠ¶æ€ï¼Œä¸»è¦æŒ‡æ¿€æ´»è®°å¿†ã€‚

æè¿°äº†GPUä¸Šå¿…é¡»å¯ç”¨çš„æœ€å°å†…å­˜é‡æ¥æ”¯æŒè®­ç»ƒï¼Œå‡è®¾æ¨¡å‹å’Œå‰©ä½™çŠ¶æ€å¯ä»¥æˆåŠŸåœ°ä»GPUå†…å­˜ä¸­å¸è½½ã€‚

ä½¿ç”¨Adamä¼˜åŒ–å™¨è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒæ—¶ï¼Œå‚æ•°å’Œæ¢¯åº¦å­˜å‚¨åœ¨FP16ä¸­ï¼Œä¼˜åŒ–å™¨çŠ¶æ€ç”±FP32åŠ¨é‡ã€æ–¹å·®ã€å‚æ•°å’Œæ¢¯åº¦ç»„æˆã€‚æ¯ä¸ªå‚æ•°æ€»å…±éœ€è¦20å­—èŠ‚çš„å†…å­˜ã€‚

ï¼ˆå‡è®¾ä½¿ç”¨Adamä¼˜åŒ–å™¨è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒï¼š2ä¸ªå­—èŠ‚çš„å‚æ•°ã€æ¢¯åº¦ï¼Œ4ä¸ªå­—èŠ‚çš„å‚æ•°ã€æ¢¯åº¦ã€åŠ¨é‡ã€æ–¹å·®ï¼‰

åŸºäºTransformerçš„æ¨¡å‹ä¸­å‚æ•°çš„æ€»æ•°ä¸»è¦å–å†³äºéšè—ç»´åº¦(hğ‘‘)å’ŒTransformerå±‚çš„æ•°é‡(ğ‘›ğ‘™)ã€‚

éœ€è¦æ€»å†…å­˜240 Ã—ğ‘›ğ‘™Ã— $hğ‘‘^2$å­—èŠ‚æ¥å­˜å‚¨æ¨¡å‹çŠ¶æ€ã€‚

å‰©ä½™çŠ¶æ€ä¸»è¦ç”±æ¿€æ´»å†…å­˜ç»„æˆï¼Œå®ƒå–å†³äºæ¨¡å‹ä½“ç³»ç»“æ„ã€æ‰¹å¤„ç†å¤§å°(ğ‘ğ‘ )å’Œåºåˆ—é•¿åº¦(ğ‘ ğ‘’ğ‘)ï¼Œå¹¶ä¸”å¯èƒ½ç›¸å½“å¤§ã€‚ä»ç§¯æçš„æ–¹é¢æ¥çœ‹ï¼Œæ¿€æ´»æ‰€éœ€çš„å†…å­˜å¯ä»¥é€šè¿‡æ¿€æ´»æ£€æŸ¥ç‚¹æ˜¾è‘—å‡å°‘[29]

å­˜å‚¨æ¿€æ´»æ£€æŸ¥ç‚¹æ‰€éœ€çš„å†…å­˜ä¼°è®¡ä¸º

ğ‘æ˜¯ä¸¤ä¸ªæ¿€æ´»æ£€æŸ¥ç‚¹ä¹‹é—´çš„Transformerå—çš„æ•°ç›®

è™½ç„¶ç”Ÿæˆçš„æ¿€æ´»æ£€æŸ¥ç‚¹æ¯”å®Œæ•´çš„æ¿€æ´»é›†åˆ(ç¬¬6åˆ—)å°å‡ ä¸ªæ•°é‡çº§ï¼Œä½†è¶…è¿‡ä¸€ä¸‡äº¿å‚æ•°åï¼Œå®ƒä»¬ä»ç„¶å¤ªå¤§ï¼Œæ— æ³•é€‚åº”GPUå†…å­˜ä¸­çš„æ‰¹å¤„ç†å¤§å°å’Œæ‰€è€ƒè™‘çš„åºåˆ—é•¿åº¦ã€‚

æ¨¡å‹çŠ¶æ€å·¥ä½œå†…å­˜(MSWM)æ˜¯åœ¨æ‰€æœ‰æ¨¡å‹çŠ¶æ€è¢«å¸è½½åˆ°CPUæˆ–NVMeä¹‹åï¼Œåœ¨æ¨¡å‹ä¸­æœ€å¤§çš„å•ä¸ªè¿ç®—ç¬¦ä¸Šæ‰§è¡Œæ­£å‘æˆ–å‘åä¼ æ’­æ‰€éœ€çš„æœ€å°GPUå†…å­˜é‡ã€‚è¿™å¤§çº¦æ˜¯ç”±æ¨¡å‹ä¸­è¯¥æ“ä½œç¬¦çš„å‚æ•°å’Œæ¢¯åº¦çš„å¤§å°ç»™å‡ºçš„ï¼Œå› ä¸ºå¿…é¡»è‡³å°‘æœ‰è¶³å¤Ÿçš„å†…å­˜æ¥ä¿å­˜å‚æ•°åŠå…¶æ¢¯åº¦ä»¥è¿›è¡Œåå‘ä¼ æ’­ã€‚

MSWM:åœ¨æ¨¡å‹ä¸­æœ€å¤§çš„å•ä¸ªè¿ç®—ç¬¦ä¸Šæ‰§è¡Œæ­£å‘æˆ–å‘åä¼ æ’­æ‰€éœ€çš„æœ€å°GPUå†…å­˜é‡(å‚æ•°å’Œæ¢¯åº¦)

å¯¹äºåŸºäºTransformerçš„æ¨¡å‹ï¼Œæœ€å¤§çš„ç®—å­æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå®ƒå°†éšè—çŠ¶æ€ä»ğ‘‘è½¬æ¢ä¸º4 ğ‘‘ã€‚è¯¥çº¿æ€§å±‚çš„å‚æ•°å’Œæ¢¯åº¦å¤§å°ä¸º4 Ã— hğ‘‘Ã— 4 hğ‘‘å­—èŠ‚ã€‚

MSWM(å›¾2aåˆ—8)åœ¨è¶…è¿‡1000äº¿ä¸ªå‚æ•°çš„æƒ…å†µä¸‹æ˜¾è‘—å¢é•¿ï¼Œéœ€è¦å¤šä¸ªgbçš„è¿ç»­å†…å­˜ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨è®­ç»ƒæœŸé—´å†…å­˜è€—å°½ï¼Œå› ä¸ºç¼ºä¹è¶³å¤Ÿçš„è¿ç»­å†…å­˜æ¥æ»¡è¶³è¿™äº›å‚æ•°

3D Parallelismç­‰æœ€å…ˆè¿›çš„æ–¹æ³•é€šè¿‡æ¨¡å‹å¹¶è¡Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œå°†å•ä¸ªè¿ç®—ç¬¦æ‹†åˆ†åˆ°å¤šä¸ªgpuä¸Šã€‚

æ¿€æ´»å·¥ä½œå†…å­˜(Activation Working Memory, AWM)æ˜¯åœ¨æ‰§è¡Œå®é™…çš„å‘åä¼ æ’­ä¹‹å‰é‡æ–°è®¡ç®—æ¿€æ´»æ‰€éœ€çš„å†…å­˜ã€‚è¿™æ˜¯ä¸¤ä¸ªè¿ç»­æ¿€æ´»æ£€æŸ¥ç‚¹ä¹‹é—´çš„æ¿€æ´»å¤§å°ã€‚å¦‚æœæˆ‘ä»¬ä¸ºæ¯ä¸ªTransformerå—åˆ›å»ºä¸€ä¸ªæ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œåˆ™å†…å­˜ç”±æ¯ä¸ªTransformerå—çš„æ€»æ¿€æ´»å¤§å°æä¾›ã€‚

ğ‘ğ‘ ğ‘§Ã—ğ‘ ğ‘’ğ‘Ã—ğ‘ğ‘–Ã—(16Ã—â„ğ‘‘+ 2Ã—ğ‘ğ‘¡ğ‘¡ğ‘›_â„ğ‘’ğ‘ğ‘‘ğ‘ Ã—ğ‘ ğ‘’ğ‘)

Unlike MSWM that is only composed of a single parameter(largest single operatorçš„å‚æ•°) and gradient

BANDWIDTH REQUIREMENTS:

æœ¬èŠ‚æè¿°äº†å¸¦å®½å¯¹è®­ç»ƒæ•ˆç‡çš„å½±å“ã€‚

æ¯æ¬¡è¿­ä»£çš„æ€»è®¡ç®—é‡ç”±Transformer çš„çº¿æ€§å±‚ä¸­çš„è®¡ç®—é‡ä¸»å¯¼ã€‚

åœ¨å‘å‰å’Œå‘åä¼ æ’­æœŸé—´ï¼Œæ¨¡å‹å‚æ•°å¿…é¡»ä»æºä½ç½®åŠ è½½åˆ°GPUå¯„å­˜å™¨è‡³å°‘ä¸¤æ¬¡ï¼Œåœ¨æ¿€æ´»æ£€æŸ¥ç‚¹å­˜åœ¨çš„æƒ…å†µä¸‹ï¼Œå‚æ•°å¯èƒ½ä¼šè¢«åŠ è½½ä¸€æ¬¡ï¼Œæ¢¯åº¦å¿…é¡»ä»GPUå¯„å­˜å™¨å­˜å‚¨åˆ°å…¶æœ€ç»ˆä½ç½®è‡³å°‘ä¸€æ¬¡

ï¼ˆç›¸å¯¹GPUä¸€å…±è¦ç§»åŠ¨3æ¬¡å‚æ•°å’Œ1æ¬¡æ¢¯åº¦ï¼Œå‰å‘ã€åå‘ã€é‡è®¡ç®—ã€å¸è½½æ¢¯åº¦ï¼‰

the total data movement during the forward and backward pass would be 4Ã—ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ , i.e. 2Ã—4Ã—ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘  in bytes.

ğ‘ğ‘–ğ‘¡ w.r.t parameter and gradients is ğ‘ ğ‘’ğ‘ Ã— ğ‘ğ‘ ğ‘§.

åœ¨ä¼˜åŒ–å™¨æ­¥éª¤æœŸé—´ï¼Œå¿…é¡»è‡³å°‘è¯»å–ä¸€æ¬¡ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå¹¶ä¸”å¿…é¡»è‡³å°‘å†™å…¥ä¸€æ¬¡ä¼˜åŒ–å™¨çŠ¶æ€ã€‚ï¼ˆè°è®¡ç®—ï¼Ÿåº”è¯¥æ˜¯gpuï¼‰

ğ‘ğ‘–ğ‘¡ w.r.t optimizer states during a full training iteration isğ‘ ğ‘’ğ‘Ã—ğ‘ğ‘ ğ‘§/4.

æ¿€æ´»æ£€æŸ¥ç‚¹åœ¨å‘å‰ä¼ æ’­æœŸé—´ï¼Œå¿…é¡»å°†æ¿€æ´»æ£€æŸ¥ç‚¹ä¿å­˜åˆ°å®ƒä»¬çš„æœ€ç»ˆä½ç½®ï¼Œå¹¶ä¸”å¿…é¡»åœ¨å‘åä¼ æ’­æœŸé—´è¿›è¡Œæ£€ç´¢ã€‚å› æ­¤,æ•°æ®ç§»åŠ¨w.r.tæ¿€æ´»æ£€æŸ¥ç«™åœ¨å­—èŠ‚æ€»æ•°æ˜¯2Ã—ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘â„ğ‘’ğ‘ğ‘˜ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘ _ğ‘–ğ‘›_ğ‘ğ‘¦ğ‘¡ğ‘’ğ‘ ç”±4Ã—æä¾›ğ‘›ğ‘™/ğ‘ğ‘–Ã—â„ğ‘‘Ã—ğ‘ ğ‘’ğ‘Ã—ğ‘ğ‘ ğ‘§Eqã€‚(1)æ€»è®¡ç®—æ¯ä¸ªè¿­ä»£ç”±4.1ç§’ã€‚ã€‚å› æ­¤ï¼Œğ‘ï¿£ğ‘¡w.r.tæ¿€æ´»æ£€æŸ¥ç‚¹ç”±24 Ã— ğ‘‘Ã—ğ‘ï¿£ç»™å‡ºã€‚

(å‰å‘æ—¶ä¿æŒæ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œåå‘æ—¶åŠ è½½)

ç”±äºAITçš„å˜åŒ–ï¼Œæ¨¡å‹çŠ¶æ€å’Œæ¿€æ´»æ£€æŸ¥ç‚¹å…·æœ‰éå¸¸ä¸åŒçš„å¸¦å®½éœ€æ±‚ï¼Œä»¥å®ç°è‰¯å¥½çš„æ•ˆç‡ã€‚å‰è€…ä»…å–å†³äºæ‰¹å¤§å°å’Œåºåˆ—é•¿åº¦ï¼Œåè€…ä»…å–å†³äºæ¿€æ´»æ£€æŸ¥ç‚¹çš„é¢‘ç‡å’Œæ¨¡å‹çš„éšè—ç»´åº¦å¤§å°ã€‚

æ•ˆç‡åœ¨ä¸åŒçš„æ¨¡å‹å’Œæ®‹å·®çŠ¶æ€ä¸‹æ˜¯å¦‚ä½•éšå¸¦å®½w.r.tå˜åŒ–çš„ï¼Œç„¶åè®¨è®ºäº†æ·±åº¦å­¦ä¹ è®­ç»ƒæ•ˆç‡å¯¹è¿™äº›çŠ¶æ€çš„å¸¦å®½éœ€æ±‚ã€‚

åœ¨å¤§é‡GPUä¸Šè¿è¡Œæ—¶ï¼Œæ¯ä¸ªGPUä½¿ç”¨è¾ƒå°çš„æ‰¹å¤„ç†å¤§å°ï¼Œè€Œåœ¨ç›¸å¯¹è¾ƒå°‘çš„GPUä¸Šè®­ç»ƒæ—¶ï¼Œæ¯ä¸ªGPUä½¿ç”¨è¾ƒå¤§çš„æ‰¹å¤„ç†å¤§å°ï¼Œä»¥ä¿æŒåˆç†æœ‰æ•ˆçš„è®­ç»ƒæ‰¹å¤§å°ã€‚

å›¾3aæ˜¾ç¤ºï¼Œå½“å‚æ•°å’Œæ¢¯åº¦çš„å¸¦å®½è¶…è¿‡70 GB/sæ—¶ï¼Œå³ä½¿æ˜¯æœ€å°çš„æ‰¹å¤„ç†å¤§å°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å®ç°è¶…è¿‡50%çš„æ•ˆç‡ã€‚åœ¨è¿™ä¸ªå¸¦å®½ä¸‹ï¼Œç†è®ºä¸Šæ•°æ®ç§»åŠ¨å¯ä»¥ä¸è®¡ç®—å®Œå…¨é‡åˆï¼Œè¾¾åˆ°100%çš„æ•ˆç‡ã€‚

ä¸å‚æ•°å’Œæ¢¯åº¦ç›¸æ¯”ï¼Œä¼˜åŒ–å™¨çŠ¶æ€éœ€è¦è¿‘4å€çš„å¸¦å®½æ‰èƒ½è¾¾åˆ°50%çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œä¼˜åŒ–å™¨çŠ¶æ€åœ¨å‘å‰å’Œå‘åä¼ æ’­ç»“æŸæ—¶æ›´æ–°ï¼Œå¹¶ä¸”ä¸èƒ½ä¸è®¡ç®—é‡å ã€‚å› æ­¤ï¼Œå®ƒä»¬éœ€è¦æ›´å¤§çš„å¸¦å®½æ¥ä¿æŒæ•´ä¸ªDLå·¥ä½œè´Ÿè½½çš„æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œæ¯ä¸ªGPUçš„æ‰¹å¤„ç†å¤§å°ä¸º2ï¼Œè¦è¾¾åˆ°90%çš„æ•ˆç‡ï¼Œéœ€è¦è¿‘1.5 TB/sçš„æœ‰æ•ˆå¸¦å®½ï¼Œè¿™ç”šè‡³å¤§äºGPUçš„å†…å­˜å¸¦å®½ã€‚

å¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹åï¼Œå³ä½¿éšè—å¤§å°ä¸º2ğ¾ï¼Œ2 GB/sçš„å¾®è–„å¸¦å®½ä¹Ÿèƒ½å¤Ÿç»´æŒ50%ä»¥ä¸Šçš„æ•ˆç‡ã€‚å½“éšè—å¤§å°è¶…è¿‡8æ—¶ï¼Œå¸¦å®½éœ€æ±‚ä¸‹é™åˆ°1 GB/sä»¥ä¸‹ğ¾ã€‚

é™¤äº†æ¨¡å‹çŠ¶æ€ä¹‹å¤–ï¼ŒZeRO-Infinityè¿˜å¯ä»¥åœ¨å¿…è¦æ—¶å°†æ¿€æ´»å†…å­˜å¸è½½åˆ°CPUå†…å­˜ä¸­ã€‚

ä¸ºäº†å‡å°‘å¤§å‹æ¨¡å‹çš„æ·±åº¦å­¦ä¹ è®­ç»ƒå¯¹å·¥ä½œå†…å­˜çš„éœ€æ±‚ï¼Œé€šè¿‡å°†å¤§å‹æ“ä½œåˆ†è§£ä¸ºå¯ä»¥é¡ºåºæ‰§è¡Œçš„è¾ƒå°çš„å¹³é“ºæ¥å‡å°‘å·¥ä½œå†…å­˜éœ€æ±‚ã€‚

ï¼ˆå¤§å‹ç®—å­å¯¹GPUçš„å†…å­˜éœ€æ±‚è¾ƒå¤§ï¼Œå¯ä»¥å°†å¤§å‹ç®—å­åˆ†è§£æˆå¯ä»¥é¡ºåºæ‰§è¡Œçš„è¾ƒå°çš„tilesï¼Œtileå¯¹GPUçš„å†…å­˜éœ€æ±‚è¾ƒå°ï¼‰

ä¸ºäº†å‡å°‘å¤§å‹çº¿æ€§æ“ä½œç¬¦çš„å·¥ä½œå†…å­˜ï¼ŒZeRO-Infinityå°†è¯¥æ“ä½œç¬¦è¡¨ç¤ºä¸ºç”±åŸå§‹æ“ä½œç¬¦çš„å‚æ•°å—ç»„æˆçš„è¾ƒå°çº¿æ€§æ“ä½œç¬¦çš„æ•°å­¦ç­‰æ•ˆåºåˆ—ï¼Œå¹¶ä¾æ¬¡æ‰§è¡Œå®ƒä»¬ã€‚å½“ä¸ZeRO-3ç»“åˆä½¿ç”¨æ—¶ï¼Œæ¯ä¸ªtileçš„å‚æ•°å’Œæ¢¯åº¦å¯ä»¥ä¸€æ¬¡è·å–å’Œé‡Šæ”¾ä¸€ä¸ªï¼Œä¸tileçš„æ•°é‡æˆæ¯”ä¾‹åœ°å‡å°‘å·¥ä½œå†…å­˜ã€‚å› æ­¤ï¼ŒZeRO-Infinityå¯ä»¥æ”¯æŒä»»æ„å¤§å°çš„è¿ç®—ç¬¦ï¼Œè€Œæ— éœ€ä¾èµ–äºæ¨¡å‹å¹¶è¡Œæ€§æ¥é€‚åº”æœ‰é™çš„GPUå†…å­˜ã€‚

æ•ˆç‡w.r.tå‚æ•°å’Œæ¢¯åº¦ã€‚å‚æ•°å’Œæ¢¯åº¦çš„æ•°æ®ç§»åŠ¨å¸¦å®½å¿…é¡»å¤§äº70GB/sï¼Œæ¥è¿‘DGX-2é›†ç¾¤ä¸Šå¯ç”¨çš„GPU-GPUå¸¦å®½[42]ã€‚åƒZeRO3[11]è¿™æ ·çš„æ·±åº¦å­¦ä¹ å¹¶è¡Œè®­ç»ƒè§£å†³æ–¹æ¡ˆï¼Œåœ¨å‘å‰æˆ–å‘åä¼ æ’­ä¹‹å‰å°†å‚æ•°ä»æ‰€æœ‰è€…GPUå¹¿æ’­ç»™å…¶ä»–GPUï¼Œåªè¦é€šä¿¡æ˜¯é‡å çš„ï¼Œå°±å¯ä»¥é«˜æ•ˆè¿è¡Œã€‚

On the contrary, a meager 12 GB/s PCIe bandwidth from a single
GPU to CPU memory or NVMe (see Fig. 2b) or vice-versa is simply
not sufficient to support heterogeneous training at scale
. Therefore, existing heterogeneous solutions like ZeRO-Offload where the parameters must be first moved from CPU to owner GPU before broadcasting requires significantly large batch sizes per GPU to achieve enough ğ‘ğ‘–ğ‘¡ necessary to be efficient under the limited bandwidth. 

ï¼ˆä½†ç”±äºå‚æ•°å’Œæ¢¯åº¦æ˜¯å¸è½½åˆ°CPUæˆ–NVMeå†…å­˜ï¼Œå¸¦å®½å—åˆ°PCIeçš„é™åˆ¶ï¼Œåªæœ‰12GB/s)

è¿™å¸¦æ¥äº†ä¸¤ä¸ªé—®é¢˜:1)å¯¹äºå¤§è§„æ¨¡æ¨¡å‹ï¼Œæ¿€æ´»å†…å­˜ä¼šå˜å¾—å¤ªå¤§ï¼Œç”šè‡³æ— æ³•å®¹çº³CPUå†…å­˜;2)å½“æ‰©å±•åˆ°æ•°ç™¾æˆ–æ•°åƒä¸ªgpuä»¥å®ç°æœ‰æ•ˆæ”¶æ•›æ—¶ï¼Œæœ‰æ•ˆæ‰¹å¤„ç†å¤§å°ä¼šå˜å¾—å¤ªå¤§ã€‚

ZeRO-Infinityä»¥ä¸¤ç§æ–¹å¼è§£å†³äº†è¿™äº›æŒ‘æˆ˜:i)ä»¥å¸¦å®½ä¸ºä¸­å¿ƒçš„åˆ†åŒº:ä¸€ç§æ–°çš„æ•°æ®æ˜ å°„å’Œå¹¶è¡Œæ•°æ®æ£€ç´¢ç­–ç•¥ï¼Œç”¨äºå¸è½½å‚æ•°å’Œæ¢¯åº¦ï¼Œå…è®¸ZeRO-Infinityå®ç°å‡ ä¹æ— é™çš„å¼‚æ„å†…å­˜å¸¦å®½(è¯¦ç»†ä¿¡æ¯è§ç¬¬6.1èŠ‚)ï¼Œii)ä»¥é‡å ä¸ºä¸­å¿ƒçš„è®¾è®¡ï¼Œå…è®¸ZeRO-Infinityä¸ä»…ä¸è®¡ç®—é‡å GPU-GPUé€šä¿¡ï¼Œè¿˜é‡å NVMe-CPUå’ŒCPU-GPUé€šä¿¡é€šè¿‡PCIe(è¯¦ç»†ä¿¡æ¯è§ç¬¬5.1.3èŠ‚)ã€‚

Unlike parameters and gradients that are consumed and produced sequentially during the forward and backward propagation, optimizer states can be updated in parallel, all at once.(æ•°æ®å¹¶è¡Œåˆ’åˆ†äº†ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œæ‰€æœ‰çš„ä¼˜åŒ–å™¨çŠ¶æ€åˆ†åŒºå¯ä»¥ç‹¬ç«‹å¹¶è¡Œæ›´æ–°ï¼Œä¸ç”¨é¡ºåº)

ï¼ˆæ¢¯åº¦æ˜¯å…ˆåœ¨gpuèšåˆï¼Ÿè¿˜æ˜¯å¸è½½åˆ°CPUå†èšåˆï¼Œæ„Ÿè§‰æ˜¯åé¢ä¸€ç§ï¼Œå¸è½½çš„æ—¶å€™é€Ÿåº¦å¿«ï¼Œå‰é¢ä¸€ç§ï¼Œåªæœ‰ä¸€ä¸ªgpuåœ¨å¸è½½ï¼Œåé¢æ˜¯å¤šgpuå¸è½½ï¼Œä½†å ç”¨å†…å­˜å¤§ï¼Œåˆ°åº•æ˜¯å“ªç§ï¼Œæˆ–è€…æ˜¯all-reduceå†åˆ†åˆ«å¸è½½ï¼‰

This property is leveraged by both ZeRO-3 and ZeRO-Offload, that store and update the optimizer states in GPUï¼ˆzero-3) and CPU(zero-offload) memory, respectively, in parallel across all
available GPUs(å¤šgpuåˆ†åŒºåŒæ—¶æ›´æ–°) and CPUs(å¤šcpuåˆ†åŒºåŒæ—¶æ›´æ–°ï¼‰.

éšç€GPUæˆ–CPUæ•°é‡çš„å¢åŠ ï¼ŒèšåˆGPUæˆ–CPUå†…å­˜å¸¦å®½å¯ä»¥è¿œè¿œé«˜äºæ‰€éœ€çš„1.5TB/sã€‚

Since ZeRO-Infinity is built upon ZeRO-3, it can also leverage
the aggregate GPU and CPU memory bandwidth as well as the
aggregate CPU compute for optimizer step, when offloading optimizer states to CPU memoryï¼ˆcpuæ›´æ–°ï¼‰

ç„¶è€Œï¼Œä½¿ç”¨NVMeå¸è½½ï¼Œæœ‰å¿…è¦å°†æ•°æ®ä»NVMeå¸¦åˆ°CPUå†…å­˜ä¸­ï¼Œç„¶åä»¥é€‚åˆCPUå†…å­˜çš„å—çš„å½¢å¼è¿”å›ï¼Œä»¥æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ï¼Œæ¯æ¬¡ä¸€ä¸ªå—

The optimizer step is therefore limited by the NVMe-CPU memory bandwidth: while ZeRO-Infinity can achieve aggregate NVMe bandwidth across multiple nodes, it is crucial to achieve near peak NVMe bandwidth per node, to allow supporting the necessary bandwidth of over 1.5 TB/s with as few nodes, and as small batch size as possibleï¼ˆè™½ç„¶åˆ†åŒºå¹¶è¡Œï¼Œä½†è¿˜æ˜¯è¦å°½åŠ›æé«˜å¸¦å®½ï¼‰

å°†æ•°æ®ä»NVMeå¯¼å…¥åˆ°CPUå†…å­˜æˆ–ä»CPUå†…å­˜å¯¼å…¥åˆ°GPUå†…å­˜çš„è¿‡ç¨‹å¯èƒ½ä¼šå¯¼è‡´GPUå’ŒCPUä¸­çš„CPUå†…å­˜ç¢ç‰‡ï¼Œä»è€Œå¯¼è‡´å†…å­˜ä¸è¶³ï¼Œå³ä½¿ä»ç„¶æœ‰å¤§é‡å†…å­˜å¯ç”¨ã€‚

æ— é™å¸è½½å¼•æ“ä¸ä»…å¯ä»¥å®ç°æ¥è¿‘å³°å€¼çš„NVMeå¸¦å®½ï¼Œå®ƒè¿˜å¯ä»¥å…è®¸ZeRO-Infinityå°†NVMeåˆ°CPUçš„è¯»å–ä¸CPUåˆ°NVMeçš„å†™å…¥é‡å ï¼Œä»¥åŠä¼˜åŒ–å™¨æ­¥éª¤çš„CPUè®¡ç®—ï¼ŒåŒæ—¶å…è®¸ZeRO-Infinityåœ¨å°‘é‡gpuä¸Šä¿æŒé€‚åº¦çš„æ‰¹é‡å¤§å°ï¼Œå¹¶åœ¨å¤§é‡gpuä¸Šä¿æŒå°æ‰¹é‡å¤§å°ã€‚åŒæ—¶ï¼Œå®ƒé€šè¿‡å°å¿ƒåœ°ä¸ºæ•°æ®ç§»åŠ¨é‡ç”¨ä¸´æ—¶ç¼“å†²åŒºæ¥æœ€å°åŒ–å†…å­˜ç¢ç‰‡ã€‚

On a DGX-2 node, each GPU can
read and write data at about 3 GB/s to CPU memory in parallelï¼ˆä¸€ä¸ªcpuå†…å­˜å¯¹åº”å¤šä¸ªgpuæ˜¾å­˜ï¼Œæ¯ä¸ªgpuéƒ½ä¼ æ¿€æ´»åˆ°å†…å­˜ï¼Œå¹¶è¡Œï¼‰
over the PCIe allowing activation checkpoints to be offloaded to
CPU memory while retaining over 80% efficiency for hidden size
larger 8ğ¾ or larger

ï¼ˆé€šè¿‡PCIeä¼ è¾“æ¿€æ´»æ£€æŸ¥ç‚¹æ˜¯è¶³å¤Ÿçš„ï¼Œå¯ä»¥å‡å°‘æ¿€æ´»æ£€æŸ¥ç‚¹çš„é¢‘ç‡ã€å°†æ¿€æ´»æ£€æŸ¥ç‚¹ä¸CPUå†…å­˜ä¹‹é—´çš„é€šä¿¡ä¸GPUä¸Šçš„å‰å‘å’Œåå‘è®¡ç®—é‡å ï¼‰

ä¸ºäº†åœ¨è¾ƒå°çš„éšè—å°ºå¯¸ä¸‹å®ç°é«˜æ•ˆç‡ï¼ŒZeRO-Infinityå¯ä»¥å‡å°‘æ¿€æ´»æ£€æŸ¥ç‚¹çš„é¢‘ç‡ï¼Œå¹¶æœ‰æ•ˆåœ°å°†æ¿€æ´»æ£€æŸ¥ç‚¹ä¸CPUå†…å­˜ä¹‹é—´çš„é€šä¿¡ä¸GPUä¸Šçš„å‰å‘å’Œåå‘è®¡ç®—é‡å ã€‚

ä¸ZeROå’ŒZeRO- offloadä¸åŒï¼Œå…¶ä¸­æ¯å±‚çš„å‚æ•°ç”±å•ä¸ªæ•°æ®å¹¶è¡Œè¿›ç¨‹æ‹¥æœ‰ï¼Œå¹¶åœ¨éœ€è¦æ—¶å°†å…¶å¹¿æ’­ç»™å…¶ä»–æ•°æ®å¹¶è¡Œè¿›ç¨‹ï¼ŒZeRO- infinityåœ¨æ‰€æœ‰æ•°æ®å¹¶è¡Œè¿›ç¨‹ä¸­åˆ’åˆ†æ¯ä¸€å±‚çš„å‚æ•°ï¼Œå¹¶åœ¨éœ€è¦è®¿é—®å‚æ•°æ—¶ä½¿ç”¨allgatherè€Œä¸æ˜¯å¹¿æ’­ã€‚

åœ¨åŸºäºå¹¿æ’­çš„æ–¹æ³•ä¸­ï¼Œç”±äºæ¯ä¸ªå‚æ•°å®Œå…¨ç”±ä¸€ä¸ªæ•°æ®å¹¶è¡Œè¿›ç¨‹æ‹¥æœ‰ï¼Œå› æ­¤åœ¨å¹¿æ’­å‘ç”Ÿä¹‹å‰ï¼Œå‚æ•°å¿…é¡»é¦–å…ˆä»å…¶æºä½ç½®(CPUæˆ–NVMe)é€šè¿‡PCIeé€šä¿¡åˆ°GPUå†…å­˜ã€‚

åªæœ‰ä¸€ä¸ªPCIeå¯ä»¥æ¿€æ´»æ­¤è¿›ç¨‹ï¼Œè€Œè¿æ¥åˆ°æ‰€æœ‰å…¶ä»–gpuçš„æ‰€æœ‰PCIeé“¾è·¯éƒ½æ˜¯ç©ºé—²çš„ã€‚

åœ¨ZeRO-Infinityä¸­ä½¿ç”¨åˆ†åŒºå‚æ•°å’ŒåŸºäºallgatherçš„æ–¹æ³•ï¼Œæ‰€æœ‰PCIeé“¾è·¯å¹¶è¡Œæ´»åŠ¨ï¼Œæ¯ä¸ªé“¾è·¯å¼•å…¥å‚æ•°çš„1/ğ‘‘ğ‘ğ‘¡éƒ¨åˆ†ï¼Œå…¶ä¸­ğ‘‘ğ‘ä¸ºæ•°æ®å¹¶è¡Œåº¦ã€‚å› æ­¤ï¼ŒNVMeæˆ–CPUä¸GPUä¹‹é—´çš„æœ‰æ•ˆé€šä¿¡å¸¦å®½ï¼Œéšç€ğ‘‘ğ‘åº¦çš„å¢åŠ è€Œçº¿æ€§å¢åŠ ã€‚

ä½¿ç”¨åŸºäºå¹¿æ’­çš„æ–¹æ³•ï¼ŒCPU/NVMeåˆ°GPUçš„å¸¦å®½åœ¨PCIe Gen 3ä¸­ä¿æŒæ’å®šåœ¨12 GB/så·¦å³ï¼Œå³ä½¿åœ¨DGX-2ä¸Šå…·æœ‰16è·¯æ•°æ®å¹¶è¡Œæ€§ã€‚ç„¶è€Œï¼Œé‡‡ç”¨åŸºäºå…¨é›†åˆçš„æ–¹æ³•ï¼Œæœ‰æ•ˆå¯å®ç°å¸¦å®½åˆ†åˆ«å¢åŠ åˆ°çº¦48/25 GB/s(æ¯ä¸ªGPU 3.0/1.6 GB/s)(è§å›¾2b)ï¼Œä»…å—æ¯ä¸ªDGX-2èŠ‚ç‚¹çš„æœ€å¤§èšåˆPCIeå¸¦å®½å’Œæœ€å¤§NVMeå¸¦å®½çš„é™åˆ¶ã€‚ä»è¿™é‡Œå¼€å§‹ï¼Œå¸¦å®½éšç€èŠ‚ç‚¹çš„å¢åŠ è€Œçº¿æ€§å¢é•¿ã€‚

å› æ­¤ï¼Œåœ¨å¤§è§„æ¨¡è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹æ—¶ï¼ŒZeRO-Infinityå¯ä»¥æä¾›æ¯”å¿…è¦(å®é™…ä¸Šæ˜¯æ— é™çš„)æ›´å¤šçš„å¼‚æ„å†…å­˜å¸¦å®½ï¼Œä»¥ä¿æŒè®­ç»ƒçš„æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œåœ¨64ä¸ªDGX-2èŠ‚ç‚¹ä¸Šï¼ŒZeRO-Infinityå¯ä»¥è®¿é—®è¶…è¿‡3TB/sçš„CPUå†…å­˜å¸¦å®½å’Œè¶…è¿‡1.5TB/sçš„NVMeå¸¦å®½ã€‚

è™½ç„¶ZeRO-Infinityå¯ä»¥åœ¨å¤šèŠ‚ç‚¹è®¾ç½®ä¸Šåˆ©ç”¨è¶³å¤Ÿçš„å¼‚æ„å†…å­˜å¸¦å®½ï¼Œä½†å¸¦å®½ä»ç„¶å¯èƒ½æˆä¸ºå•ä¸ªGPUæˆ–å•ä¸ªèŠ‚ç‚¹è®¾ç½®çš„ç“¶é¢ˆã€‚ï¼ˆåœ¨å•GPUä¸Šï¼Œå¸¦å®½ä¾æ—§æ˜¯ç“¶é¢ˆï¼‰å³ä½¿GPU-GPU allgatheré€šä¿¡åœ¨å°æ‰¹é‡è¿è¡Œæ—¶ä¹Ÿä¼šå¯¹æ•ˆç‡äº§ç”Ÿå¾ˆå¤§å½±å“(å›¾3)ã€‚ï¼ˆå³ä½¿æ˜¯GPUçš„å¸¦å®½ä¹Ÿä¸å¤Ÿæ•ˆç‡ï¼Œæ‰€ä»¥è¦é‡å ï¼‰

è®¿é—®NVMeå†…å­˜éœ€è¦ä¸‰æ­¥è¿‡ç¨‹:i)å°†æ•°æ®ä»NVMeè¯»å–åˆ°CPUå†…å­˜(nc-transfer)ï¼Œ ii)å°†æ•°æ®ä»CPUå†…å­˜å¤åˆ¶åˆ°GPUå†…å­˜(cg-transfer)ï¼Œ iii)æ‰§è¡Œallgatherä»¥åœ¨æ‰€æœ‰GPUä¸Šæ„é€ å®Œæ•´å‚æ•°(gg-transfer)ã€‚è¿™äº›æ•°æ®ç§»åŠ¨çš„é¡ºåºæ€§è´¨æ„å‘³ç€ï¼Œå¦‚æœç®€å•åœ°è¿›è¡Œï¼Œæ€»é€šä¿¡æ—¶é—´å°†æ˜¯è¿™ä¸‰ä¸ªæ•°æ®ç§»åŠ¨æˆæœ¬çš„æ€»å’Œï¼Œå³ä½¿æ¯ä¸ªé˜¶æ®µçš„æ•°æ®ç§»åŠ¨å¸¦å®½éƒ½è¶³å¤Ÿï¼Œä¹Ÿä¼šå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚

ï¼ˆè®¿é—®MVMeå†…å­˜éœ€è¦ä¸‰æ­¥ï¼ŒNVMe-CPU-GPUï¼Œå¦‚æœé¡ºåºè¿›è¡Œï¼Œä¹Ÿä¼šå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚ï¼‰

 overlap engine å°†GPU-GPUé€šä¿¡ä¸GPUè®¡ç®—é‡å ï¼Œè¿˜å°†NVMeä¸CPUã€CPUä¸GPUé€šä¿¡é‡å 

i) A dynamic prefetcher for overlapping the data movement required to reconstruct parameters before they are consumed
in the forward or backward pass, and ii) a communication and
offload overlapping mechanism for executing the data movement
required by gradients in parallel with the backward computation.

ï¼ˆåœ¨ä¸‹ä¸ªç®—å­è®¡ç®—ä¹‹å‰é¢„å–å‚æ•°ï¼Œåœ¨åå‘çš„åŒæ—¶å¸è½½æ¢¯åº¦ï¼‰

before executing the ğ‘–ğ‘¡â„ operator, the prefetcher can invoke nc, cg, and gg-transfer for parameters required by ğ‘– +3,ğ‘– +2, and ğ‘– +1 operators, respectively. Note that all of these data movement can happen in parallel with the execution of the ğ‘–ğ‘¡â„ operator. 

ï¼ˆå‰å‘ï¼šåœ¨æ‰§è¡Œç¬¬iä¸ªç®—å­çš„åŒæ—¶ï¼Œä½¿ç”¨GPU-GPUé€šä¿¡ä¼ è¾“ç¬¬i+1ä¸ªç®—å­çš„å‚æ•°ï¼Œä½¿ç”¨CPU-GPUé€šä¿¡ä¼ è¾“ç¬¬i+2ä¸ªç®—å­çš„å‚æ•°ï¼Œä½¿ç”¨NVMe-CPUé€šä¿¡ä¼ è¾“ç¬¬i+3ä¸ªç®—å­çš„å‚æ•°ï¼‰

in the backward pass, ZeRO-Infinity can overlap the
reduce-scatter for gradients of the parameters in (ğ‘– + 1)
ğ‘¡â„ operator with the computation of the ğ‘–ğ‘¡â„ operator, while simultaneous transferring the partitioned gradients from the reduce-scatter of the gradients of the (ğ‘– + 2)ğ‘¡â„ operator to the CPU or NVMe.

ï¼ˆåå‘ï¼šåœ¨æ‰§è¡Œç¬¬iä¸ªç®—å­çš„åŒæ—¶ï¼Œå¯¹ç¬¬i+1ä¸ªç®—å­çš„æ¢¯åº¦ä½¿ç”¨reduce-scatterï¼Œå¸è½½ç¬¬i+2ä¸ªç®—å­çš„æ¢¯åº¦ï¼‰

å…·æœ‰æ•°ä¸‡äº¿ä¸ªå‚æ•°çš„æ¨¡å‹å…·æœ‰å‡ºè‰²çš„è®­ç»ƒæ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚

æˆ‘ä»¬å°†æ¨¡å‹çŠ¶æ€å’Œæ¢¯åº¦å¸è½½åˆ°å…·æœ‰è¶³å¤Ÿå®¹é‡çš„æœ€å¿«å†…å­˜ï¼Œå› ä¸ºå®ƒä»¥æœ€å°çš„é€šä¿¡å¼€é”€èŠ‚çœäº†æœ€å¤§çš„å†…å­˜ã€‚æ¥ä¸‹æ¥ï¼Œåœ¨å‚æ•°å’Œæ¿€æ´»æ£€æŸ¥ç‚¹ä¹‹é—´ï¼Œå¦‚æœåªæœ‰ä¸€ä¸ªéœ€è¦å¸è½½åˆ°CPUå†…å­˜ï¼Œæˆ‘ä»¬æ ¹æ®ç»éªŒé€‰æ‹©å¸è½½æä¾›æ›´å¥½æ€§èƒ½çš„é‚£ä¸ªã€‚å½“ä¸¤è€…éƒ½éœ€è¦å¸è½½æ—¶ï¼Œæ¿€æ´»æ£€æŸ¥ç‚¹è¢«å¸è½½åˆ°CPUï¼Œå‚æ•°è¢«å¸è½½åˆ°å…·æœ‰è¶³å¤Ÿå®¹é‡çš„æœ€å¿«å†…å­˜ã€‚

å›¾5aæ˜¾ç¤ºäº†ä»10T (43 TFlops/GPU)åˆ°20T (34 TFlops/GPU)çš„æ€§èƒ½ä¸‹é™ã€‚è¿™ç§ä¸‹é™ä¸æ˜¯ç”±äºNVMeå¸¦å®½ï¼Œå› ä¸ºä¸¤ç§æ¨¡å‹å¤§å°éƒ½ä½¿ç”¨NVMeå¸è½½ï¼Œè€Œæ˜¯ç”±äºæ¯ä¸ªGPUåœ¨20Tè§„æ¨¡ä¸‹çš„æ‰¹å¤„ç†å¤§å°éå¸¸å°(è¡¨1)ï¼Œå› ä¸ºCPUå†…å­˜æœ‰é™ï¼Œæ— æ³•å­˜å‚¨æ¿€æ´»æ£€æŸ¥ç‚¹ã€‚è¿™å¯ä»¥é€šè¿‡åœ¨å°†æ¥çš„å®ç°ä¸­å¢åŠ CPUå†…å­˜æˆ–å°†æ¿€æ´»æ£€æŸ¥ç‚¹å¸è½½åˆ°NVMeæ¥æ”¹è¿›ã€‚

ï¼ˆå¸è½½åˆ°CPUå¯¼è‡´æ‰¹å¤„ç†å°ï¼‰

è¿™æ˜¯ä¸€ä¸ªå¼±æ‰©å±•ç»“æœï¼Œæˆ‘ä»¬å°†æ¯ä¸ªèŠ‚ç‚¹çš„æ‰¹å¤§å°ä¿æŒä¸å˜ï¼Œå¹¶éšç€èŠ‚ç‚¹æ•°é‡çš„å¢åŠ è€Œå¢åŠ æ€»æ‰¹å¤§å°ã€‚ZeRO-Infinityé€šè¿‡æœ‰æ•ˆåœ°åˆ©ç”¨èšåˆPCIeå’ŒNVMeå¸¦å®½çš„çº¿æ€§å¢é•¿æ¥åŠ é€Ÿå‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„å¸è½½ï¼Œå¹¶åˆ©ç”¨æ¥è‡ªå…¶ä»–èŠ‚ç‚¹çš„CPUè®¡ç®—æ¥è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œä»è€Œè¶…è¶Šäº†å®Œç¾çš„çº¿æ€§æ‰©å±•ã€‚

åœ¨512 GPUè§„æ¨¡ä¸‹ï¼Œè®¡ç®—å‡ºçš„æ•ˆç‡ä¸‹é™æ¯”w.r.tä¼˜åŒ–å™¨çŠ¶æ€çš„å‚æ•°å’Œæ¢¯åº¦æ›´å¤§ã€‚æ•ˆç‡ä¸‹é™çš„ä¸»è¦åŸå› æ˜¯GPU-GPUçš„å¸¦å®½æœ‰é™ï¼Œåªæœ‰70 GB/sã€‚å› æ­¤ï¼Œå½“æ‰¹å¤„ç†è§„æ¨¡è¾ƒå°æ—¶ï¼Œæ­¤å¸¦å®½æ˜¯ZeRO-Infinityä¸­æœ€çªå‡ºçš„æ€§èƒ½ç“¶é¢ˆæ¥æºã€‚